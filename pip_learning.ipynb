{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# author identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as mltp\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import sklearn as sk\n",
    "import seaborn as sea\n",
    "import re \n",
    "from tqdm import tqdm\n",
    "from statistics import median\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durchführung:\n",
    "1. Cleaning & Preprocessing\n",
    "    1. Removing punctuation\n",
    "    2. Lemmatization\n",
    "    3. Sonderzeichen, welche unnutzbar sind\n",
    "2. tokenize\n",
    "    1. word_tokenize\n",
    "    2. sent_tokenize\n",
    "    3. stopwords\n",
    "    4. stemwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zu hohe Anzahl an Daten, weshalb eine Reduktion dieser wichtig wurde, von fast 700k zu 20k und 6k Testdaten\n",
    "1. Nur Englischsprachige\n",
    "2. Haben 39 Kategorien --> je 500 Einträge, auf welchen trainiert wird\n",
    "\n",
    "Nächste Schritte:\n",
    "- LDA machen für die Topics und deren Wörter\n",
    "- ML basierend auf den Vektoren machen, um die Eigenschaften zu identifizieren\n",
    "- anbinden an Frontend für eine Angabe der Eigenschaften"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripping(liste):\n",
    "    return [i.strip() for i in liste]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data/df_tokenized_port_2.csv',converters={'sent_tokenize': lambda x: x[1:-1].strip('][').replace(\"'\",\"\").strip().split(','),'word_tokenize': lambda x: x[1:-1].strip('][').replace(\"'\",\"\").strip().split(','),\n",
    "'word_tokenize_without_stopwords': lambda x: x[1:-1].strip('][').replace(\"'\",\"\").strip().split(',')\n",
    ",'word_tokenize_without_stopwords_port': lambda x: x[1:-1].strip('][').replace(\"'\",\"\").strip().split(',')\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_tokenize']=df['word_tokenize'].apply(stripping)\n",
    "df['sent_tokenize']=df['sent_tokenize'].apply(stripping)\n",
    "df['word_tokenize_without_stopwords']=df['word_tokenize_without_stopwords'].apply(stripping)\n",
    "df['word_tokenize_without_stopwords_port']=df['word_tokenize_without_stopwords_port'].apply(stripping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "644247"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>language_2</th>\n",
       "      <th>word_tokenize</th>\n",
       "      <th>sent_tokenize</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>word_tokenize_num_of_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords_port</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages and 4.5 MB ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Info, has, been, found, (, +/-, 100, pages, a...</td>\n",
       "      <td>[Info has been found (+/- 100 pages and 4.5 MB...</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>[Info, found, (, +/-, 100, pages, 4.5, MB, .pd...</td>\n",
       "      <td>[info, found, (, +/-, 100, page, 4.5, mb, .pdf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members: Drewes van der Laa...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[These, are, the, team, members, :, Drewes, va...</td>\n",
       "      <td>[These are the team members: Drewes van der La...</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[These, team, members, :, Drewes, van, der, La...</td>\n",
       "      <td>[these, team, member, :, drew, van, der, laag,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo's Toolbar I can now 'capture' ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Thanks, to, Yahoo, \"s\", Toolbar, I, can, now,...</td>\n",
       "      <td>[\"Thanks to Yahoos Toolbar I can now capture t...</td>\n",
       "      <td>77</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>[Thanks, Yahoo, \"s\", Toolbar, I, \"capture\", \"\"...</td>\n",
       "      <td>[thank, yahoo, \"s\", toolbar, i, \"captur\", \"\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>I had an interesting conversation with my Dad ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[I, had, an, interesting, conversation, with, ...</td>\n",
       "      <td>[I had an interesting conversation with my Dad...</td>\n",
       "      <td>760</td>\n",
       "      <td>36</td>\n",
       "      <td>298</td>\n",
       "      <td>[I, interesting, conversation, Dad, morning, ....</td>\n",
       "      <td>[i, interest, convers, dad, morn, ., we, talk,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>Somehow Coca-Cola has a way of summing up thin...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Somehow, Coca-Cola, has, a, way, of, summing,...</td>\n",
       "      <td>[Somehow Coca-Cola has a way of summing up thi...</td>\n",
       "      <td>226</td>\n",
       "      <td>14</td>\n",
       "      <td>91</td>\n",
       "      <td>[Somehow, Coca-Cola, way, summing, things, wel...</td>\n",
       "      <td>[somehow, coca-cola, way, sum, thing, well, .,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id gender  age              topic      sign          date  \\\n",
       "0  2059027   male   15            Student       Leo   14,May,2004   \n",
       "1  2059027   male   15            Student       Leo   13,May,2004   \n",
       "2  3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "3  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "4  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "\n",
       "                                                text lang language_2  \\\n",
       "0  Info has been found (+/- 100 pages and 4.5 MB ...   en         en   \n",
       "1  These are the team members: Drewes van der Laa...   en         en   \n",
       "2  Thanks to Yahoo's Toolbar I can now 'capture' ...   en         en   \n",
       "3  I had an interesting conversation with my Dad ...   en         en   \n",
       "4  Somehow Coca-Cola has a way of summing up thin...   en         en   \n",
       "\n",
       "                                       word_tokenize  \\\n",
       "0  [Info, has, been, found, (, +/-, 100, pages, a...   \n",
       "1  [These, are, the, team, members, :, Drewes, va...   \n",
       "2  [Thanks, to, Yahoo, \"s\", Toolbar, I, can, now,...   \n",
       "3  [I, had, an, interesting, conversation, with, ...   \n",
       "4  [Somehow, Coca-Cola, has, a, way, of, summing,...   \n",
       "\n",
       "                                       sent_tokenize  count_word  count_sent  \\\n",
       "0  [Info has been found (+/- 100 pages and 4.5 MB...          31           1   \n",
       "1  [These are the team members: Drewes van der La...          23           1   \n",
       "2  [\"Thanks to Yahoos Toolbar I can now capture t...          77           3   \n",
       "3  [I had an interesting conversation with my Dad...         760          36   \n",
       "4  [Somehow Coca-Cola has a way of summing up thi...         226          14   \n",
       "\n",
       "   word_tokenize_num_of_stopwords  \\\n",
       "0                              11   \n",
       "1                               3   \n",
       "2                              26   \n",
       "3                             298   \n",
       "4                              91   \n",
       "\n",
       "                     word_tokenize_without_stopwords  \\\n",
       "0  [Info, found, (, +/-, 100, pages, 4.5, MB, .pd...   \n",
       "1  [These, team, members, :, Drewes, van, der, La...   \n",
       "2  [Thanks, Yahoo, \"s\", Toolbar, I, \"capture\", \"\"...   \n",
       "3  [I, interesting, conversation, Dad, morning, ....   \n",
       "4  [Somehow, Coca-Cola, way, summing, things, wel...   \n",
       "\n",
       "                word_tokenize_without_stopwords_port  \n",
       "0  [info, found, (, +/-, 100, page, 4.5, mb, .pdf...  \n",
       "1  [these, team, member, :, drew, van, der, laag,...  \n",
       "2  [thank, yahoo, \"s\", toolbar, i, \"captur\", \"\", ...  \n",
       "3  [i, interest, convers, dad, morn, ., we, talk,...  \n",
       "4  [somehow, coca-cola, way, sum, thing, well, .,...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>language_2</th>\n",
       "      <th>word_tokenize</th>\n",
       "      <th>sent_tokenize</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>word_tokenize_num_of_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords_port</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo's Toolbar I can now 'capture' ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Thanks, to, Yahoo, \"s\", Toolbar, I, can, now,...</td>\n",
       "      <td>[\"Thanks to Yahoos Toolbar I can now capture t...</td>\n",
       "      <td>77</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>[Thanks, Yahoo, \"s\", Toolbar, I, \"capture\", \"\"...</td>\n",
       "      <td>[thank, yahoo, \"s\", toolbar, i, \"captur\", \"\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>I surf the English news sites a lot looking fo...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[I, surf, the, English, news, sites, a, lot, l...</td>\n",
       "      <td>[\"I surf the English news sites a lot looking ...</td>\n",
       "      <td>183</td>\n",
       "      <td>7</td>\n",
       "      <td>64</td>\n",
       "      <td>[I, surf, English, news, sites, lot, looking, ...</td>\n",
       "      <td>[i, surf, english, news, site, lot, look, tidb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>There is so much that is different here from a...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[There, is, so, much, that, is, different, her...</td>\n",
       "      <td>[\"There is so much that is different here from...</td>\n",
       "      <td>74</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>[There, much, different, anything, I, \"ve\", ev...</td>\n",
       "      <td>[there, much, differ, anyth, i, \"ve\", ever, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>22,June,2004</td>\n",
       "      <td>You may have noticed a new feature on my blog....</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[You, may, have, noticed, a, new, feature, on,...</td>\n",
       "      <td>[\"You may have noticed a new feature on my blo...</td>\n",
       "      <td>179</td>\n",
       "      <td>7</td>\n",
       "      <td>65</td>\n",
       "      <td>[You, may, noticed, new, feature, blog, ..., \"...</td>\n",
       "      <td>[you, may, notic, new, featur, blog, ..., \"s\",...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>02,July,2004</td>\n",
       "      <td>It seems everything is not all that smooth in ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[It, seems, everything, is, not, all, that, sm...</td>\n",
       "      <td>[It seems everything is not all that smooth in...</td>\n",
       "      <td>92</td>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>[It, seems, everything, smooth, Seoul, concern...</td>\n",
       "      <td>[it, seem, everyth, smooth, seoul, concern, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644240</th>\n",
       "      <td>1713845</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>Student</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>02,July,2004</td>\n",
       "      <td>Dear Susan There once was a bird the size of a...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Dear, Susan, There, once, was, a, bird, the, ...</td>\n",
       "      <td>[Dear Susan There once was a bird the size of ...</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>[Dear, Susan, There, bird, size, turd, sitting...</td>\n",
       "      <td>[dear, susan, there, bird, size, turd, sit, te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644241</th>\n",
       "      <td>1713845</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>Student</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>01,July,2004</td>\n",
       "      <td>Dear Susan you keep asking me who your 'dad' i...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Dear, Susan, you, keep, asking, me, who, your...</td>\n",
       "      <td>[\"Dear Susan you keep asking me who your dad i...</td>\n",
       "      <td>101</td>\n",
       "      <td>8</td>\n",
       "      <td>49</td>\n",
       "      <td>[Dear, Susan, keep, asking, \"dad\", \"\", ., gues...</td>\n",
       "      <td>[dear, susan, keep, ask, \"dad\", \"\", ., guess, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644242</th>\n",
       "      <td>1713845</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>Student</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>01,July,2004</td>\n",
       "      <td>Dear Susan I could write some really bitter di...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Dear, Susan, I, could, write, some, really, b...</td>\n",
       "      <td>[Dear Susan I could write some really bitter d...</td>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>[Dear, Susan, I, could, write, really, bitter,...</td>\n",
       "      <td>[dear, susan, i, could, write, realli, bitter,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644243</th>\n",
       "      <td>1713845</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>Student</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>01,July,2004</td>\n",
       "      <td>Dear Susan 'I have the second yeast infection ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Dear, Susan, \"\", I, have, the, second, yeast,...</td>\n",
       "      <td>[\"Dear Susan I have the second yeast infection...</td>\n",
       "      <td>77</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>[Dear, Susan, \"\", I, second, yeast, infection,...</td>\n",
       "      <td>[dear, susan, \"\", i, second, yeast, infect, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644245</th>\n",
       "      <td>1713845</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>Student</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>01,July,2004</td>\n",
       "      <td>Dear Susan: Just to clarify I am asking you to...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Dear, Susan, :, Just, to, clarify, I, am, ask...</td>\n",
       "      <td>[\"Dear Susan: Just to clarify I am asking you ...</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>[Dear, Susan, :, Just, clarify, I, asking, lea...</td>\n",
       "      <td>[dear, susan, :, just, clarifi, i, ask, leav, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>295394 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id gender  age              topic      sign          date  \\\n",
       "2       3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "7       3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "10      3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "19      3581210   male   33  InvestmentBanking  Aquarius  22,June,2004   \n",
       "21      3581210   male   33  InvestmentBanking  Aquarius  02,July,2004   \n",
       "...         ...    ...  ...                ...       ...           ...   \n",
       "644240  1713845   male   23            Student    Taurus  02,July,2004   \n",
       "644241  1713845   male   23            Student    Taurus  01,July,2004   \n",
       "644242  1713845   male   23            Student    Taurus  01,July,2004   \n",
       "644243  1713845   male   23            Student    Taurus  01,July,2004   \n",
       "644245  1713845   male   23            Student    Taurus  01,July,2004   \n",
       "\n",
       "                                                     text lang language_2  \\\n",
       "2       Thanks to Yahoo's Toolbar I can now 'capture' ...   en         en   \n",
       "7       I surf the English news sites a lot looking fo...   en         en   \n",
       "10      There is so much that is different here from a...   en         en   \n",
       "19      You may have noticed a new feature on my blog....   en         en   \n",
       "21      It seems everything is not all that smooth in ...   en         en   \n",
       "...                                                   ...  ...        ...   \n",
       "644240  Dear Susan There once was a bird the size of a...   en         en   \n",
       "644241  Dear Susan you keep asking me who your 'dad' i...   en         en   \n",
       "644242  Dear Susan I could write some really bitter di...   en         en   \n",
       "644243  Dear Susan 'I have the second yeast infection ...   en         en   \n",
       "644245  Dear Susan: Just to clarify I am asking you to...   en         en   \n",
       "\n",
       "                                            word_tokenize  \\\n",
       "2       [Thanks, to, Yahoo, \"s\", Toolbar, I, can, now,...   \n",
       "7       [I, surf, the, English, news, sites, a, lot, l...   \n",
       "10      [There, is, so, much, that, is, different, her...   \n",
       "19      [You, may, have, noticed, a, new, feature, on,...   \n",
       "21      [It, seems, everything, is, not, all, that, sm...   \n",
       "...                                                   ...   \n",
       "644240  [Dear, Susan, There, once, was, a, bird, the, ...   \n",
       "644241  [Dear, Susan, you, keep, asking, me, who, your...   \n",
       "644242  [Dear, Susan, I, could, write, some, really, b...   \n",
       "644243  [Dear, Susan, \"\", I, have, the, second, yeast,...   \n",
       "644245  [Dear, Susan, :, Just, to, clarify, I, am, ask...   \n",
       "\n",
       "                                            sent_tokenize  count_word  \\\n",
       "2       [\"Thanks to Yahoos Toolbar I can now capture t...          77   \n",
       "7       [\"I surf the English news sites a lot looking ...         183   \n",
       "10      [\"There is so much that is different here from...          74   \n",
       "19      [\"You may have noticed a new feature on my blo...         179   \n",
       "21      [It seems everything is not all that smooth in...          92   \n",
       "...                                                   ...         ...   \n",
       "644240  [Dear Susan There once was a bird the size of ...          64   \n",
       "644241  [\"Dear Susan you keep asking me who your dad i...         101   \n",
       "644242  [Dear Susan I could write some really bitter d...          45   \n",
       "644243  [\"Dear Susan I have the second yeast infection...          77   \n",
       "644245  [\"Dear Susan: Just to clarify I am asking you ...          68   \n",
       "\n",
       "        count_sent  word_tokenize_num_of_stopwords  \\\n",
       "2                3                              26   \n",
       "7                7                              64   \n",
       "10               4                              26   \n",
       "19               7                              65   \n",
       "21               4                              39   \n",
       "...            ...                             ...   \n",
       "644240           5                              26   \n",
       "644241           8                              49   \n",
       "644242           4                              15   \n",
       "644243           5                              24   \n",
       "644245           2                              28   \n",
       "\n",
       "                          word_tokenize_without_stopwords  \\\n",
       "2       [Thanks, Yahoo, \"s\", Toolbar, I, \"capture\", \"\"...   \n",
       "7       [I, surf, English, news, sites, lot, looking, ...   \n",
       "10      [There, much, different, anything, I, \"ve\", ev...   \n",
       "19      [You, may, noticed, new, feature, blog, ..., \"...   \n",
       "21      [It, seems, everything, smooth, Seoul, concern...   \n",
       "...                                                   ...   \n",
       "644240  [Dear, Susan, There, bird, size, turd, sitting...   \n",
       "644241  [Dear, Susan, keep, asking, \"dad\", \"\", ., gues...   \n",
       "644242  [Dear, Susan, I, could, write, really, bitter,...   \n",
       "644243  [Dear, Susan, \"\", I, second, yeast, infection,...   \n",
       "644245  [Dear, Susan, :, Just, clarify, I, asking, lea...   \n",
       "\n",
       "                     word_tokenize_without_stopwords_port  \n",
       "2       [thank, yahoo, \"s\", toolbar, i, \"captur\", \"\", ...  \n",
       "7       [i, surf, english, news, site, lot, look, tidb...  \n",
       "10      [there, much, differ, anyth, i, \"ve\", ever, se...  \n",
       "19      [you, may, notic, new, featur, blog, ..., \"s\",...  \n",
       "21      [it, seem, everyth, smooth, seoul, concern, re...  \n",
       "...                                                   ...  \n",
       "644240  [dear, susan, there, bird, size, turd, sit, te...  \n",
       "644241  [dear, susan, keep, ask, \"dad\", \"\", ., guess, ...  \n",
       "644242  [dear, susan, i, could, write, realli, bitter,...  \n",
       "644243  [dear, susan, \"\", i, second, yeast, infect, pa...  \n",
       "644245  [dear, susan, :, just, clarifi, i, ask, leav, ...  \n",
       "\n",
       "[295394 rows x 16 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['count_sent']>1)&(df['count_sent']<20)&(df['count_word']>1)&(df['count_word']<200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Info',\n",
       " 'found',\n",
       " '(',\n",
       " '+/-',\n",
       " '100',\n",
       " 'pages',\n",
       " '4.5',\n",
       " 'MB',\n",
       " '.pdf',\n",
       " 'files',\n",
       " ')',\n",
       " 'Now',\n",
       " 'wait',\n",
       " 'untill',\n",
       " 'team',\n",
       " 'leader',\n",
       " 'processed',\n",
       " 'learns',\n",
       " 'html',\n",
       " '.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['word_tokenize_without_stopwords'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "644247"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/niclascramer/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/niclascramer/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#defining the object for Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#defining the function for lemmatization\n",
    "def lemmatizer(text):\n",
    "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "    return lemm_text\n",
    "df['msg_lemmatized']=df['word_tokenize_without_stopwords'].apply(lambda x:lemmatizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>language_2</th>\n",
       "      <th>word_tokenize</th>\n",
       "      <th>sent_tokenize</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>word_tokenize_num_of_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords_port</th>\n",
       "      <th>msg_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages and 4.5 MB ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Info, has, been, found, (, +/-, 100, pages, a...</td>\n",
       "      <td>[Info has been found (+/- 100 pages and 4.5 MB...</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>[Info, found, (, +/-, 100, pages, 4.5, MB, .pd...</td>\n",
       "      <td>[info, found, (, +/-, 100, page, 4.5, mb, .pdf...</td>\n",
       "      <td>[Info, found, (, +/-, 100, page, 4.5, MB, .pdf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members: Drewes van der Laa...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[These, are, the, team, members, :, Drewes, va...</td>\n",
       "      <td>[These are the team members: Drewes van der La...</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[These, team, members, :, Drewes, van, der, La...</td>\n",
       "      <td>[these, team, member, :, drew, van, der, laag,...</td>\n",
       "      <td>[These, team, member, :, Drewes, van, der, Laa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo's Toolbar I can now 'capture' ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Thanks, to, Yahoo, \"s\", Toolbar, I, can, now,...</td>\n",
       "      <td>[\"Thanks to Yahoos Toolbar I can now capture t...</td>\n",
       "      <td>77</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>[Thanks, Yahoo, \"s\", Toolbar, I, \"capture\", \"\"...</td>\n",
       "      <td>[thank, yahoo, \"s\", toolbar, i, \"captur\", \"\", ...</td>\n",
       "      <td>[Thanks, Yahoo, \"s\", Toolbar, I, \"capture\", \"\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>I had an interesting conversation with my Dad ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[I, had, an, interesting, conversation, with, ...</td>\n",
       "      <td>[I had an interesting conversation with my Dad...</td>\n",
       "      <td>760</td>\n",
       "      <td>36</td>\n",
       "      <td>298</td>\n",
       "      <td>[I, interesting, conversation, Dad, morning, ....</td>\n",
       "      <td>[i, interest, convers, dad, morn, ., we, talk,...</td>\n",
       "      <td>[I, interesting, conversation, Dad, morning, ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>Somehow Coca-Cola has a way of summing up thin...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Somehow, Coca-Cola, has, a, way, of, summing,...</td>\n",
       "      <td>[Somehow Coca-Cola has a way of summing up thi...</td>\n",
       "      <td>226</td>\n",
       "      <td>14</td>\n",
       "      <td>91</td>\n",
       "      <td>[Somehow, Coca-Cola, way, summing, things, wel...</td>\n",
       "      <td>[somehow, coca-cola, way, sum, thing, well, .,...</td>\n",
       "      <td>[Somehow, Coca-Cola, way, summing, thing, well...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id gender  age              topic      sign          date  \\\n",
       "0  2059027   male   15            Student       Leo   14,May,2004   \n",
       "1  2059027   male   15            Student       Leo   13,May,2004   \n",
       "2  3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "3  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "4  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "\n",
       "                                                text lang language_2  \\\n",
       "0  Info has been found (+/- 100 pages and 4.5 MB ...   en         en   \n",
       "1  These are the team members: Drewes van der Laa...   en         en   \n",
       "2  Thanks to Yahoo's Toolbar I can now 'capture' ...   en         en   \n",
       "3  I had an interesting conversation with my Dad ...   en         en   \n",
       "4  Somehow Coca-Cola has a way of summing up thin...   en         en   \n",
       "\n",
       "                                       word_tokenize  \\\n",
       "0  [Info, has, been, found, (, +/-, 100, pages, a...   \n",
       "1  [These, are, the, team, members, :, Drewes, va...   \n",
       "2  [Thanks, to, Yahoo, \"s\", Toolbar, I, can, now,...   \n",
       "3  [I, had, an, interesting, conversation, with, ...   \n",
       "4  [Somehow, Coca-Cola, has, a, way, of, summing,...   \n",
       "\n",
       "                                       sent_tokenize  count_word  count_sent  \\\n",
       "0  [Info has been found (+/- 100 pages and 4.5 MB...          31           1   \n",
       "1  [These are the team members: Drewes van der La...          23           1   \n",
       "2  [\"Thanks to Yahoos Toolbar I can now capture t...          77           3   \n",
       "3  [I had an interesting conversation with my Dad...         760          36   \n",
       "4  [Somehow Coca-Cola has a way of summing up thi...         226          14   \n",
       "\n",
       "   word_tokenize_num_of_stopwords  \\\n",
       "0                              11   \n",
       "1                               3   \n",
       "2                              26   \n",
       "3                             298   \n",
       "4                              91   \n",
       "\n",
       "                     word_tokenize_without_stopwords  \\\n",
       "0  [Info, found, (, +/-, 100, pages, 4.5, MB, .pd...   \n",
       "1  [These, team, members, :, Drewes, van, der, La...   \n",
       "2  [Thanks, Yahoo, \"s\", Toolbar, I, \"capture\", \"\"...   \n",
       "3  [I, interesting, conversation, Dad, morning, ....   \n",
       "4  [Somehow, Coca-Cola, way, summing, things, wel...   \n",
       "\n",
       "                word_tokenize_without_stopwords_port  \\\n",
       "0  [info, found, (, +/-, 100, page, 4.5, mb, .pdf...   \n",
       "1  [these, team, member, :, drew, van, der, laag,...   \n",
       "2  [thank, yahoo, \"s\", toolbar, i, \"captur\", \"\", ...   \n",
       "3  [i, interest, convers, dad, morn, ., we, talk,...   \n",
       "4  [somehow, coca-cola, way, sum, thing, well, .,...   \n",
       "\n",
       "                                      msg_lemmatized  \n",
       "0  [Info, found, (, +/-, 100, page, 4.5, MB, .pdf...  \n",
       "1  [These, team, member, :, Drewes, van, der, Laa...  \n",
       "2  [Thanks, Yahoo, \"s\", Toolbar, I, \"capture\", \"\"...  \n",
       "3  [I, interesting, conversation, Dad, morning, ....  \n",
       "4  [Somehow, Coca-Cola, way, summing, thing, well...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df['cleanLinks'] = df['text'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[~df['text'].str.contains('https:\\/\\/.*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laenge_der_Saetze(liste):\n",
    "    laenge=[]\n",
    "    for x in liste:\n",
    "        laenge.append(len(x.split()))\n",
    "    return median(laenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def läenge(df):\n",
    "    return len(df['word_tokenize'])/len(df['sent_tokenize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_per_sent_mean']=round(df['count_word']/df['count_sent'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>language_2</th>\n",
       "      <th>word_tokenize</th>\n",
       "      <th>sent_tokenize</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>word_tokenize_num_of_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords_port</th>\n",
       "      <th>msg_lemmatized</th>\n",
       "      <th>cleanLinks</th>\n",
       "      <th>laenge_saetze</th>\n",
       "      <th>word_per_sent_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages and 4.5 MB ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Info, has, been, found, (, +/-, 100, pages, a...</td>\n",
       "      <td>[Info has been found (+/- 100 pages and 4.5 MB...</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>[Info, found, (, +/-, 100, pages, 4.5, MB, .pd...</td>\n",
       "      <td>[info, found, (, +/-, 100, page, 4.5, mb, .pdf...</td>\n",
       "      <td>[Info, found, (, +/-, 100, page, 4.5, MB, .pdf...</td>\n",
       "      <td>Info has been found (+/- 100 pages and 4.5 MB ...</td>\n",
       "      <td>1</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members: Drewes van der Laa...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[These, are, the, team, members, :, Drewes, va...</td>\n",
       "      <td>[These are the team members: Drewes van der La...</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[These, team, members, :, Drewes, van, der, La...</td>\n",
       "      <td>[these, team, member, :, drew, van, der, laag,...</td>\n",
       "      <td>[These, team, member, :, Drewes, van, der, Laa...</td>\n",
       "      <td>These are the team members: Drewes van der Laa...</td>\n",
       "      <td>1</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo's Toolbar I can now 'capture' ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Thanks, to, Yahoo, \"s\", Toolbar, I, can, now,...</td>\n",
       "      <td>[\"Thanks to Yahoos Toolbar I can now capture t...</td>\n",
       "      <td>77</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>[Thanks, Yahoo, \"s\", Toolbar, I, \"capture\", \"\"...</td>\n",
       "      <td>[thank, yahoo, \"s\", toolbar, i, \"captur\", \"\", ...</td>\n",
       "      <td>[Thanks, Yahoo, \"s\", Toolbar, I, \"capture\", \"\"...</td>\n",
       "      <td>Thanks to Yahoo's Toolbar I can now 'capture' ...</td>\n",
       "      <td>3</td>\n",
       "      <td>25.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>I had an interesting conversation with my Dad ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[I, had, an, interesting, conversation, with, ...</td>\n",
       "      <td>[I had an interesting conversation with my Dad...</td>\n",
       "      <td>760</td>\n",
       "      <td>36</td>\n",
       "      <td>298</td>\n",
       "      <td>[I, interesting, conversation, Dad, morning, ....</td>\n",
       "      <td>[i, interest, convers, dad, morn, ., we, talk,...</td>\n",
       "      <td>[I, interesting, conversation, Dad, morning, ....</td>\n",
       "      <td>I had an interesting conversation with my Dad ...</td>\n",
       "      <td>36</td>\n",
       "      <td>21.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>Somehow Coca-Cola has a way of summing up thin...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Somehow, Coca-Cola, has, a, way, of, summing,...</td>\n",
       "      <td>[Somehow Coca-Cola has a way of summing up thi...</td>\n",
       "      <td>226</td>\n",
       "      <td>14</td>\n",
       "      <td>91</td>\n",
       "      <td>[Somehow, Coca-Cola, way, summing, things, wel...</td>\n",
       "      <td>[somehow, coca-cola, way, sum, thing, well, .,...</td>\n",
       "      <td>[Somehow, Coca-Cola, way, summing, thing, well...</td>\n",
       "      <td>Somehow Coca-Cola has a way of summing up thin...</td>\n",
       "      <td>14</td>\n",
       "      <td>16.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id gender  age              topic      sign          date  \\\n",
       "0  2059027   male   15            Student       Leo   14,May,2004   \n",
       "1  2059027   male   15            Student       Leo   13,May,2004   \n",
       "2  3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "3  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "4  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "\n",
       "                                                text lang language_2  \\\n",
       "0  Info has been found (+/- 100 pages and 4.5 MB ...   en         en   \n",
       "1  These are the team members: Drewes van der Laa...   en         en   \n",
       "2  Thanks to Yahoo's Toolbar I can now 'capture' ...   en         en   \n",
       "3  I had an interesting conversation with my Dad ...   en         en   \n",
       "4  Somehow Coca-Cola has a way of summing up thin...   en         en   \n",
       "\n",
       "                                       word_tokenize  \\\n",
       "0  [Info, has, been, found, (, +/-, 100, pages, a...   \n",
       "1  [These, are, the, team, members, :, Drewes, va...   \n",
       "2  [Thanks, to, Yahoo, \"s\", Toolbar, I, can, now,...   \n",
       "3  [I, had, an, interesting, conversation, with, ...   \n",
       "4  [Somehow, Coca-Cola, has, a, way, of, summing,...   \n",
       "\n",
       "                                       sent_tokenize  count_word  count_sent  \\\n",
       "0  [Info has been found (+/- 100 pages and 4.5 MB...          31           1   \n",
       "1  [These are the team members: Drewes van der La...          23           1   \n",
       "2  [\"Thanks to Yahoos Toolbar I can now capture t...          77           3   \n",
       "3  [I had an interesting conversation with my Dad...         760          36   \n",
       "4  [Somehow Coca-Cola has a way of summing up thi...         226          14   \n",
       "\n",
       "   word_tokenize_num_of_stopwords  \\\n",
       "0                              11   \n",
       "1                               3   \n",
       "2                              26   \n",
       "3                             298   \n",
       "4                              91   \n",
       "\n",
       "                     word_tokenize_without_stopwords  \\\n",
       "0  [Info, found, (, +/-, 100, pages, 4.5, MB, .pd...   \n",
       "1  [These, team, members, :, Drewes, van, der, La...   \n",
       "2  [Thanks, Yahoo, \"s\", Toolbar, I, \"capture\", \"\"...   \n",
       "3  [I, interesting, conversation, Dad, morning, ....   \n",
       "4  [Somehow, Coca-Cola, way, summing, things, wel...   \n",
       "\n",
       "                word_tokenize_without_stopwords_port  \\\n",
       "0  [info, found, (, +/-, 100, page, 4.5, mb, .pdf...   \n",
       "1  [these, team, member, :, drew, van, der, laag,...   \n",
       "2  [thank, yahoo, \"s\", toolbar, i, \"captur\", \"\", ...   \n",
       "3  [i, interest, convers, dad, morn, ., we, talk,...   \n",
       "4  [somehow, coca-cola, way, sum, thing, well, .,...   \n",
       "\n",
       "                                      msg_lemmatized  \\\n",
       "0  [Info, found, (, +/-, 100, page, 4.5, MB, .pdf...   \n",
       "1  [These, team, member, :, Drewes, van, der, Laa...   \n",
       "2  [Thanks, Yahoo, \"s\", Toolbar, I, \"capture\", \"\"...   \n",
       "3  [I, interesting, conversation, Dad, morning, ....   \n",
       "4  [Somehow, Coca-Cola, way, summing, thing, well...   \n",
       "\n",
       "                                          cleanLinks  laenge_saetze  \\\n",
       "0  Info has been found (+/- 100 pages and 4.5 MB ...              1   \n",
       "1  These are the team members: Drewes van der Laa...              1   \n",
       "2  Thanks to Yahoo's Toolbar I can now 'capture' ...              3   \n",
       "3  I had an interesting conversation with my Dad ...             36   \n",
       "4  Somehow Coca-Cola has a way of summing up thin...             14   \n",
       "\n",
       "   word_per_sent_mean  \n",
       "0               31.00  \n",
       "1               23.00  \n",
       "2               25.67  \n",
       "3               21.11  \n",
       "4               16.14  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wörter von 1 bis 707\n",
    "Sätze 1 bis 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "591760"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[(df['count_word']<708)&(df['count_sent']<34)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[(df['count_word']<708)&(df['count_sent']<34)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/final_gesamt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['topic'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indUnk                     238179\n",
       "Student                    145103\n",
       "Technology                  39631\n",
       "Arts                        30754\n",
       "Education                   28213\n",
       "Communications-Media        19085\n",
       "Internet                    15006\n",
       "Non-Profit                  13906\n",
       "Engineering                 10873\n",
       "Law                          8685\n",
       "Publishing                   7356\n",
       "Science                      6824\n",
       "Government                   6459\n",
       "Consulting                   5516\n",
       "Religion                     5020\n",
       "Fashion                      4539\n",
       "Marketing                    4492\n",
       "Advertising                  4425\n",
       "BusinessServices             4194\n",
       "Banking                      3776\n",
       "Accounting                   3662\n",
       "Chemicals                    3596\n",
       "Telecommunications           3561\n",
       "Military                     3006\n",
       "Museums-Libraries            2999\n",
       "Sports-Recreation            2886\n",
       "HumanResources               2833\n",
       "RealEstate                   2783\n",
       "Transportation               2220\n",
       "Manufacturing                2144\n",
       "Biotech                      2113\n",
       "Tourism                      1855\n",
       "LawEnforcement-Security      1780\n",
       "Architecture                 1434\n",
       "InvestmentBanking            1181\n",
       "Agriculture                  1163\n",
       "Automotive                   1142\n",
       "Construction                  989\n",
       "Environment                   557\n",
       "Maritime                      269\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>language_2</th>\n",
       "      <th>word_tokenize</th>\n",
       "      <th>sent_tokenize</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>word_tokenize_num_of_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords_port</th>\n",
       "      <th>msg_lemmatized</th>\n",
       "      <th>cleanLinks</th>\n",
       "      <th>laenge_saetze</th>\n",
       "      <th>word_per_sent_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages and 4.5 MB ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Info, has, been, found, (, +/-, 100, pages, a...</td>\n",
       "      <td>[Info has been found (+/- 100 pages and 4.5 MB...</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>[Info, found, (, +/-, 100, pages, 4.5, MB, .pd...</td>\n",
       "      <td>[info, found, (, +/-, 100, page, 4.5, mb, .pdf...</td>\n",
       "      <td>[Info, found, (, +/-, 100, page, 4.5, MB, .pdf...</td>\n",
       "      <td>Info has been found (+/- 100 pages and 4.5 MB ...</td>\n",
       "      <td>1</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members: Drewes van der Laa...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[These, are, the, team, members, :, Drewes, va...</td>\n",
       "      <td>[These are the team members: Drewes van der La...</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[These, team, members, :, Drewes, van, der, La...</td>\n",
       "      <td>[these, team, member, :, drew, van, der, laag,...</td>\n",
       "      <td>[These, team, member, :, Drewes, van, der, Laa...</td>\n",
       "      <td>These are the team members: Drewes van der Laa...</td>\n",
       "      <td>1</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo's Toolbar I can now 'capture' ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Thanks, to, Yahoo, \"s\", Toolbar, I, can, now,...</td>\n",
       "      <td>[\"Thanks to Yahoos Toolbar I can now capture t...</td>\n",
       "      <td>77</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>[Thanks, Yahoo, \"s\", Toolbar, I, \"capture\", \"\"...</td>\n",
       "      <td>[thank, yahoo, \"s\", toolbar, i, \"captur\", \"\", ...</td>\n",
       "      <td>[Thanks, Yahoo, \"s\", Toolbar, I, \"capture\", \"\"...</td>\n",
       "      <td>Thanks to Yahoo's Toolbar I can now 'capture' ...</td>\n",
       "      <td>3</td>\n",
       "      <td>25.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>If anything Korea is a country of extremes. Ev...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[If, anything, Korea, is, a, country, of, extr...</td>\n",
       "      <td>[If anything Korea is a country of extremes., ...</td>\n",
       "      <td>430</td>\n",
       "      <td>23</td>\n",
       "      <td>168</td>\n",
       "      <td>[If, anything, Korea, country, extremes, ., Ev...</td>\n",
       "      <td>[if, anyth, korea, countri, extrem, ., everyth...</td>\n",
       "      <td>[If, anything, Korea, country, extreme, ., Eve...</td>\n",
       "      <td>If anything Korea is a country of extremes. Ev...</td>\n",
       "      <td>23</td>\n",
       "      <td>18.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>Take a read of this news article from urlLink ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Take, a, read, of, this, news, article, from,...</td>\n",
       "      <td>[\"Take a read of this news article from urlLin...</td>\n",
       "      <td>430</td>\n",
       "      <td>11</td>\n",
       "      <td>158</td>\n",
       "      <td>[Take, read, news, article, urlLink, JoongAng,...</td>\n",
       "      <td>[take, read, news, articl, urllink, joongang, ...</td>\n",
       "      <td>[Take, read, news, article, urlLink, JoongAng,...</td>\n",
       "      <td>Take a read of this news article from urlLink ...</td>\n",
       "      <td>11</td>\n",
       "      <td>39.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>I surf the English news sites a lot looking fo...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[I, surf, the, English, news, sites, a, lot, l...</td>\n",
       "      <td>[\"I surf the English news sites a lot looking ...</td>\n",
       "      <td>183</td>\n",
       "      <td>7</td>\n",
       "      <td>64</td>\n",
       "      <td>[I, surf, English, news, sites, lot, looking, ...</td>\n",
       "      <td>[i, surf, english, news, site, lot, look, tidb...</td>\n",
       "      <td>[I, surf, English, news, site, lot, looking, t...</td>\n",
       "      <td>I surf the English news sites a lot looking fo...</td>\n",
       "      <td>7</td>\n",
       "      <td>26.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>If you click on my profile you'll make a not-s...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[If, you, click, on, my, profile, you, \"ll\", m...</td>\n",
       "      <td>[\"If you click on my profile youll make a not-...</td>\n",
       "      <td>563</td>\n",
       "      <td>22</td>\n",
       "      <td>236</td>\n",
       "      <td>[If, click, profile, \"ll\", make, not-so-startl...</td>\n",
       "      <td>[if, click, profil, \"ll\", make, not-so-startl,...</td>\n",
       "      <td>[If, click, profile, \"ll\", make, not-so-startl...</td>\n",
       "      <td>If you click on my profile you'll make a not-s...</td>\n",
       "      <td>22</td>\n",
       "      <td>25.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>Last night was pretty fun...mostly because of ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Last, night, was, pretty, fun, ..., mostly, b...</td>\n",
       "      <td>[Last night was pretty fun...mostly because of...</td>\n",
       "      <td>658</td>\n",
       "      <td>23</td>\n",
       "      <td>230</td>\n",
       "      <td>[Last, night, pretty, fun, ..., mostly, compan...</td>\n",
       "      <td>[last, night, pretti, fun, ..., mostli, compan...</td>\n",
       "      <td>[Last, night, pretty, fun, ..., mostly, compan...</td>\n",
       "      <td>Last night was pretty fun...mostly because of ...</td>\n",
       "      <td>23</td>\n",
       "      <td>28.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>There is so much that is different here from a...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[There, is, so, much, that, is, different, her...</td>\n",
       "      <td>[\"There is so much that is different here from...</td>\n",
       "      <td>74</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>[There, much, different, anything, I, \"ve\", ev...</td>\n",
       "      <td>[there, much, differ, anyth, i, \"ve\", ever, se...</td>\n",
       "      <td>[There, much, different, anything, I, \"ve\", ev...</td>\n",
       "      <td>There is so much that is different here from a...</td>\n",
       "      <td>4</td>\n",
       "      <td>18.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>09,June,2004</td>\n",
       "      <td>urlLink Here it is the superfantastic phonebox...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[urlLink, Here, it, is, the, superfantastic, p...</td>\n",
       "      <td>[urlLink Here it is the superfantastic phonebo...</td>\n",
       "      <td>373</td>\n",
       "      <td>17</td>\n",
       "      <td>148</td>\n",
       "      <td>[urlLink, Here, superfantastic, phonebox, Toda...</td>\n",
       "      <td>[urllink, here, superfantast, phonebox, today,...</td>\n",
       "      <td>[urlLink, Here, superfantastic, phonebox, Toda...</td>\n",
       "      <td>urlLink Here it is the superfantastic phonebox...</td>\n",
       "      <td>17</td>\n",
       "      <td>21.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id gender  age              topic      sign          date  \\\n",
       "0   2059027   male   15            Student       Leo   14,May,2004   \n",
       "1   2059027   male   15            Student       Leo   13,May,2004   \n",
       "2   3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "5   3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "6   3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "7   3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "8   3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "9   3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "10  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "11  3581210   male   33  InvestmentBanking  Aquarius  09,June,2004   \n",
       "\n",
       "                                                 text lang language_2  \\\n",
       "0   Info has been found (+/- 100 pages and 4.5 MB ...   en         en   \n",
       "1   These are the team members: Drewes van der Laa...   en         en   \n",
       "2   Thanks to Yahoo's Toolbar I can now 'capture' ...   en         en   \n",
       "5   If anything Korea is a country of extremes. Ev...   en         en   \n",
       "6   Take a read of this news article from urlLink ...   en         en   \n",
       "7   I surf the English news sites a lot looking fo...   en         en   \n",
       "8   If you click on my profile you'll make a not-s...   en         en   \n",
       "9   Last night was pretty fun...mostly because of ...   en         en   \n",
       "10  There is so much that is different here from a...   en         en   \n",
       "11  urlLink Here it is the superfantastic phonebox...   en         en   \n",
       "\n",
       "                                        word_tokenize  \\\n",
       "0   [Info, has, been, found, (, +/-, 100, pages, a...   \n",
       "1   [These, are, the, team, members, :, Drewes, va...   \n",
       "2   [Thanks, to, Yahoo, \"s\", Toolbar, I, can, now,...   \n",
       "5   [If, anything, Korea, is, a, country, of, extr...   \n",
       "6   [Take, a, read, of, this, news, article, from,...   \n",
       "7   [I, surf, the, English, news, sites, a, lot, l...   \n",
       "8   [If, you, click, on, my, profile, you, \"ll\", m...   \n",
       "9   [Last, night, was, pretty, fun, ..., mostly, b...   \n",
       "10  [There, is, so, much, that, is, different, her...   \n",
       "11  [urlLink, Here, it, is, the, superfantastic, p...   \n",
       "\n",
       "                                        sent_tokenize  count_word  count_sent  \\\n",
       "0   [Info has been found (+/- 100 pages and 4.5 MB...          31           1   \n",
       "1   [These are the team members: Drewes van der La...          23           1   \n",
       "2   [\"Thanks to Yahoos Toolbar I can now capture t...          77           3   \n",
       "5   [If anything Korea is a country of extremes., ...         430          23   \n",
       "6   [\"Take a read of this news article from urlLin...         430          11   \n",
       "7   [\"I surf the English news sites a lot looking ...         183           7   \n",
       "8   [\"If you click on my profile youll make a not-...         563          22   \n",
       "9   [Last night was pretty fun...mostly because of...         658          23   \n",
       "10  [\"There is so much that is different here from...          74           4   \n",
       "11  [urlLink Here it is the superfantastic phonebo...         373          17   \n",
       "\n",
       "    word_tokenize_num_of_stopwords  \\\n",
       "0                               11   \n",
       "1                                3   \n",
       "2                               26   \n",
       "5                              168   \n",
       "6                              158   \n",
       "7                               64   \n",
       "8                              236   \n",
       "9                              230   \n",
       "10                              26   \n",
       "11                             148   \n",
       "\n",
       "                      word_tokenize_without_stopwords  \\\n",
       "0   [Info, found, (, +/-, 100, pages, 4.5, MB, .pd...   \n",
       "1   [These, team, members, :, Drewes, van, der, La...   \n",
       "2   [Thanks, Yahoo, \"s\", Toolbar, I, \"capture\", \"\"...   \n",
       "5   [If, anything, Korea, country, extremes, ., Ev...   \n",
       "6   [Take, read, news, article, urlLink, JoongAng,...   \n",
       "7   [I, surf, English, news, sites, lot, looking, ...   \n",
       "8   [If, click, profile, \"ll\", make, not-so-startl...   \n",
       "9   [Last, night, pretty, fun, ..., mostly, compan...   \n",
       "10  [There, much, different, anything, I, \"ve\", ev...   \n",
       "11  [urlLink, Here, superfantastic, phonebox, Toda...   \n",
       "\n",
       "                 word_tokenize_without_stopwords_port  \\\n",
       "0   [info, found, (, +/-, 100, page, 4.5, mb, .pdf...   \n",
       "1   [these, team, member, :, drew, van, der, laag,...   \n",
       "2   [thank, yahoo, \"s\", toolbar, i, \"captur\", \"\", ...   \n",
       "5   [if, anyth, korea, countri, extrem, ., everyth...   \n",
       "6   [take, read, news, articl, urllink, joongang, ...   \n",
       "7   [i, surf, english, news, site, lot, look, tidb...   \n",
       "8   [if, click, profil, \"ll\", make, not-so-startl,...   \n",
       "9   [last, night, pretti, fun, ..., mostli, compan...   \n",
       "10  [there, much, differ, anyth, i, \"ve\", ever, se...   \n",
       "11  [urllink, here, superfantast, phonebox, today,...   \n",
       "\n",
       "                                       msg_lemmatized  \\\n",
       "0   [Info, found, (, +/-, 100, page, 4.5, MB, .pdf...   \n",
       "1   [These, team, member, :, Drewes, van, der, Laa...   \n",
       "2   [Thanks, Yahoo, \"s\", Toolbar, I, \"capture\", \"\"...   \n",
       "5   [If, anything, Korea, country, extreme, ., Eve...   \n",
       "6   [Take, read, news, article, urlLink, JoongAng,...   \n",
       "7   [I, surf, English, news, site, lot, looking, t...   \n",
       "8   [If, click, profile, \"ll\", make, not-so-startl...   \n",
       "9   [Last, night, pretty, fun, ..., mostly, compan...   \n",
       "10  [There, much, different, anything, I, \"ve\", ev...   \n",
       "11  [urlLink, Here, superfantastic, phonebox, Toda...   \n",
       "\n",
       "                                           cleanLinks  laenge_saetze  \\\n",
       "0   Info has been found (+/- 100 pages and 4.5 MB ...              1   \n",
       "1   These are the team members: Drewes van der Laa...              1   \n",
       "2   Thanks to Yahoo's Toolbar I can now 'capture' ...              3   \n",
       "5   If anything Korea is a country of extremes. Ev...             23   \n",
       "6   Take a read of this news article from urlLink ...             11   \n",
       "7   I surf the English news sites a lot looking fo...              7   \n",
       "8   If you click on my profile you'll make a not-s...             22   \n",
       "9   Last night was pretty fun...mostly because of ...             23   \n",
       "10  There is so much that is different here from a...              4   \n",
       "11  urlLink Here it is the superfantastic phonebox...             17   \n",
       "\n",
       "    word_per_sent_mean  \n",
       "0                31.00  \n",
       "1                23.00  \n",
       "2                25.67  \n",
       "5                18.70  \n",
       "6                39.09  \n",
       "7                26.14  \n",
       "8                25.59  \n",
       "9                28.61  \n",
       "10               18.50  \n",
       "11               21.94  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.iloc[[0, 1, 2, 4, 5, 6, 7, 8, 9, 10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['topic']!='Maritime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random  500 inhlate rausziehen für die jeweilige kategorie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X=df.drop(columns = ['gender','age','topic','sign'],axis=1)\n",
    "y = df[['gender','age','topic','sign']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1,test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.merge(X_train, y_train, left_index=True, right_index=True)\n",
    "df_test=pd.merge(X_test, y_test, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('data/df_train.csv')\n",
    "df_test.to_csv('data/df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lsit=pd.DataFrame(df_train['topic'].value_counts()).reset_index().rename(columns={\"index\": \"topic\", \"topic\": \"num\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>indUnk</td>\n",
       "      <td>217471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student</td>\n",
       "      <td>131955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Technology</td>\n",
       "      <td>36847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arts</td>\n",
       "      <td>27453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Education</td>\n",
       "      <td>25218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Communications-Media</td>\n",
       "      <td>17130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Internet</td>\n",
       "      <td>13850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Non-Profit</td>\n",
       "      <td>12574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Engineering</td>\n",
       "      <td>9899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Law</td>\n",
       "      <td>7930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Publishing</td>\n",
       "      <td>6391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Science</td>\n",
       "      <td>6202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Government</td>\n",
       "      <td>5971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Consulting</td>\n",
       "      <td>4851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Religion</td>\n",
       "      <td>4495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Fashion</td>\n",
       "      <td>4198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Marketing</td>\n",
       "      <td>4170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Advertising</td>\n",
       "      <td>4037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BusinessServices</td>\n",
       "      <td>3747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Banking</td>\n",
       "      <td>3468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Chemicals</td>\n",
       "      <td>3413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Accounting</td>\n",
       "      <td>3230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Telecommunications</td>\n",
       "      <td>3206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Museums-Libraries</td>\n",
       "      <td>2799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Military</td>\n",
       "      <td>2643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>HumanResources</td>\n",
       "      <td>2579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sports-Recreation</td>\n",
       "      <td>2503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RealEstate</td>\n",
       "      <td>2399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Transportation</td>\n",
       "      <td>1978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Biotech</td>\n",
       "      <td>1961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>1831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Tourism</td>\n",
       "      <td>1630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>LawEnforcement-Security</td>\n",
       "      <td>1602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Architecture</td>\n",
       "      <td>1359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>1083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Agriculture</td>\n",
       "      <td>1075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Automotive</td>\n",
       "      <td>1038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Construction</td>\n",
       "      <td>919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Environment</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      topic     num\n",
       "0                    indUnk  217471\n",
       "1                   Student  131955\n",
       "2                Technology   36847\n",
       "3                      Arts   27453\n",
       "4                 Education   25218\n",
       "5      Communications-Media   17130\n",
       "6                  Internet   13850\n",
       "7                Non-Profit   12574\n",
       "8               Engineering    9899\n",
       "9                       Law    7930\n",
       "10               Publishing    6391\n",
       "11                  Science    6202\n",
       "12               Government    5971\n",
       "13               Consulting    4851\n",
       "14                 Religion    4495\n",
       "15                  Fashion    4198\n",
       "16                Marketing    4170\n",
       "17              Advertising    4037\n",
       "18         BusinessServices    3747\n",
       "19                  Banking    3468\n",
       "20                Chemicals    3413\n",
       "21               Accounting    3230\n",
       "22       Telecommunications    3206\n",
       "23        Museums-Libraries    2799\n",
       "24                 Military    2643\n",
       "25           HumanResources    2579\n",
       "26        Sports-Recreation    2503\n",
       "27               RealEstate    2399\n",
       "28           Transportation    1978\n",
       "29                  Biotech    1961\n",
       "30            Manufacturing    1831\n",
       "31                  Tourism    1630\n",
       "32  LawEnforcement-Security    1602\n",
       "33             Architecture    1359\n",
       "34        InvestmentBanking    1083\n",
       "35              Agriculture    1075\n",
       "36               Automotive    1038\n",
       "37             Construction     919\n",
       "38              Environment     518"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lsit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['indUnk',\n",
       " 'Student',\n",
       " 'Technology',\n",
       " 'Arts',\n",
       " 'Education',\n",
       " 'Communications-Media',\n",
       " 'Internet',\n",
       " 'Non-Profit',\n",
       " 'Engineering',\n",
       " 'Law',\n",
       " 'Publishing',\n",
       " 'Science',\n",
       " 'Government',\n",
       " 'Consulting',\n",
       " 'Religion',\n",
       " 'Fashion',\n",
       " 'Marketing',\n",
       " 'Advertising',\n",
       " 'BusinessServices',\n",
       " 'Banking',\n",
       " 'Chemicals',\n",
       " 'Accounting',\n",
       " 'Telecommunications',\n",
       " 'Museums-Libraries',\n",
       " 'Military',\n",
       " 'HumanResources',\n",
       " 'Sports-Recreation',\n",
       " 'RealEstate',\n",
       " 'Transportation',\n",
       " 'Biotech',\n",
       " 'Manufacturing',\n",
       " 'Tourism',\n",
       " 'LawEnforcement-Security',\n",
       " 'Architecture',\n",
       " 'InvestmentBanking',\n",
       " 'Agriculture',\n",
       " 'Automotive',\n",
       " 'Construction',\n",
       " 'Environment']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_values= df_lsit[df_lsit['num']>500].topic.to_list()\n",
    "list_of_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train['topic'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Student', 'indUnk', 'Arts', 'Publishing', 'Communications-Media',\n",
       "       'Education', 'Technology', 'Consulting', 'LawEnforcement-Security',\n",
       "       'Biotech', 'Government', 'Transportation', 'RealEstate',\n",
       "       'Internet', 'Chemicals', 'Non-Profit', 'Telecommunications',\n",
       "       'Museums-Libraries', 'Tourism', 'Engineering', 'InvestmentBanking',\n",
       "       'Accounting', 'Science', 'BusinessServices', 'Military',\n",
       "       'Religion', 'Law', 'Fashion', 'Construction', 'Environment',\n",
       "       'Marketing', 'Manufacturing', 'Banking', 'Advertising',\n",
       "       'Sports-Recreation', 'Automotive', 'HumanResources',\n",
       "       'Architecture', 'Agriculture'], dtype=object)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['topic'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(df_2,col,anzahl=500):\n",
    "    liste_index=[]\n",
    "    topic_list=df_2[col].unique()\n",
    "    for element in tqdm(topic_list):\n",
    "        liste=df_2[df_2['topic']==element].sample(n=anzahl, replace = False,random_state = 2).index.tolist()\n",
    "        liste_index=liste_index+liste\n",
    "    return liste_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:06<00:00,  6.13it/s]\n"
     ]
    }
   ],
   "source": [
    "df_500_liste=loop(df_train,'topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19500"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_500_liste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Student                    500\n",
       "Environment                500\n",
       "Science                    500\n",
       "BusinessServices           500\n",
       "Military                   500\n",
       "Religion                   500\n",
       "Law                        500\n",
       "Fashion                    500\n",
       "Construction               500\n",
       "Marketing                  500\n",
       "InvestmentBanking          500\n",
       "Manufacturing              500\n",
       "Banking                    500\n",
       "Advertising                500\n",
       "Sports-Recreation          500\n",
       "Automotive                 500\n",
       "HumanResources             500\n",
       "Architecture               500\n",
       "Accounting                 500\n",
       "Engineering                500\n",
       "indUnk                     500\n",
       "Biotech                    500\n",
       "Arts                       500\n",
       "Publishing                 500\n",
       "Communications-Media       500\n",
       "Education                  500\n",
       "Technology                 500\n",
       "Consulting                 500\n",
       "LawEnforcement-Security    500\n",
       "Government                 500\n",
       "Tourism                    500\n",
       "Transportation             500\n",
       "RealEstate                 500\n",
       "Internet                   500\n",
       "Chemicals                  500\n",
       "Non-Profit                 500\n",
       "Telecommunications         500\n",
       "Museums-Libraries          500\n",
       "Agriculture                500\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[df_500_liste].topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500=df_train.loc[df_500_liste]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500.to_csv('data/train_500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This blog is being posted due to the fact that little development has happened recently regarding the last day of school uniform swap. If we all plan to switch uniforms with a certain individual we must somehow organize something soon. Also what do we consider the last day of school I think if everyone participates this will be a success. We must plan this soon and pray that there is not a drop-the-dress-code-day on the last day of school.'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_500['text'][445164]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>language_2</th>\n",
       "      <th>word_tokenize</th>\n",
       "      <th>sent_tokenize</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>word_tokenize_num_of_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords_port</th>\n",
       "      <th>msg_lemmatized</th>\n",
       "      <th>cleanLinks</th>\n",
       "      <th>laenge_saetze</th>\n",
       "      <th>word_per_sent_mean</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>445164</th>\n",
       "      <td>2970791</td>\n",
       "      <td>25,May,2004</td>\n",
       "      <td>This blog is being posted due to the fact that...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[This, blog, is, being, posted, due, to, the, ...</td>\n",
       "      <td>[This blog is being posted due to the fact tha...</td>\n",
       "      <td>83</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>[This, blog, posted, due, fact, little, develo...</td>\n",
       "      <td>[thi, blog, post, due, fact, littl, develop, h...</td>\n",
       "      <td>[This, blog, posted, due, fact, little, develo...</td>\n",
       "      <td>This blog is being posted due to the fact that...</td>\n",
       "      <td>4</td>\n",
       "      <td>20.75</td>\n",
       "      <td>male</td>\n",
       "      <td>14</td>\n",
       "      <td>Student</td>\n",
       "      <td>Sagittarius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293091</th>\n",
       "      <td>3931851</td>\n",
       "      <td>02,agosto,2004</td>\n",
       "      <td>So I have a big fucking interview tomorrow for...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[So, I, have, a, big, fucking, interview, tomo...</td>\n",
       "      <td>[So I have a big fucking interview tomorrow fo...</td>\n",
       "      <td>88</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>[So, I, big, fucking, interview, tomorrow, new...</td>\n",
       "      <td>[so, i, big, fuck, interview, tomorrow, new, s...</td>\n",
       "      <td>[So, I, big, fucking, interview, tomorrow, new...</td>\n",
       "      <td>So I have a big fucking interview tomorrow for...</td>\n",
       "      <td>5</td>\n",
       "      <td>17.60</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Pisces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594697</th>\n",
       "      <td>3481650</td>\n",
       "      <td>07,July,2004</td>\n",
       "      <td>I was reminded just now of the time Ashley and...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[I, was, reminded, just, now, of, the, time, A...</td>\n",
       "      <td>[I was reminded just now of the time Ashley an...</td>\n",
       "      <td>574</td>\n",
       "      <td>32</td>\n",
       "      <td>234</td>\n",
       "      <td>[I, reminded, time, Ashley, I, drove, Kemah, w...</td>\n",
       "      <td>[i, remind, time, ashley, i, drove, kemah, win...</td>\n",
       "      <td>[I, reminded, time, Ashley, I, drove, Kemah, w...</td>\n",
       "      <td>I was reminded just now of the time Ashley and...</td>\n",
       "      <td>32</td>\n",
       "      <td>17.94</td>\n",
       "      <td>female</td>\n",
       "      <td>17</td>\n",
       "      <td>Student</td>\n",
       "      <td>Gemini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3764</th>\n",
       "      <td>4187211</td>\n",
       "      <td>20,August,2004</td>\n",
       "      <td>I was checking up on my cousin Dylan and Fanni...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[I, was, checking, up, on, my, cousin, Dylan, ...</td>\n",
       "      <td>[\"I was checking up on my cousin Dylan and Fan...</td>\n",
       "      <td>84</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>[I, checking, cousin, Dylan, Fannie, \"s\", wedd...</td>\n",
       "      <td>[i, check, cousin, dylan, fanni, \"s\", wed, sit...</td>\n",
       "      <td>[I, checking, cousin, Dylan, Fannie, \"s\", wedd...</td>\n",
       "      <td>I was checking up on my cousin Dylan and Fanni...</td>\n",
       "      <td>3</td>\n",
       "      <td>28.00</td>\n",
       "      <td>female</td>\n",
       "      <td>23</td>\n",
       "      <td>Student</td>\n",
       "      <td>Taurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162953</th>\n",
       "      <td>3686696</td>\n",
       "      <td>24,June,2004</td>\n",
       "      <td>for the NME interview click urlLink part 1 and...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[for, the, NME, interview, click, urlLink, par...</td>\n",
       "      <td>[for the NME interview click urlLink part 1 an...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[NME, interview, click, urlLink, part, 1, urlL...</td>\n",
       "      <td>[nme, interview, click, urllink, part, 1, urll...</td>\n",
       "      <td>[NME, interview, click, urlLink, part, 1, urlL...</td>\n",
       "      <td>for the NME interview click urlLink part 1 and...</td>\n",
       "      <td>1</td>\n",
       "      <td>12.00</td>\n",
       "      <td>female</td>\n",
       "      <td>23</td>\n",
       "      <td>Student</td>\n",
       "      <td>Aquarius</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id            date  \\\n",
       "445164  2970791     25,May,2004   \n",
       "293091  3931851  02,agosto,2004   \n",
       "594697  3481650    07,July,2004   \n",
       "3764    4187211  20,August,2004   \n",
       "162953  3686696    24,June,2004   \n",
       "\n",
       "                                                     text lang language_2  \\\n",
       "445164  This blog is being posted due to the fact that...   en         en   \n",
       "293091  So I have a big fucking interview tomorrow for...   en         en   \n",
       "594697  I was reminded just now of the time Ashley and...   en         en   \n",
       "3764    I was checking up on my cousin Dylan and Fanni...   en         en   \n",
       "162953  for the NME interview click urlLink part 1 and...   en         en   \n",
       "\n",
       "                                            word_tokenize  \\\n",
       "445164  [This, blog, is, being, posted, due, to, the, ...   \n",
       "293091  [So, I, have, a, big, fucking, interview, tomo...   \n",
       "594697  [I, was, reminded, just, now, of, the, time, A...   \n",
       "3764    [I, was, checking, up, on, my, cousin, Dylan, ...   \n",
       "162953  [for, the, NME, interview, click, urlLink, par...   \n",
       "\n",
       "                                            sent_tokenize  count_word  \\\n",
       "445164  [This blog is being posted due to the fact tha...          83   \n",
       "293091  [So I have a big fucking interview tomorrow fo...          88   \n",
       "594697  [I was reminded just now of the time Ashley an...         574   \n",
       "3764    [\"I was checking up on my cousin Dylan and Fan...          84   \n",
       "162953  [for the NME interview click urlLink part 1 an...          12   \n",
       "\n",
       "        count_sent  word_tokenize_num_of_stopwords  \\\n",
       "445164           4                              34   \n",
       "293091           5                              31   \n",
       "594697          32                             234   \n",
       "3764             3                              32   \n",
       "162953           1                               3   \n",
       "\n",
       "                          word_tokenize_without_stopwords  \\\n",
       "445164  [This, blog, posted, due, fact, little, develo...   \n",
       "293091  [So, I, big, fucking, interview, tomorrow, new...   \n",
       "594697  [I, reminded, time, Ashley, I, drove, Kemah, w...   \n",
       "3764    [I, checking, cousin, Dylan, Fannie, \"s\", wedd...   \n",
       "162953  [NME, interview, click, urlLink, part, 1, urlL...   \n",
       "\n",
       "                     word_tokenize_without_stopwords_port  \\\n",
       "445164  [thi, blog, post, due, fact, littl, develop, h...   \n",
       "293091  [so, i, big, fuck, interview, tomorrow, new, s...   \n",
       "594697  [i, remind, time, ashley, i, drove, kemah, win...   \n",
       "3764    [i, check, cousin, dylan, fanni, \"s\", wed, sit...   \n",
       "162953  [nme, interview, click, urllink, part, 1, urll...   \n",
       "\n",
       "                                           msg_lemmatized  \\\n",
       "445164  [This, blog, posted, due, fact, little, develo...   \n",
       "293091  [So, I, big, fucking, interview, tomorrow, new...   \n",
       "594697  [I, reminded, time, Ashley, I, drove, Kemah, w...   \n",
       "3764    [I, checking, cousin, Dylan, Fannie, \"s\", wedd...   \n",
       "162953  [NME, interview, click, urlLink, part, 1, urlL...   \n",
       "\n",
       "                                               cleanLinks  laenge_saetze  \\\n",
       "445164  This blog is being posted due to the fact that...              4   \n",
       "293091  So I have a big fucking interview tomorrow for...              5   \n",
       "594697  I was reminded just now of the time Ashley and...             32   \n",
       "3764    I was checking up on my cousin Dylan and Fanni...              3   \n",
       "162953  for the NME interview click urlLink part 1 and...              1   \n",
       "\n",
       "        word_per_sent_mean  gender  age    topic         sign  \n",
       "445164               20.75    male   14  Student  Sagittarius  \n",
       "293091               17.60    male   15  Student       Pisces  \n",
       "594697               17.94  female   17  Student       Gemini  \n",
       "3764                 28.00  female   23  Student       Taurus  \n",
       "162953               12.00  female   23  Student     Aquarius  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_500.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5acfa5c39e67e2472f0526b7a591c118eeb4df36d9467199f2ee9d4e593bba17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as mltp\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import sklearn as sk\n",
    "import seaborn as sea\n",
    "import re \n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripping(liste):\n",
    "    return [i.strip() for i in liste]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data/df_tokenized_port_2.csv',converters={'sent_tokenize': lambda x: x[1:-1].strip('][').replace(\"'\",\"\").strip().split(','),'word_tokenize': lambda x: x[1:-1].strip('][').replace(\"'\",\"\").strip().split(','),\n",
    "'word_tokenize_without_stopwords': lambda x: x[1:-1].strip('][').replace(\"'\",\"\").strip().split(',')\n",
    ",'word_tokenize_without_stopwords_port': lambda x: x[1:-1].strip('][').replace(\"'\",\"\").strip().split(',')\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_tokenize']=df['word_tokenize'].apply(stripping)\n",
    "df['sent_tokenize']=df['sent_tokenize'].apply(stripping)\n",
    "df['word_tokenize_without_stopwords']=df['word_tokenize_without_stopwords'].apply(stripping)\n",
    "df['word_tokenize_without_stopwords_port']=df['word_tokenize_without_stopwords_port'].apply(stripping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>language_2</th>\n",
       "      <th>word_tokenize</th>\n",
       "      <th>sent_tokenize</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>word_tokenize_num_of_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords_port</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages and 4.5 MB ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Info, has, been, found, (, +/-, 100, pages, a...</td>\n",
       "      <td>[Info has been found (+/- 100 pages and 4.5 MB...</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>[Info, found, (, +/-, 100, pages, 4.5, MB, .pd...</td>\n",
       "      <td>[info, found, (, +/-, 100, page, 4.5, mb, .pdf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members: Drewes van der Laa...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[These, are, the, team, members, :, Drewes, va...</td>\n",
       "      <td>[These are the team members: Drewes van der La...</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[These, team, members, :, Drewes, van, der, La...</td>\n",
       "      <td>[these, team, member, :, drew, van, der, laag,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo's Toolbar I can now 'capture' ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Thanks, to, Yahoo, \"s\", Toolbar, I, can, now,...</td>\n",
       "      <td>[\"Thanks to Yahoos Toolbar I can now capture t...</td>\n",
       "      <td>77</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>[Thanks, Yahoo, \"s\", Toolbar, I, \"capture\", \"\"...</td>\n",
       "      <td>[thank, yahoo, \"s\", toolbar, i, \"captur\", \"\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>I had an interesting conversation with my Dad ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[I, had, an, interesting, conversation, with, ...</td>\n",
       "      <td>[I had an interesting conversation with my Dad...</td>\n",
       "      <td>760</td>\n",
       "      <td>36</td>\n",
       "      <td>298</td>\n",
       "      <td>[I, interesting, conversation, Dad, morning, ....</td>\n",
       "      <td>[i, interest, convers, dad, morn, ., we, talk,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>Somehow Coca-Cola has a way of summing up thin...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Somehow, Coca-Cola, has, a, way, of, summing,...</td>\n",
       "      <td>[Somehow Coca-Cola has a way of summing up thi...</td>\n",
       "      <td>226</td>\n",
       "      <td>14</td>\n",
       "      <td>91</td>\n",
       "      <td>[Somehow, Coca-Cola, way, summing, things, wel...</td>\n",
       "      <td>[somehow, coca-cola, way, sum, thing, well, .,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id gender  age              topic      sign          date  \\\n",
       "0  2059027   male   15            Student       Leo   14,May,2004   \n",
       "1  2059027   male   15            Student       Leo   13,May,2004   \n",
       "2  3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "3  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "4  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "\n",
       "                                                text lang language_2  \\\n",
       "0  Info has been found (+/- 100 pages and 4.5 MB ...   en         en   \n",
       "1  These are the team members: Drewes van der Laa...   en         en   \n",
       "2  Thanks to Yahoo's Toolbar I can now 'capture' ...   en         en   \n",
       "3  I had an interesting conversation with my Dad ...   en         en   \n",
       "4  Somehow Coca-Cola has a way of summing up thin...   en         en   \n",
       "\n",
       "                                       word_tokenize  \\\n",
       "0  [Info, has, been, found, (, +/-, 100, pages, a...   \n",
       "1  [These, are, the, team, members, :, Drewes, va...   \n",
       "2  [Thanks, to, Yahoo, \"s\", Toolbar, I, can, now,...   \n",
       "3  [I, had, an, interesting, conversation, with, ...   \n",
       "4  [Somehow, Coca-Cola, has, a, way, of, summing,...   \n",
       "\n",
       "                                       sent_tokenize  count_word  count_sent  \\\n",
       "0  [Info has been found (+/- 100 pages and 4.5 MB...          31           1   \n",
       "1  [These are the team members: Drewes van der La...          23           1   \n",
       "2  [\"Thanks to Yahoos Toolbar I can now capture t...          77           3   \n",
       "3  [I had an interesting conversation with my Dad...         760          36   \n",
       "4  [Somehow Coca-Cola has a way of summing up thi...         226          14   \n",
       "\n",
       "   word_tokenize_num_of_stopwords  \\\n",
       "0                              11   \n",
       "1                               3   \n",
       "2                              26   \n",
       "3                             298   \n",
       "4                              91   \n",
       "\n",
       "                     word_tokenize_without_stopwords  \\\n",
       "0  [Info, found, (, +/-, 100, pages, 4.5, MB, .pd...   \n",
       "1  [These, team, members, :, Drewes, van, der, La...   \n",
       "2  [Thanks, Yahoo, \"s\", Toolbar, I, \"capture\", \"\"...   \n",
       "3  [I, interesting, conversation, Dad, morning, ....   \n",
       "4  [Somehow, Coca-Cola, way, summing, things, wel...   \n",
       "\n",
       "                word_tokenize_without_stopwords_port  \n",
       "0  [info, found, (, +/-, 100, page, 4.5, mb, .pdf...  \n",
       "1  [these, team, member, :, drew, van, der, laag,...  \n",
       "2  [thank, yahoo, \"s\", toolbar, i, \"captur\", \"\", ...  \n",
       "3  [i, interest, convers, dad, morn, ., we, talk,...  \n",
       "4  [somehow, coca-cola, way, sum, thing, well, .,...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male      158043\n",
       "female    137351\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['count_sent']>1)&(df['count_sent']<20)&(df['count_word']>1)&(df['count_word']<200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Info',\n",
       " 'found',\n",
       " '(',\n",
       " '+/-',\n",
       " '100',\n",
       " 'pages',\n",
       " '4.5',\n",
       " 'MB',\n",
       " '.pdf',\n",
       " 'files',\n",
       " ')',\n",
       " 'Now',\n",
       " 'wait',\n",
       " 'untill',\n",
       " 'team',\n",
       " 'leader',\n",
       " 'processed',\n",
       " 'learns',\n",
       " 'html',\n",
       " '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['word_tokenize_without_stopwords'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "644247"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/niclascramer/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/niclascramer/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#defining the object for Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#defining the function for lemmatization\n",
    "def lemmatizer(text):\n",
    "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "    return lemm_text\n",
    "df['msg_lemmatized']=df['word_tokenize_without_stopwords'].apply(lambda x:lemmatizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>language_2</th>\n",
       "      <th>word_tokenize</th>\n",
       "      <th>sent_tokenize</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>word_tokenize_num_of_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords_port</th>\n",
       "      <th>msg_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages and 4.5 MB ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Info, has, been, found, (, +/-, 100, pages, a...</td>\n",
       "      <td>[Info has been found (+/- 100 pages and 4.5 MB...</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>[Info, found, (, +/-, 100, pages, 4.5, MB, .pd...</td>\n",
       "      <td>[info, found, (, +/-, 100, page, 4.5, mb, .pdf...</td>\n",
       "      <td>[Info, found, (, +/-, 100, page, 4.5, MB, .pdf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members: Drewes van der Laa...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[These, are, the, team, members, :, Drewes, va...</td>\n",
       "      <td>[These are the team members: Drewes van der La...</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[These, team, members, :, Drewes, van, der, La...</td>\n",
       "      <td>[these, team, member, :, drew, van, der, laag,...</td>\n",
       "      <td>[These, team, member, :, Drewes, van, der, Laa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo's Toolbar I can now 'capture' ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Thanks, to, Yahoo, \"s\", Toolbar, I, can, now,...</td>\n",
       "      <td>[\"Thanks to Yahoos Toolbar I can now capture t...</td>\n",
       "      <td>77</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>[Thanks, Yahoo, \"s\", Toolbar, I, \"capture\", \"\"...</td>\n",
       "      <td>[thank, yahoo, \"s\", toolbar, i, \"captur\", \"\", ...</td>\n",
       "      <td>[Thanks, Yahoo, \"s\", Toolbar, I, \"capture\", \"\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>I had an interesting conversation with my Dad ...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[I, had, an, interesting, conversation, with, ...</td>\n",
       "      <td>[I had an interesting conversation with my Dad...</td>\n",
       "      <td>760</td>\n",
       "      <td>36</td>\n",
       "      <td>298</td>\n",
       "      <td>[I, interesting, conversation, Dad, morning, ....</td>\n",
       "      <td>[i, interest, convers, dad, morn, ., we, talk,...</td>\n",
       "      <td>[I, interesting, conversation, Dad, morning, ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>10,June,2004</td>\n",
       "      <td>Somehow Coca-Cola has a way of summing up thin...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>[Somehow, Coca-Cola, has, a, way, of, summing,...</td>\n",
       "      <td>[Somehow Coca-Cola has a way of summing up thi...</td>\n",
       "      <td>226</td>\n",
       "      <td>14</td>\n",
       "      <td>91</td>\n",
       "      <td>[Somehow, Coca-Cola, way, summing, things, wel...</td>\n",
       "      <td>[somehow, coca-cola, way, sum, thing, well, .,...</td>\n",
       "      <td>[Somehow, Coca-Cola, way, summing, thing, well...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id gender  age              topic      sign          date  \\\n",
       "0  2059027   male   15            Student       Leo   14,May,2004   \n",
       "1  2059027   male   15            Student       Leo   13,May,2004   \n",
       "2  3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "3  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "4  3581210   male   33  InvestmentBanking  Aquarius  10,June,2004   \n",
       "\n",
       "                                                text lang language_2  \\\n",
       "0  Info has been found (+/- 100 pages and 4.5 MB ...   en         en   \n",
       "1  These are the team members: Drewes van der Laa...   en         en   \n",
       "2  Thanks to Yahoo's Toolbar I can now 'capture' ...   en         en   \n",
       "3  I had an interesting conversation with my Dad ...   en         en   \n",
       "4  Somehow Coca-Cola has a way of summing up thin...   en         en   \n",
       "\n",
       "                                       word_tokenize  \\\n",
       "0  [Info, has, been, found, (, +/-, 100, pages, a...   \n",
       "1  [These, are, the, team, members, :, Drewes, va...   \n",
       "2  [Thanks, to, Yahoo, \"s\", Toolbar, I, can, now,...   \n",
       "3  [I, had, an, interesting, conversation, with, ...   \n",
       "4  [Somehow, Coca-Cola, has, a, way, of, summing,...   \n",
       "\n",
       "                                       sent_tokenize  count_word  count_sent  \\\n",
       "0  [Info has been found (+/- 100 pages and 4.5 MB...          31           1   \n",
       "1  [These are the team members: Drewes van der La...          23           1   \n",
       "2  [\"Thanks to Yahoos Toolbar I can now capture t...          77           3   \n",
       "3  [I had an interesting conversation with my Dad...         760          36   \n",
       "4  [Somehow Coca-Cola has a way of summing up thi...         226          14   \n",
       "\n",
       "   word_tokenize_num_of_stopwords  \\\n",
       "0                              11   \n",
       "1                               3   \n",
       "2                              26   \n",
       "3                             298   \n",
       "4                              91   \n",
       "\n",
       "                     word_tokenize_without_stopwords  \\\n",
       "0  [Info, found, (, +/-, 100, pages, 4.5, MB, .pd...   \n",
       "1  [These, team, members, :, Drewes, van, der, La...   \n",
       "2  [Thanks, Yahoo, \"s\", Toolbar, I, \"capture\", \"\"...   \n",
       "3  [I, interesting, conversation, Dad, morning, ....   \n",
       "4  [Somehow, Coca-Cola, way, summing, things, wel...   \n",
       "\n",
       "                word_tokenize_without_stopwords_port  \\\n",
       "0  [info, found, (, +/-, 100, page, 4.5, mb, .pdf...   \n",
       "1  [these, team, member, :, drew, van, der, laag,...   \n",
       "2  [thank, yahoo, \"s\", toolbar, i, \"captur\", \"\", ...   \n",
       "3  [i, interest, convers, dad, morn, ., we, talk,...   \n",
       "4  [somehow, coca-cola, way, sum, thing, well, .,...   \n",
       "\n",
       "                                      msg_lemmatized  \n",
       "0  [Info, found, (, +/-, 100, page, 4.5, MB, .pdf...  \n",
       "1  [These, team, member, :, Drewes, van, der, Laa...  \n",
       "2  [Thanks, Yahoo, \"s\", Toolbar, I, \"capture\", \"\"...  \n",
       "3  [I, interesting, conversation, Dad, morning, ....  \n",
       "4  [Somehow, Coca-Cola, way, summing, thing, well...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df['cleanLinks'] = df['text'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[~df['text'].str.contains('https:\\/\\/.*')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8bd85b9f08aaa5135c8387a0c1e096579749b0a2e64b071c4a34f95dfb0716ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

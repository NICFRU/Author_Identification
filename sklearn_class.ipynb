{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Age Prediction mit Klassifizierung\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"agetrain.csv\")\n",
        "df_vali = pd.read_csv(\"agevali.csv\")\n",
        "df_test = pd.read_csv(\"agetest.csv\")\n",
        "\n",
        "df_train.labels  = df_train.labels.apply(str)\n",
        "df_vali.labels  = df_vali.labels.apply(str)\n",
        "df_test.labels  = df_test.labels.apply(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/constantin/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "        \n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "    text = BeautifulSoup(text, \"html.parser\").text # HTML decoding\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
        "    return text\n",
        "    \n",
        "df_train['text'] = df_train['text'].apply(clean_text)\n",
        "df_vali['text'] = df_vali['text'].apply(clean_text)\n",
        "df_test['text'] = df_test['text'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train = df_train['text']\n",
        "y_train = df_train['labels']\n",
        "x_test = df_vali['text']\n",
        "y_test = df_vali['labels']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "wordvecpipe = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Models Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Naives Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('prevec',\n",
              "                 Pipeline(steps=[('vect', CountVectorizer()),\n",
              "                                 ('tfidf', TfidfTransformer())])),\n",
              "                ('model', MultinomialNB())])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipe = Pipeline([\n",
        "    ('prevec',wordvecpipe),\n",
        "    ('model', MultinomialNB())\n",
        "])\n",
        "\n",
        "pipe.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acc: 0.19621913580246914\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           5       0.30      0.38      0.34      1620\n",
            "           1       0.21      0.35      0.26      1620\n",
            "           3       0.13      0.13      0.13      1620\n",
            "           2       0.16      0.16      0.16      1620\n",
            "           0       0.15      0.10      0.12      1620\n",
            "           4       0.17      0.09      0.12      1620\n",
            "           7       0.24      0.17      0.20      1620\n",
            "           6       0.16      0.18      0.17      1620\n",
            "\n",
            "    accuracy                           0.20     12960\n",
            "   macro avg       0.19      0.20      0.19     12960\n",
            "weighted avg       0.19      0.20      0.19     12960\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred = pipe.predict(x_test)\n",
        "\n",
        "print(f'Acc: {accuracy_score(y_test, y_pred)}')\n",
        "print(classification_report(y_test, y_pred,target_names=y_train.unique()))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SGDClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
            "[CV 1/5] END model__l1_ratio=0.1, model__loss=modified_huber, model__penalty=elasticnet;, score=0.187 total time=   4.3s\n",
            "[CV 3/5] END model__l1_ratio=0.1, model__loss=modified_huber, model__penalty=elasticnet;, score=0.253 total time=   4.4s\n",
            "[CV 2/5] END model__l1_ratio=0.1, model__loss=modified_huber, model__penalty=elasticnet;, score=0.195 total time=   4.5s\n",
            "[CV 4/5] END model__l1_ratio=0.1, model__loss=modified_huber, model__penalty=elasticnet;, score=0.274 total time=   4.7s\n",
            "[CV 5/5] END model__l1_ratio=0.1, model__loss=modified_huber, model__penalty=elasticnet;, score=0.223 total time=   4.8s\n",
            "[CV 1/5] END model__l1_ratio=0.15, model__loss=modified_huber, model__penalty=elasticnet;, score=0.185 total time=   4.8s\n",
            "[CV 2/5] END model__l1_ratio=0.15, model__loss=modified_huber, model__penalty=elasticnet;, score=0.193 total time=   4.0s\n",
            "[CV 4/5] END model__l1_ratio=0.15, model__loss=modified_huber, model__penalty=elasticnet;, score=0.272 total time=   3.8s\n",
            "[CV 3/5] END model__l1_ratio=0.15, model__loss=modified_huber, model__penalty=elasticnet;, score=0.256 total time=   3.9s\n",
            "[CV 5/5] END model__l1_ratio=0.15, model__loss=modified_huber, model__penalty=elasticnet;, score=0.223 total time=   2.7s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[('prevec',\n",
              "                                        Pipeline(steps=[('vect',\n",
              "                                                         CountVectorizer()),\n",
              "                                                        ('tfidf',\n",
              "                                                         TfidfTransformer())])),\n",
              "                                       ('model',\n",
              "                                        SGDClassifier(random_state=42))]),\n",
              "             n_jobs=3,\n",
              "             param_grid={'model__l1_ratio': [0.1, 0.15],\n",
              "                         'model__loss': ['modified_huber'],\n",
              "                         'model__penalty': ['elasticnet']},\n",
              "             scoring='accuracy', verbose=4)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipe = Pipeline([\n",
        "    ('prevec',wordvecpipe),\n",
        "    ('model', SGDClassifier(random_state=RANDOM_SEED))\n",
        "])\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=pipe, \n",
        "    param_grid={\n",
        "                'model__loss' : ['modified_huber'], # unnötige gelöscht nach durchführung des gridsearchs mit allen loss\n",
        "                'model__penalty' : ['elasticnet'], # ebenfall unnötige gelöscht\n",
        "                'model__l1_ratio':[0.1,0.15]\n",
        "                },\n",
        "    cv=5,\n",
        "    n_jobs=3,\n",
        "    verbose=4,\n",
        "    scoring='accuracy',\n",
        "    refit=True\n",
        ")\n",
        "grid.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model parameters: {'model__l1_ratio': 0.1, 'model__loss': 'modified_huber', 'model__penalty': 'elasticnet'}\n",
            "Best score: 0.22621527777777778\n"
          ]
        }
      ],
      "source": [
        "print(f\"Best model parameters: {grid.best_params_}\")\n",
        "print(f\"Best score: {grid.best_score_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acc: 0.1882716049382716\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           5       0.29      0.38      0.33      1620\n",
            "           1       0.21      0.23      0.22      1620\n",
            "           3       0.14      0.14      0.14      1620\n",
            "           2       0.15      0.14      0.15      1620\n",
            "           0       0.15      0.17      0.16      1620\n",
            "           4       0.16      0.12      0.14      1620\n",
            "           7       0.21      0.20      0.21      1620\n",
            "           6       0.16      0.12      0.13      1620\n",
            "\n",
            "    accuracy                           0.19     12960\n",
            "   macro avg       0.18      0.19      0.18     12960\n",
            "weighted avg       0.18      0.19      0.18     12960\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred = grid.predict(x_test)\n",
        "\n",
        "print(f'Acc: {accuracy_score(y_test, y_pred)}')\n",
        "print(classification_report(y_test, y_pred,target_names=y_train.unique()))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "[CV 1/5] END model__dual=True, model__max_iter=100, model__penalty=l2;, score=nan total time=   0.8s\n",
            "[CV 3/5] END model__dual=True, model__max_iter=100, model__penalty=l2;, score=nan total time=   0.8s\n",
            "[CV 2/5] END model__dual=True, model__max_iter=100, model__penalty=l2;, score=nan total time=   0.9s\n",
            "[CV 4/5] END model__dual=True, model__max_iter=100, model__penalty=l2;, score=nan total time=   0.8s\n",
            "[CV 5/5] END model__dual=True, model__max_iter=100, model__penalty=l2;, score=nan total time=   0.8s\n",
            "[CV 1/5] END model__dual=True, model__max_iter=100, model__penalty=l1;, score=nan total time=   0.8s\n",
            "[CV 2/5] END model__dual=True, model__max_iter=100, model__penalty=l1;, score=nan total time=   0.8s\n",
            "[CV 4/5] END model__dual=True, model__max_iter=100, model__penalty=l1;, score=nan total time=   0.7s\n",
            "[CV 3/5] END model__dual=True, model__max_iter=100, model__penalty=l1;, score=nan total time=   0.8s\n",
            "[CV 5/5] END model__dual=True, model__max_iter=100, model__penalty=l1;, score=nan total time=   0.8s\n",
            "[CV 1/5] END model__dual=True, model__max_iter=100, model__penalty=elasticnet;, score=nan total time=   0.8s\n",
            "[CV 2/5] END model__dual=True, model__max_iter=100, model__penalty=elasticnet;, score=nan total time=   0.8s\n",
            "[CV 3/5] END model__dual=True, model__max_iter=100, model__penalty=elasticnet;, score=nan total time=   0.8s\n",
            "[CV 4/5] END model__dual=True, model__max_iter=100, model__penalty=elasticnet;, score=nan total time=   0.7s\n",
            "[CV 5/5] END model__dual=True, model__max_iter=100, model__penalty=elasticnet;, score=nan total time=   0.8s\n",
            "[CV 1/5] END model__dual=True, model__max_iter=500, model__penalty=l2;, score=nan total time=   0.8s\n",
            "[CV 2/5] END model__dual=True, model__max_iter=500, model__penalty=l2;, score=nan total time=   0.8s\n",
            "[CV 3/5] END model__dual=True, model__max_iter=500, model__penalty=l2;, score=nan total time=   0.8s\n",
            "[CV 4/5] END model__dual=True, model__max_iter=500, model__penalty=l2;, score=nan total time=   0.8s\n",
            "[CV 5/5] END model__dual=True, model__max_iter=500, model__penalty=l2;, score=nan total time=   0.8s\n",
            "[CV 1/5] END model__dual=True, model__max_iter=500, model__penalty=l1;, score=nan total time=   0.8s\n",
            "[CV 2/5] END model__dual=True, model__max_iter=500, model__penalty=l1;, score=nan total time=   0.9s\n",
            "[CV 3/5] END model__dual=True, model__max_iter=500, model__penalty=l1;, score=nan total time=   0.9s\n",
            "[CV 4/5] END model__dual=True, model__max_iter=500, model__penalty=l1;, score=nan total time=   0.8s\n",
            "[CV 5/5] END model__dual=True, model__max_iter=500, model__penalty=l1;, score=nan total time=   0.9s\n",
            "[CV 1/5] END model__dual=True, model__max_iter=500, model__penalty=elasticnet;, score=nan total time=   0.9s\n",
            "[CV 2/5] END model__dual=True, model__max_iter=500, model__penalty=elasticnet;, score=nan total time=   0.9s\n",
            "[CV 3/5] END model__dual=True, model__max_iter=500, model__penalty=elasticnet;, score=nan total time=   0.8s\n",
            "[CV 4/5] END model__dual=True, model__max_iter=500, model__penalty=elasticnet;, score=nan total time=   0.8s\n",
            "[CV 5/5] END model__dual=True, model__max_iter=500, model__penalty=elasticnet;, score=nan total time=   0.9s\n",
            "[CV 1/5] END model__dual=True, model__max_iter=1000, model__penalty=l2;, score=nan total time=   0.8s\n",
            "[CV 2/5] END model__dual=True, model__max_iter=1000, model__penalty=l2;, score=nan total time=   0.9s\n",
            "[CV 3/5] END model__dual=True, model__max_iter=1000, model__penalty=l2;, score=nan total time=   0.8s\n",
            "[CV 4/5] END model__dual=True, model__max_iter=1000, model__penalty=l2;, score=nan total time=   0.8s\n",
            "[CV 5/5] END model__dual=True, model__max_iter=1000, model__penalty=l2;, score=nan total time=   0.9s\n",
            "[CV 1/5] END model__dual=True, model__max_iter=1000, model__penalty=l1;, score=nan total time=   0.9s\n",
            "[CV 2/5] END model__dual=True, model__max_iter=1000, model__penalty=l1;, score=nan total time=   1.1s\n",
            "[CV 3/5] END model__dual=True, model__max_iter=1000, model__penalty=l1;, score=nan total time=   1.1s\n",
            "[CV 4/5] END model__dual=True, model__max_iter=1000, model__penalty=l1;, score=nan total time=   1.0s\n",
            "[CV 5/5] END model__dual=True, model__max_iter=1000, model__penalty=l1;, score=nan total time=   1.0s\n",
            "[CV 1/5] END model__dual=True, model__max_iter=1000, model__penalty=elasticnet;, score=nan total time=   1.0s\n",
            "[CV 2/5] END model__dual=True, model__max_iter=1000, model__penalty=elasticnet;, score=nan total time=   1.0s\n",
            "[CV 3/5] END model__dual=True, model__max_iter=1000, model__penalty=elasticnet;, score=nan total time=   0.8s\n",
            "[CV 4/5] END model__dual=True, model__max_iter=1000, model__penalty=elasticnet;, score=nan total time=   0.8s\n",
            "[CV 5/5] END model__dual=True, model__max_iter=1000, model__penalty=elasticnet;, score=nan total time=   0.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 1/5] END model__dual=False, model__max_iter=100, model__penalty=l2;, score=0.190 total time=  16.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 3/5] END model__dual=False, model__max_iter=100, model__penalty=l2;, score=0.256 total time=  16.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 2/5] END model__dual=False, model__max_iter=100, model__penalty=l2;, score=0.191 total time=  16.7s\n",
            "[CV 1/5] END model__dual=False, model__max_iter=100, model__penalty=l1;, score=nan total time=   0.8s\n",
            "[CV 2/5] END model__dual=False, model__max_iter=100, model__penalty=l1;, score=nan total time=   1.1s\n",
            "[CV 3/5] END model__dual=False, model__max_iter=100, model__penalty=l1;, score=nan total time=   1.1s\n",
            "[CV 4/5] END model__dual=False, model__max_iter=100, model__penalty=l1;, score=nan total time=   1.0s\n",
            "[CV 5/5] END model__dual=False, model__max_iter=100, model__penalty=l1;, score=nan total time=   1.3s\n",
            "[CV 1/5] END model__dual=False, model__max_iter=100, model__penalty=elasticnet;, score=nan total time=   1.4s\n",
            "[CV 2/5] END model__dual=False, model__max_iter=100, model__penalty=elasticnet;, score=nan total time=   2.0s\n",
            "[CV 3/5] END model__dual=False, model__max_iter=100, model__penalty=elasticnet;, score=nan total time=   1.6s\n",
            "[CV 4/5] END model__dual=False, model__max_iter=100, model__penalty=elasticnet;, score=nan total time=   1.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 4/5] END model__dual=False, model__max_iter=100, model__penalty=l2;, score=0.281 total time=  13.5s\n",
            "[CV 5/5] END model__dual=False, model__max_iter=100, model__penalty=elasticnet;, score=nan total time=   1.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV 5/5] END model__dual=False, model__max_iter=100, model__penalty=l2;, score=0.228 total time=  15.4s\n",
            "[CV 1/5] END model__dual=False, model__max_iter=500, model__penalty=l2;, score=0.191 total time=  27.7s\n",
            "[CV 2/5] END model__dual=False, model__max_iter=500, model__penalty=l2;, score=0.190 total time=  31.8s\n",
            "[CV 3/5] END model__dual=False, model__max_iter=500, model__penalty=l2;, score=0.257 total time=  30.3s\n",
            "[CV 1/5] END model__dual=False, model__max_iter=500, model__penalty=l1;, score=nan total time=   0.9s\n",
            "[CV 2/5] END model__dual=False, model__max_iter=500, model__penalty=l1;, score=nan total time=   1.0s\n",
            "[CV 3/5] END model__dual=False, model__max_iter=500, model__penalty=l1;, score=nan total time=   1.1s\n",
            "[CV 4/5] END model__dual=False, model__max_iter=500, model__penalty=l1;, score=nan total time=   1.0s\n",
            "[CV 5/5] END model__dual=False, model__max_iter=500, model__penalty=l1;, score=nan total time=   1.1s\n",
            "[CV 1/5] END model__dual=False, model__max_iter=500, model__penalty=elasticnet;, score=nan total time=   1.1s\n",
            "[CV 2/5] END model__dual=False, model__max_iter=500, model__penalty=elasticnet;, score=nan total time=   1.1s\n",
            "[CV 3/5] END model__dual=False, model__max_iter=500, model__penalty=elasticnet;, score=nan total time=   1.1s\n",
            "[CV 4/5] END model__dual=False, model__max_iter=500, model__penalty=elasticnet;, score=nan total time=   1.0s\n",
            "[CV 5/5] END model__dual=False, model__max_iter=500, model__penalty=elasticnet;, score=nan total time=   1.1s\n",
            "[CV 4/5] END model__dual=False, model__max_iter=500, model__penalty=l2;, score=0.280 total time=  23.0s\n",
            "[CV 5/5] END model__dual=False, model__max_iter=500, model__penalty=l2;, score=0.227 total time=  28.8s\n",
            "[CV 1/5] END model__dual=False, model__max_iter=1000, model__penalty=l2;, score=0.191 total time=  31.3s\n",
            "[CV 2/5] END model__dual=False, model__max_iter=1000, model__penalty=l2;, score=0.190 total time=  34.5s\n",
            "[CV 3/5] END model__dual=False, model__max_iter=1000, model__penalty=l2;, score=0.257 total time=  34.8s\n",
            "[CV 1/5] END model__dual=False, model__max_iter=1000, model__penalty=l1;, score=nan total time=   1.2s\n",
            "[CV 2/5] END model__dual=False, model__max_iter=1000, model__penalty=l1;, score=nan total time=   1.2s\n",
            "[CV 3/5] END model__dual=False, model__max_iter=1000, model__penalty=l1;, score=nan total time=   1.2s\n",
            "[CV 4/5] END model__dual=False, model__max_iter=1000, model__penalty=l1;, score=nan total time=   1.3s\n",
            "[CV 5/5] END model__dual=False, model__max_iter=1000, model__penalty=l1;, score=nan total time=   1.2s\n",
            "[CV 4/5] END model__dual=False, model__max_iter=1000, model__penalty=l2;, score=0.280 total time=  27.2s\n",
            "[CV 1/5] END model__dual=False, model__max_iter=1000, model__penalty=elasticnet;, score=nan total time=   0.9s\n",
            "[CV 2/5] END model__dual=False, model__max_iter=1000, model__penalty=elasticnet;, score=nan total time=   1.0s\n",
            "[CV 3/5] END model__dual=False, model__max_iter=1000, model__penalty=elasticnet;, score=nan total time=   1.0s\n",
            "[CV 4/5] END model__dual=False, model__max_iter=1000, model__penalty=elasticnet;, score=nan total time=   0.9s\n",
            "[CV 5/5] END model__dual=False, model__max_iter=1000, model__penalty=elasticnet;, score=nan total time=   0.9s\n",
            "[CV 5/5] END model__dual=False, model__max_iter=1000, model__penalty=l2;, score=0.227 total time=  27.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "75 fits failed out of a total of 90.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 452, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "30 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan 0.22901235        nan        nan\n",
            " 0.22887731        nan        nan 0.2289159         nan        nan]\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[('prevec',\n",
              "                                        Pipeline(steps=[('vect',\n",
              "                                                         CountVectorizer()),\n",
              "                                                        ('tfidf',\n",
              "                                                         TfidfTransformer())])),\n",
              "                                       ('model',\n",
              "                                        LogisticRegression(random_state=42))]),\n",
              "             n_jobs=3,\n",
              "             param_grid={'model__dual': [True, False],\n",
              "                         'model__max_iter': [100, 500, 1000],\n",
              "                         'model__penalty': ['l2', 'l1', 'elasticnet']},\n",
              "             scoring='accuracy', verbose=4)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipe = Pipeline([\n",
        "    ('prevec',wordvecpipe),\n",
        "    ('model', LogisticRegression(random_state=RANDOM_SEED))\n",
        "])\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=pipe, \n",
        "    param_grid={'model__dual': [True,False],\n",
        "                'model__penalty' : ['l2', 'l1', 'elasticnet'],\n",
        "                'model__max_iter' : [100,500,1000]\n",
        "                },\n",
        "    cv=5,\n",
        "    n_jobs=3,\n",
        "    verbose=4,\n",
        "    scoring='accuracy',\n",
        "    refit=True\n",
        ")\n",
        "grid.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model parameters: {'model__dual': False, 'model__max_iter': 100, 'model__penalty': 'l2'}\n",
            "Best score: 0.22901234567901235\n"
          ]
        }
      ],
      "source": [
        "print(f\"Best model parameters: {grid.best_params_}\")\n",
        "print(f\"Best score: {grid.best_score_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acc: 0.19027777777777777\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           5       0.32      0.34      0.33      1620\n",
            "           1       0.22      0.26      0.24      1620\n",
            "           3       0.15      0.15      0.15      1620\n",
            "           2       0.15      0.16      0.16      1620\n",
            "           0       0.14      0.16      0.15      1620\n",
            "           4       0.16      0.13      0.15      1620\n",
            "           7       0.21      0.21      0.21      1620\n",
            "           6       0.15      0.10      0.12      1620\n",
            "\n",
            "    accuracy                           0.19     12960\n",
            "   macro avg       0.19      0.19      0.19     12960\n",
            "weighted avg       0.19      0.19      0.19     12960\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred = grid.predict(x_test)\n",
        "\n",
        "print(f'Acc: {accuracy_score(y_test, y_pred)}')\n",
        "print(classification_report(y_test, y_pred,target_names=y_train.unique()))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
            "[CV 1/5] END model__max_depth=2, model__n_estimators=200;, score=0.166 total time=   2.6s\n",
            "[CV 2/5] END model__max_depth=2, model__n_estimators=200;, score=0.160 total time=   2.6s\n",
            "[CV 3/5] END model__max_depth=2, model__n_estimators=200;, score=0.230 total time=   2.6s\n",
            "[CV 4/5] END model__max_depth=2, model__n_estimators=200;, score=0.223 total time=   2.4s\n",
            "[CV 5/5] END model__max_depth=2, model__n_estimators=200;, score=0.164 total time=   2.4s\n",
            "[CV 1/5] END model__max_depth=2, model__n_estimators=300;, score=0.166 total time=   3.1s\n",
            "[CV 2/5] END model__max_depth=2, model__n_estimators=300;, score=0.160 total time=   3.1s\n",
            "[CV 3/5] END model__max_depth=2, model__n_estimators=300;, score=0.228 total time=   3.1s\n",
            "[CV 4/5] END model__max_depth=2, model__n_estimators=300;, score=0.230 total time=   3.0s\n",
            "[CV 5/5] END model__max_depth=2, model__n_estimators=300;, score=0.169 total time=   3.0s\n",
            "[CV 1/5] END model__max_depth=2, model__n_estimators=2000;, score=0.172 total time=  14.3s\n",
            "[CV 2/5] END model__max_depth=2, model__n_estimators=2000;, score=0.166 total time=  14.3s\n",
            "[CV 3/5] END model__max_depth=2, model__n_estimators=2000;, score=0.233 total time=  14.4s\n",
            "[CV 1/5] END model__max_depth=5, model__n_estimators=200;, score=0.169 total time=   2.8s\n",
            "[CV 2/5] END model__max_depth=5, model__n_estimators=200;, score=0.164 total time=   2.9s\n",
            "[CV 3/5] END model__max_depth=5, model__n_estimators=200;, score=0.228 total time=   2.8s\n",
            "[CV 4/5] END model__max_depth=2, model__n_estimators=2000;, score=0.233 total time=  14.5s\n",
            "[CV 4/5] END model__max_depth=5, model__n_estimators=200;, score=0.217 total time=   2.8s\n",
            "[CV 5/5] END model__max_depth=2, model__n_estimators=2000;, score=0.181 total time=  14.4s\n",
            "[CV 5/5] END model__max_depth=5, model__n_estimators=200;, score=0.169 total time=   2.8s\n",
            "[CV 1/5] END model__max_depth=5, model__n_estimators=300;, score=0.170 total time=   3.6s\n",
            "[CV 2/5] END model__max_depth=5, model__n_estimators=300;, score=0.168 total time=   3.6s\n",
            "[CV 3/5] END model__max_depth=5, model__n_estimators=300;, score=0.229 total time=   3.6s\n",
            "[CV 4/5] END model__max_depth=5, model__n_estimators=300;, score=0.213 total time=   3.5s\n",
            "[CV 5/5] END model__max_depth=5, model__n_estimators=300;, score=0.176 total time=   3.6s\n",
            "[CV 1/5] END model__max_depth=5, model__n_estimators=2000;, score=0.170 total time=  17.2s\n",
            "[CV 2/5] END model__max_depth=5, model__n_estimators=2000;, score=0.165 total time=  17.3s\n",
            "[CV 3/5] END model__max_depth=5, model__n_estimators=2000;, score=0.233 total time=  17.3s\n",
            "[CV 1/5] END model__max_depth=10, model__n_estimators=200;, score=0.171 total time=   3.6s\n",
            "[CV 2/5] END model__max_depth=10, model__n_estimators=200;, score=0.165 total time=   3.7s\n",
            "[CV 3/5] END model__max_depth=10, model__n_estimators=200;, score=0.232 total time=   3.6s\n",
            "[CV 4/5] END model__max_depth=10, model__n_estimators=200;, score=0.229 total time=   3.7s\n",
            "[CV 4/5] END model__max_depth=5, model__n_estimators=2000;, score=0.242 total time=  17.5s\n",
            "[CV 5/5] END model__max_depth=5, model__n_estimators=2000;, score=0.182 total time=  17.5s\n",
            "[CV 5/5] END model__max_depth=10, model__n_estimators=200;, score=0.179 total time=   3.8s\n",
            "[CV 1/5] END model__max_depth=10, model__n_estimators=300;, score=0.173 total time=   5.0s\n",
            "[CV 2/5] END model__max_depth=10, model__n_estimators=300;, score=0.168 total time=   5.0s\n",
            "[CV 3/5] END model__max_depth=10, model__n_estimators=300;, score=0.233 total time=   4.9s\n",
            "[CV 4/5] END model__max_depth=10, model__n_estimators=300;, score=0.233 total time=   4.7s\n",
            "[CV 5/5] END model__max_depth=10, model__n_estimators=300;, score=0.183 total time=   4.8s\n",
            "[CV 1/5] END model__max_depth=10, model__n_estimators=2000;, score=0.177 total time=  26.7s\n",
            "[CV 2/5] END model__max_depth=10, model__n_estimators=2000;, score=0.169 total time=  26.6s\n",
            "[CV 3/5] END model__max_depth=10, model__n_estimators=2000;, score=0.236 total time=  26.8s\n",
            "[CV 4/5] END model__max_depth=10, model__n_estimators=2000;, score=0.251 total time=  24.9s\n",
            "[CV 5/5] END model__max_depth=10, model__n_estimators=2000;, score=0.186 total time=  25.4s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[('prevec',\n",
              "                                        Pipeline(steps=[('vect',\n",
              "                                                         CountVectorizer()),\n",
              "                                                        ('tfidf',\n",
              "                                                         TfidfTransformer())])),\n",
              "                                       ('model',\n",
              "                                        RandomForestClassifier(random_state=42))]),\n",
              "             n_jobs=3,\n",
              "             param_grid={'model__max_depth': [2, 5, 10],\n",
              "                         'model__n_estimators': [200, 300, 2000]},\n",
              "             scoring='accuracy', verbose=4)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipe = Pipeline([\n",
        "    ('prevec',wordvecpipe),\n",
        "    ('model', RandomForestClassifier(random_state=RANDOM_SEED))\n",
        "])\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=pipe, \n",
        "    param_grid={\n",
        "                'model__max_depth':[2,5,10],\n",
        "                'model__n_estimators':[200,300,2000]\n",
        "                },\n",
        "    cv=5,\n",
        "    n_jobs=3,\n",
        "    verbose=4,\n",
        "    scoring='accuracy',\n",
        "    refit=True\n",
        ")\n",
        "grid.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model parameters: {'model__max_depth': 10, 'model__n_estimators': 2000}\n",
            "Best score: 0.2036651234567901\n"
          ]
        }
      ],
      "source": [
        "print(f\"Best model parameters: {grid.best_params_}\")\n",
        "print(f\"Best score: {grid.best_score_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acc: 0.17098765432098764\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           5       0.31      0.41      0.35      1620\n",
            "           1       0.19      0.08      0.11      1620\n",
            "           3       0.12      0.12      0.12      1620\n",
            "           2       0.17      0.11      0.13      1620\n",
            "           0       0.13      0.38      0.20      1620\n",
            "           4       0.12      0.04      0.06      1620\n",
            "           7       0.20      0.17      0.18      1620\n",
            "           6       0.12      0.06      0.08      1620\n",
            "\n",
            "    accuracy                           0.17     12960\n",
            "   macro avg       0.17      0.17      0.15     12960\n",
            "weighted avg       0.17      0.17      0.15     12960\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred = grid.predict(x_test)\n",
        "\n",
        "print(f'Acc: {accuracy_score(y_test, y_pred)}')\n",
        "print(classification_report(y_test, y_pred,target_names=y_train.unique()))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stacked Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import StackingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "NaBa = ('NaBa',MultinomialNB())\n",
        "sgdc = ('SGDCl',SGDClassifier(l1_ratio= 0.1, loss= 'modified_huber', penalty= 'elasticnet'))\n",
        "logreg = LogisticRegression(dual= False, max_iter= 100, penalty= 'l2')\n",
        "\n",
        "stack = StackingClassifier(estimators=(NaBa,sgdc),\n",
        "                            final_estimator=logreg)\n",
        "                            \n",
        "pipe = Pipeline([('prevec',wordvecpipe),('stackmodel',stack)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('prevec',\n",
              "                 Pipeline(steps=[('vect', CountVectorizer()),\n",
              "                                 ('tfidf', TfidfTransformer())])),\n",
              "                ('stackmodel',\n",
              "                 StackingClassifier(estimators=(('NaBa', MultinomialNB()),\n",
              "                                                ('SGDCl',\n",
              "                                                 SGDClassifier(l1_ratio=0.1,\n",
              "                                                               loss='modified_huber',\n",
              "                                                               penalty='elasticnet'))),\n",
              "                                    final_estimator=LogisticRegression()))])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipe.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy 0.20146604938271606\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           5       0.35      0.39      0.37      1620\n",
            "           1       0.23      0.32      0.27      1620\n",
            "           3       0.15      0.16      0.15      1620\n",
            "           2       0.16      0.07      0.10      1620\n",
            "           0       0.15      0.16      0.16      1620\n",
            "           4       0.14      0.09      0.11      1620\n",
            "           7       0.22      0.22      0.22      1620\n",
            "           6       0.16      0.19      0.17      1620\n",
            "\n",
            "    accuracy                           0.20     12960\n",
            "   macro avg       0.19      0.20      0.19     12960\n",
            "weighted avg       0.19      0.20      0.19     12960\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred = pipe.predict(x_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred,target_names=y_train.unique()))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export Stacked Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['class_model.joblib']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from joblib import dump\n",
        "#dump(pipe, 'class_model.joblib')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15 (default, Nov 10 2022, 13:17:42) \n[Clang 14.0.6 ]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

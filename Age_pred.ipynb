{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "a418dfe107b14bf2ba1c348173083ad9",
            "c58beb19675d4627b8983de6a07e89f4",
            "70452e113da8477f82b0b892a72b15ca",
            "4b117decfcbf41f3822b217d51abfb5d",
            "f8e1505c1d5d4be68f35b009abc1bc32",
            "1dc12314b42a415fbc5a3e79a52f028c"
          ]
        },
        "id": "40vyXwMCRM9P",
        "outputId": "403befad-663e-4d0d-b180-c6b51c6acbae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_65352/3073776217.py:13: ParserWarning: Falling back to the 'python' engine because the separator encoded in utf-8 is > 1 char long, and the 'c' engine does not support such separators; you can avoid this warning by specifying engine='python'.\n",
            "  df_train = pd.read_csv('/home/rechcons/PP3-NLP/age_predict/train.csv',sep=\"째\")\n",
            "/tmp/ipykernel_65352/3073776217.py:14: ParserWarning: Falling back to the 'python' engine because the separator encoded in utf-8 is > 1 char long, and the 'c' engine does not support such separators; you can avoid this warning by specifying engine='python'.\n",
            "  df_test = pd.read_csv('/home/rechcons/PP3-NLP/age_predict/test.csv',sep=\"째\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "distilbert-base-uncased-finetuned-sst-2-english\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a418dfe107b14bf2ba1c348173083ad9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c58beb19675d4627b8983de6a07e89f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "2023/01/06 16:41:53 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
            "2023/01/06 16:41:53 WARNING mlflow.tracking.fluent: Exception raised while enabling autologging for tensorflow: module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'monitoring'\n",
            "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running training *****\n",
            "  Num examples = 15600\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 488\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/rechcons/PP3-NLP/.PPP-NLP-env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='488' max='488' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [488/488 02:00, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.493583</td>\n",
              "      <td>0.397436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.466644</td>\n",
              "      <td>0.411795</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3900\n",
            "  Batch size = 64\n",
            "/home/rechcons/PP3-NLP/.PPP-NLP-env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3900\n",
            "  Batch size = 64\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ProsusAI/finbert\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/ProsusAI/finbert/resolve/main/config.json from cache at /home/rechcons/.cache/huggingface/transformers/2120f4f96b5830e5a91fe94d242471b0133b0976c8d6e081594ab837ac5f17bc.ef97278c578016c8bb785f15296476b12eae86423097fed78719d1c8197a3430\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"ProsusAI/finbert\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"positive\",\n",
            "    \"1\": \"negative\",\n",
            "    \"2\": \"neutral\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"negative\": 1,\n",
            "    \"neutral\": 2,\n",
            "    \"positive\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.14.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/ProsusAI/finbert/resolve/main/vocab.txt from cache at /home/rechcons/.cache/huggingface/transformers/a5b1a5451c9cf1702eec1072ac325d4af10e675a654628eab453b8cba2c6b111.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/ProsusAI/finbert/resolve/main/tokenizer.json from cache at None\n",
            "loading file https://huggingface.co/ProsusAI/finbert/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/ProsusAI/finbert/resolve/main/special_tokens_map.json from cache at /home/rechcons/.cache/huggingface/transformers/4c21e8896b03f68c2e028133cf579267c62aba9de03a704a0845704e58eefe9e.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "loading file https://huggingface.co/ProsusAI/finbert/resolve/main/tokenizer_config.json from cache at /home/rechcons/.cache/huggingface/transformers/e3709a60694f45adca209a405cc69ce2b5d47b1cae60696ed9a901426be8c43d.8b6dccc90d16201c6d7ab0f3c6cc38e74b5f2fe587f6efadc9fa71fc0a00c606\n",
            "loading configuration file https://huggingface.co/ProsusAI/finbert/resolve/main/config.json from cache at /home/rechcons/.cache/huggingface/transformers/2120f4f96b5830e5a91fe94d242471b0133b0976c8d6e081594ab837ac5f17bc.ef97278c578016c8bb785f15296476b12eae86423097fed78719d1c8197a3430\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"ProsusAI/finbert\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"positive\",\n",
            "    \"1\": \"negative\",\n",
            "    \"2\": \"neutral\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"negative\": 1,\n",
            "    \"neutral\": 2,\n",
            "    \"positive\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.14.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/ProsusAI/finbert/resolve/main/config.json from cache at /home/rechcons/.cache/huggingface/transformers/2120f4f96b5830e5a91fe94d242471b0133b0976c8d6e081594ab837ac5f17bc.ef97278c578016c8bb785f15296476b12eae86423097fed78719d1c8197a3430\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"ProsusAI/finbert\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"positive\",\n",
            "    \"1\": \"negative\",\n",
            "    \"2\": \"neutral\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"negative\": 1,\n",
            "    \"neutral\": 2,\n",
            "    \"positive\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.14.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70452e113da8477f82b0b892a72b15ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b117decfcbf41f3822b217d51abfb5d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/ProsusAI/finbert/resolve/main/config.json from cache at /home/rechcons/.cache/huggingface/transformers/2120f4f96b5830e5a91fe94d242471b0133b0976c8d6e081594ab837ac5f17bc.ef97278c578016c8bb785f15296476b12eae86423097fed78719d1c8197a3430\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"ProsusAI/finbert\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.14.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/ProsusAI/finbert/resolve/main/pytorch_model.bin from cache at /home/rechcons/.cache/huggingface/transformers/b3ba5be9f12905cef8d1d18af435dfd568d75466fae4a117a4f20ed5faadd3e3.8764ec40d33a40810fe5d2c1e864945dcf7affafd797ed8ef1b71392bfcf8562\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ProsusAI/finbert and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "2023/01/06 16:44:26 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
            "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running training *****\n",
            "  Num examples = 15600\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 488\n",
            "/home/rechcons/PP3-NLP/.PPP-NLP-env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='488' max='488' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [488/488 03:49, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.490874</td>\n",
              "      <td>0.397692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.435749</td>\n",
              "      <td>0.425385</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3900\n",
            "  Batch size = 64\n",
            "/home/rechcons/PP3-NLP/.PPP-NLP-env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3900\n",
            "  Batch size = 64\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "j-hartmann/emotion-english-distilroberta-base\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/resolve/main/config.json from cache at /home/rechcons/.cache/huggingface/transformers/cf0d76ef122b326527bd6a56a127784dd513117bdfde0df5b011c9751bce8fd6.512111953428ded6703782e47e2f67b1c438267f7f4a7c0f19bc1850651a93b4\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"j-hartmann/emotion-english-distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"anger\",\n",
            "    \"1\": \"disgust\",\n",
            "    \"2\": \"fear\",\n",
            "    \"3\": \"joy\",\n",
            "    \"4\": \"neutral\",\n",
            "    \"5\": \"sadness\",\n",
            "    \"6\": \"surprise\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"anger\": 0,\n",
            "    \"disgust\": 1,\n",
            "    \"fear\": 2,\n",
            "    \"joy\": 3,\n",
            "    \"neutral\": 4,\n",
            "    \"sadness\": 5,\n",
            "    \"surprise\": 6\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"transformers_version\": \"4.14.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/resolve/main/vocab.json from cache at /home/rechcons/.cache/huggingface/transformers/c81beb2fff962084ecafd1e59723b32f7417ab26febe34b32d1ee518220f03d9.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
            "loading file https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/resolve/main/merges.txt from cache at /home/rechcons/.cache/huggingface/transformers/7b46710061831241eddbee3a6ba33d6b739ffbf9d1b651c365a41f47893c48a1.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
            "loading file https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/resolve/main/tokenizer.json from cache at /home/rechcons/.cache/huggingface/transformers/589f720addb5adfe0557ff3bce0e1ca6cf0f4d8def9b217b1bc57ba0eb8cab56.5e13b866659a58116cc29672daac5672bd3a95646be51fa222d6021a538cfc56\n",
            "loading file https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/resolve/main/special_tokens_map.json from cache at /home/rechcons/.cache/huggingface/transformers/61b9bf7f141e1e3d5530baf82b66d64c0e318871cfdb385a2a3b448943f24fd3.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
            "loading file https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/resolve/main/tokenizer_config.json from cache at /home/rechcons/.cache/huggingface/transformers/9c99692835590373539a08d1c7b54fa0fc25353c3ccbbc2dc3ff8b52dd509682.2ddf1b356c06d617f67df8d99c4d892198669aefe01b6afbe8c04f1b3d86abd0\n",
            "loading configuration file https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/resolve/main/config.json from cache at /home/rechcons/.cache/huggingface/transformers/cf0d76ef122b326527bd6a56a127784dd513117bdfde0df5b011c9751bce8fd6.512111953428ded6703782e47e2f67b1c438267f7f4a7c0f19bc1850651a93b4\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"j-hartmann/emotion-english-distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"anger\",\n",
            "    \"1\": \"disgust\",\n",
            "    \"2\": \"fear\",\n",
            "    \"3\": \"joy\",\n",
            "    \"4\": \"neutral\",\n",
            "    \"5\": \"sadness\",\n",
            "    \"6\": \"surprise\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"anger\": 0,\n",
            "    \"disgust\": 1,\n",
            "    \"fear\": 2,\n",
            "    \"joy\": 3,\n",
            "    \"neutral\": 4,\n",
            "    \"sadness\": 5,\n",
            "    \"surprise\": 6\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"transformers_version\": \"4.14.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8e1505c1d5d4be68f35b009abc1bc32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/16 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1dc12314b42a415fbc5a3e79a52f028c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/resolve/main/config.json from cache at /home/rechcons/.cache/huggingface/transformers/cf0d76ef122b326527bd6a56a127784dd513117bdfde0df5b011c9751bce8fd6.512111953428ded6703782e47e2f67b1c438267f7f4a7c0f19bc1850651a93b4\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"j-hartmann/emotion-english-distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"transformers_version\": \"4.14.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/resolve/main/pytorch_model.bin from cache at /home/rechcons/.cache/huggingface/transformers/dfe486a57eaa37babb1c8da1d0542143e39b66ce9e13cb5ac082a901d36b9e3c.ff670fb8c15564f094f6b848371dbdbade7105b49b47bfef9d402c8443664bd9\n",
            "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
            "\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at j-hartmann/emotion-english-distilroberta-base and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([7, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([7]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "2023/01/06 16:48:39 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
            "The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running training *****\n",
            "  Num examples = 15600\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 488\n",
            "/home/rechcons/PP3-NLP/.PPP-NLP-env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='488' max='488' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [488/488 02:07, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.467531</td>\n",
              "      <td>0.404103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.430400</td>\n",
              "      <td>0.419487</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3900\n",
            "  Batch size = 64\n",
            "/home/rechcons/PP3-NLP/.PPP-NLP-env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3900\n",
            "  Batch size = 64\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "mlist = [\n",
        "    \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    \"ProsusAI/finbert\", \n",
        "    \"j-hartmann/emotion-english-distilroberta-base\",\n",
        "    \n",
        "]\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "import mlflow\n",
        "\n",
        "\n",
        "df_train = pd.read_csv('/home/rechcons/PP3-NLP/age_predict/train.csv',sep=\"째\")\n",
        "df_test = pd.read_csv('/home/rechcons/PP3-NLP/age_predict/test.csv',sep=\"째\")\n",
        "\n",
        "\n",
        "train = Dataset.from_pandas(df_train)\n",
        "test = Dataset.from_pandas(df_test)\n",
        "\n",
        "\n",
        "dataset = DatasetDict()\n",
        "\n",
        "dataset['train'] = train\n",
        "dataset['test'] = test\n",
        "\n",
        "for x in mlist:\n",
        "  # %%\n",
        "  print(x)\n",
        "  # %%\n",
        "  from transformers import AutoTokenizer\n",
        "  model_name = x\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  # %%\n",
        "\n",
        "  def preprocess_function(examples):\n",
        "      return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "  tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "  # %%\n",
        "  from transformers import DataCollatorWithPadding\n",
        "\n",
        "  data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "  # %%\n",
        "  from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(model_name, \n",
        "                                                            num_labels=6,\n",
        "                                                            ignore_mismatched_sizes=True)\n",
        "  # %%\n",
        "  import numpy as np\n",
        "  from datasets import load_metric\n",
        "  metric = load_metric(\"accuracy\")\n",
        "  #%%\n",
        "  def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "  # %%\n",
        "  num_epochs = 2\n",
        "\n",
        "  args = TrainingArguments(\n",
        "      output_dir ='./results',          \n",
        "      num_train_epochs = num_epochs,\n",
        "      per_device_train_batch_size = 16,   \n",
        "      per_device_eval_batch_size = 16,   \n",
        "      weight_decay = 0.01,               \n",
        "      learning_rate = 2e-5,             \n",
        "      metric_for_best_model = 'accuracy',    \n",
        "      evaluation_strategy = \"epoch\",\n",
        "      )\n",
        "\n",
        "\n",
        "  trainer = Trainer(\n",
        "      model,\n",
        "      args,\n",
        "      train_dataset=tokenized_datasets['train'],\n",
        "      eval_dataset=tokenized_datasets['test'],\n",
        "      tokenizer=tokenizer,\n",
        "      data_collator=data_collator,\n",
        "      compute_metrics=compute_metrics\n",
        "  )  \n",
        "  mlflow.autolog()\n",
        "  trainer.train()\n",
        "  mlflow.log_param(\"model\",x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERGyxFU3zXY8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "NLP-New",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "06972781760676f1ec39ca0dd490c0cce4e321933a34cdf9140b46909a4aadfc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

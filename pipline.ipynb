{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as mltp\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import sklearn as sk \n",
    "import seaborn as sea\n",
    "import re \n",
    "import tqdm\n",
    "#nltk.download(\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data/df_tokenized_port.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>info has been found (+/- 100 pages,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>these are the team members:   drewe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>in het kader van kernfusie op aarde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>thanks to yahoo!'s toolbar i can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681279</th>\n",
       "      <td>1713845</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>Student</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>01,July,2004</td>\n",
       "      <td>dear susan,  i could write some really ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681280</th>\n",
       "      <td>1713845</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>Student</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>01,July,2004</td>\n",
       "      <td>dear susan,  'i have the second yeast i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681281</th>\n",
       "      <td>1713845</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>Student</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>01,July,2004</td>\n",
       "      <td>dear susan,  your 'boyfriend' is fuckin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681282</th>\n",
       "      <td>1713845</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>Student</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>01,July,2004</td>\n",
       "      <td>dear susan:    just to clarify, i am as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681283</th>\n",
       "      <td>1713845</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>Student</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>01,July,2004</td>\n",
       "      <td>hey everybody...and susan,  you might a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>681284 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id gender  age              topic      sign          date  \\\n",
       "0       2059027   male   15            Student       Leo   14,May,2004   \n",
       "1       2059027   male   15            Student       Leo   13,May,2004   \n",
       "2       2059027   male   15            Student       Leo   12,May,2004   \n",
       "3       2059027   male   15            Student       Leo   12,May,2004   \n",
       "4       3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "...         ...    ...  ...                ...       ...           ...   \n",
       "681279  1713845   male   23            Student    Taurus  01,July,2004   \n",
       "681280  1713845   male   23            Student    Taurus  01,July,2004   \n",
       "681281  1713845   male   23            Student    Taurus  01,July,2004   \n",
       "681282  1713845   male   23            Student    Taurus  01,July,2004   \n",
       "681283  1713845   male   23            Student    Taurus  01,July,2004   \n",
       "\n",
       "                                                     text  \n",
       "0                  info has been found (+/- 100 pages,...  \n",
       "1                  these are the team members:   drewe...  \n",
       "2                  in het kader van kernfusie op aarde...  \n",
       "3                        testing!!!  testing!!!            \n",
       "4                    thanks to yahoo!'s toolbar i can ...  \n",
       "...                                                   ...  \n",
       "681279         dear susan,  i could write some really ...  \n",
       "681280         dear susan,  'i have the second yeast i...  \n",
       "681281         dear susan,  your 'boyfriend' is fuckin...  \n",
       "681282         dear susan:    just to clarify, i am as...  \n",
       "681283         hey everybody...and susan,  you might a...  \n",
       "\n",
       "[681284 rows x 7 columns]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pip=df.copy()\n",
    "df_pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### removel of punktuation in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punktuation(text):\n",
    "    pattern = re.compile('[<,*?>!]')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pip['text']=df_pip['text'].apply(remove_punktuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern =re.compile('[<,*?>]')\n",
    "#pattern.search(df['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searching(text):\n",
    "    pattern = re.compile('https?://S+|www.S+')\n",
    "    return pattern.search(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_to_pip(text):\n",
    "    return re.sub(\"([.][a-z0-9])\",r'|', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spaceremove(text):\n",
    "    return \" \".join(text.split())\n",
    "df_pip['text']=df_pip['text'].apply(spaceremove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_dupplicates(text):\n",
    "    return re.sub(r'[!.]+[^a-z0-9]', '', df_pip['text'][0]) \n",
    "#df_pip['text']=df_pip['text'].apply(delete_dupplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'info has been found (+/- 100 pages and 4.5 mb of .pdf files) now i have to wait untill our team leader has processed it and learns html.'"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pip['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"U0001F600-U0001F64F\"  # emoticons\n",
    "                           u\"U0001F300-U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"U0001F680-U0001F6FF\"  # transport & map symbols\n",
    "                           u\"U0001F1E0-U0001F1FF\"  # flags (iOS)\n",
    "                           u\"U00002702-U000027B0\"\n",
    "                           u\"U000024C2-U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "#df_test['text']=df_pip['text'].apply(remove_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'info has been found (+/-  pages and . mb of .pdf files) now i have to wait untill our team leader has processed it and learns html.'"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def emoji(text):\n",
    "# emo = [\n",
    "# [u\":‑)\",\"Happy face or smiley\"],\n",
    "# [u\":)\",\"Happy face or smiley\"],\n",
    "# [u\":-]\",\"Happy face or smiley\"],\n",
    "# [u\":]\",\"Happy face or smiley\"],\n",
    "# [u\":-3\",\"Happy face smiley\"],\n",
    "# [u\":3\",\"Happy face smiley\"],\n",
    "# [u\":->\",\"Happy face smiley\"],\n",
    "# [u\":>\",\"Happy face smiley\"],\n",
    "# [u\"8-)\",\"Happy face smiley\"],\n",
    "# [u\":o)\",\"Happy face smiley\"],\n",
    "# [u\":-}\",\"Happy face smiley\"],\n",
    "# [u\":}\",\"Happy face smiley\"],\n",
    "# [u\":-)\",\"Happy face smiley\"],\n",
    "# [u\":c)\",\"Happy face smiley\"],\n",
    "# [u\":^)\",\"Happy face smiley\"],\n",
    "# [u\"=]\",\"Happy face smiley\"]\n",
    "# ]\n",
    "# for z in emo:\n",
    "#     for x,y in z:\n",
    "#         print(x,y)\n",
    "        \n",
    "            \n",
    "    #     for x,y in z: \n",
    "    #         re.sub(x,y,text)\n",
    "    # return text\n",
    "#df_pip['text']=df_pip['text'][0].apply(emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(column):\n",
    "    \"\"\"Tokenizes a Pandas dataframe column and returns a list of tokens.\n",
    "\n",
    "    Args:\n",
    "        column: Pandas dataframe column (i.e. df['text']).\n",
    "\n",
    "    Returns:\n",
    "        tokens (list): Tokenized list, i.e. [Donald, Trump, tweets]\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = nltk.word_tokenize(column)\n",
    "    return [w for w in tokens if w.isalpha()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/niclascramer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pip['text'] = df_pip['text'].astype(str)\n",
    "# df_pip['tokenized'] = df_pip.apply(lambda x: tokenize(x['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pip['word_tokenize']=df_pip.apply(lambda row: word_tokenize(row['text']), axis=1)\n",
    "df_pip['sent_tokenize']=df_pip.apply(lambda row: sent_tokenize(row['text']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pip.to_csv('Datei/df_tokenized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripping(liste):\n",
    "    return [i.strip() for i in liste]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pip['count_word']=df_pip['word_tokenize'].str.len()\n",
    "df_pip['count_sent']=df_pip['sent_tokenize'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['info',\n",
       " 'has',\n",
       " 'been',\n",
       " 'found',\n",
       " '(',\n",
       " '+/-',\n",
       " '100',\n",
       " 'pages',\n",
       " 'and',\n",
       " '4.5',\n",
       " 'mb',\n",
       " 'of',\n",
       " '.pdf',\n",
       " 'files',\n",
       " ')',\n",
       " 'now',\n",
       " 'i',\n",
       " 'have',\n",
       " 'to',\n",
       " 'wait',\n",
       " 'untill',\n",
       " 'our',\n",
       " 'team',\n",
       " 'leader',\n",
       " 'has',\n",
       " 'processed',\n",
       " 'it',\n",
       " 'and',\n",
       " 'learns',\n",
       " 'html',\n",
       " '.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=['info',\n",
    " ' has',\n",
    " ' been',\n",
    " ' found',\n",
    " ' (',\n",
    " ' +/-',\n",
    " ' 100',\n",
    " ' pages',\n",
    " ' and',\n",
    " ' 4.5',\n",
    " ' mb',\n",
    " ' of',\n",
    " ' .pdf',\n",
    " ' files',\n",
    " ' )',\n",
    " ' now',\n",
    " ' i',\n",
    " ' have',\n",
    " ' to',\n",
    " ' wait',\n",
    " ' untill',\n",
    " ' our',\n",
    " ' team',\n",
    " ' leader',\n",
    " ' has',\n",
    " ' processed',\n",
    " ' it',\n",
    " ' and',\n",
    " ' learns',\n",
    " ' html',\n",
    " ' .']\n",
    "test=[i.strip() for i in test]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['info',\n",
       " 'has',\n",
       " 'been',\n",
       " 'found',\n",
       " '(',\n",
       " '+/-',\n",
       " '100',\n",
       " 'pages',\n",
       " 'and',\n",
       " '4.5',\n",
       " 'mb',\n",
       " 'of',\n",
       " '.pdf',\n",
       " 'files',\n",
       " ')',\n",
       " 'now',\n",
       " 'i',\n",
       " 'have',\n",
       " 'to',\n",
       " 'wait',\n",
       " 'untill',\n",
       " 'our',\n",
       " 'team',\n",
       " 'leader',\n",
       " 'has',\n",
       " 'processed',\n",
       " 'it',\n",
       " 'and',\n",
       " 'learns',\n",
       " 'html',\n",
       " '.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pip['word_tokenize'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"in het kader van kernfusie op aarde: maak je eigen waterstofbom how to build an h-bomb from: ascott@tartarus.uwa.edu.au (andrew scott) newsgroups: rec.humor subject: how to build an h-bomb (humorous) date: 7 feb 1994 07:41:14 gmt organization: the university of western australia original file dated 12th november 1990. seemed to be a transcript of a seven days article.\"',\n",
       " 'poorly formatted and corrupted.',\n",
       " '\"i have added the text between examine under a microscope and malleable like gold as it was missing.\"',\n",
       " 'if anyone has the full text please distribute.',\n",
       " 'i am not responsible for the accuracy of this information.',\n",
       " 'converted to html by dionisio@infinet.com 11/13/98.',\n",
       " '(did a little spell-checking and some minor edits too.)',\n",
       " 'stolen from urllink http://my.ohio.voyager.net/~dionisio/fun/m...own-h-bomb.html and reformatted the html.',\n",
       " 'it now validates to xhtml 1.0 strict.',\n",
       " 'how to build an h-bomb making and owning an h-bomb is the kind of challenge real americans seek.',\n",
       " 'who wants to be a passive victim of nuclear war when with a little effort you can be an active participant bomb shelters are for losers.',\n",
       " 'who wants to huddle together underground eating canned spam winners want to push the button themselves.',\n",
       " '\"making your own h-bomb is a big step in nuclear assertiveness training -- its called taking charge.\"',\n",
       " '\"were sure youll enjoy the risks and the heady thrill of playing nuclear chicken.\"',\n",
       " 'introduction when the feds clamped down on the progressive magazine for attempting to publish an article on the manufacture of the hydrogen bomb it piqued our curiosity.',\n",
       " 'was it really true that atomic and hydrogen bomb technology was so simple you could build an h-bomb in your own kitchen seven days decided to find out.',\n",
       " 'food editor barbara ehrenreich investigative reporter peter biskind photographer jane melnick and nuclear scientist michio kaku were given three days to cook up a workable h-bomb.',\n",
       " 'they did and we have decided to share their culinary secrets with you.',\n",
       " 'not that seven days supports nuclear terrorism.',\n",
       " '\"we dont.\"',\n",
       " 'we would prefer to die slowly from familiar poisons like low-level radiation microwaves ddt dbcp aflatoxins pbbs pbcs or food dyes rather than unexpectedly say as hostage to a latvian nationalist brandishing a homemade bomb.',\n",
       " 'in our view the real terrorists are the governments american soviet french chinese and british that are hoarding h-bombs for their own use and worse still those governments (u.s. french and german) that are eagerly peddling advanced nuclear technology to countries like south africa brazil and argentina so that they can make their own bombs.',\n",
       " '\"when these bombs are used and they will be it will be the worlds big-time nuclear peddlers along with corporate suppliers like general electric westinghouse and gulf oil that we can thank for it.\"',\n",
       " 'gagging the progressive will do no more for national security than backyard bomb shelters because like it or not the news is out.',\n",
       " 'the heart of the successful h-bomb is the successful a-bomb.',\n",
       " '\"once youve got your a-bombs made the rest is frosting on the cake.\"',\n",
       " '\"all you have to do is set them up so that when they detonate theyll start off a hydrogen-fusion reaction.\"',\n",
       " 'part 1: making your bomb step 1: getting the ingredients uranium is the basic ingredient of the a-bomb.',\n",
       " '\"when a uranium atoms nucleus splits apart it releases a tremendous amount of energy (for its size) and it emits neutrons which go on to split other nearby uranium nuclei releasing more energy in what is called a chain reaction.\"',\n",
       " '\"(when atoms split matter is converted into energy according to einsteins equation e=mc2.\"',\n",
       " 'what better way to mark his birthday than with your own atomic fireworks) there are two kinds (isotopes) of uranium: the rare u-235 used in bombs and the more common heavier but useless u-238.',\n",
       " '\"natural uranium contains less than 1 percent u-235 and in order to be usable in bombs it has to be enriched to 90 percent u-235 and only 10 percent u-238.\"',\n",
       " 'plutonium-239 can also be used in bombs as a substitute for u-235.',\n",
       " 'ten pounds of u-235 (or slightly less plutonium) is all that is necessary for a bomb.',\n",
       " '\"less than ten pounds wont give you a critical mass.\"',\n",
       " 'so purifying or enriching naturally occurring uranium is likely to be your first big hurdle.',\n",
       " 'it is infinitely easy to steal ready-to-use enriched uranium or plutonium than to enrich some yourself.',\n",
       " 'and stealing uranium is not as hard as it sounds.',\n",
       " 'there are at least three sources of enriched uranium or plutonium... enriched uranium is manufactured at a gaseous diffusion plant in portsmouth ohio.',\n",
       " 'from there it is shipped in 10 liter bottles by airplane and trucks to conversion plants that turn it into uranium oxide or uranium metal.',\n",
       " 'each 10 liter bottle contains 7 kilograms of u-235 and there are 20 bottles to a typical shipment.',\n",
       " 'conversion facilities exist at hematite missouri; apollo pennsylvania; and erwin tennessee.',\n",
       " '\"the kerr-mcgee plant at crescent oklahoma -- where karen silkwood worked -- was a conversion plant that lost 40 lbs of plutonium.\"',\n",
       " 'enriched uranium can be stolen from these plants or from fuel-fabricating plants like those in new haven san diego; or lynchburg virginia.',\n",
       " '\"(a former kerr-mcgee supervisor james v. smith when asked at the silkwood trial if there were any security precautions at the plant to prevent theft testified that there were none of any kind no guards no fences no nothing.)\"',\n",
       " 'plutonium can be obtained from places like united nuclear in pawling new york; nuclear fuel services in erwin tennessee; general electric in pleasanton california; westinghouse in cheswick pennsylvania; nuclear materials and equipment corporation (numec) in leechburg pennsylvania; and plants in hanfford washington and morris illinois.',\n",
       " 'according to rolling stone magazine the israelis were involved in the theft of plutonium from numec.',\n",
       " '\"finally you can steal enriched uranium or plutonium while its en-route from conversion plants to fuel fabricating plants.\"',\n",
       " '\"it is usually transported (by air or truck) in the form of uranium oxide a brownish powder resembling instant coffee or as a metal coming in small chunks called broken buttons.\"',\n",
       " 'both forms are shipped in small cans stacked in 5-inch cylinders braced with welded struts in the center of ordinary 55 gallon steel drums.',\n",
       " '\"the drums weigh about 100 pounds and are clearly marked fissible material or danger plutonium.\"',\n",
       " 'a typical shipment might go from the enrichment plant at portsmouth ohio to the conversion plant in hematite missouri then to kansas city by truck where it would be flown to los angeles and then trucked down to the general atomic plant in san diego.',\n",
       " '\"the plans for the general atomic plant are on file at the nuclear regulatory commissions reading room at 1717 h street nw washington.\"',\n",
       " 'a xerox machine is provided for the convenience of the public.',\n",
       " '\"if you cant get hold of any enriched uranium youll have to settle for commercial grade (20 percent u-235).\"',\n",
       " 'this can be stolen from university reactors of a type called triga mark ii where security is even more casual than at commercial plants.',\n",
       " 'if stealing uranium seems too tacky you can buy it.',\n",
       " 'unenriched uranium is available at any chemical supply house for $23 a pound.',\n",
       " 'commercial grade (3 to 20 percent enriched) is available for $40 a pound from gulf atomic.',\n",
       " '\"youll have to enrich it further yourself.\"',\n",
       " 'quite frankly this can be something of a pain in the ass.',\n",
       " '\"youll need to start with a little more than 50 pounds of commercial-grade uranium.\"',\n",
       " '\"(its only 20 percent u-235 at best and you need 10 pounds of u-235 so... ) but with a little kitchen-table chemistry youll be able to convert the solid uranium oxide youve purchased into a liquid form.\"',\n",
       " '\"once youve done that youll be able to separate the u-235 that youll need from the u-238.\"',\n",
       " 'first pour a few gallons of concentrated hydrofluoric acid into your uranium oxide converting it to uranium tetrafluoride.',\n",
       " '(safety note: concentrated hydrofluoric acid is so corrosive that it will eat its way through glass so store it only in plastic.',\n",
       " 'used 1-gallon plastic milk containers will do.)',\n",
       " 'now you have to convert your uranium tetrafluoride to uranium hexafluoride the gaseous form of uranium which is convenient for separating out the isotope u-235 from u-238.',\n",
       " 'to get the hexafluoride form bubble fluorine gas into your container of uranium tetrafluoride.',\n",
       " 'fluorine is available in pressurized tanks from chemical-supply firms.',\n",
       " 'be careful how you use it though because fluorine is several times more deadly than chlorine the classic world war i poison gas.',\n",
       " 'chemists recommend that you carry out this step under a stove hood (the kind used to remove unpleasant cooking odors).',\n",
       " '\"if youve done your chemistry right you should now have a generous supply of uranium hexafluoride ready for enriching.\"',\n",
       " 'in the old horse-and-buggy days of a-bomb manufacture the enrichment was carried out by passing the uranium hexafluoride through hundreds of miles of pipes tubes and membranes until the u-235 was eventually separated from the u-238.',\n",
       " 'this gaseous-diffusion process as it was called is difficult time-consuming and expensive.',\n",
       " 'gaseous-diffusion plants cover hundreds of acres and cost in the neighborhood of $2-billion each.',\n",
       " 'so forget it.',\n",
       " 'there are easier and cheaper ways to enrich your uranium.',\n",
       " 'first transform the gas into a liquid by subjecting it to pressure.',\n",
       " 'you can use a bicycle pump for this.',\n",
       " 'then make a simple home centrifuge.',\n",
       " 'fill a standard-size bucket one-quarter full of liquid uranium hexafluoride.',\n",
       " 'attach a six-foot rope to the bucket handle.',\n",
       " 'now swing the rope (and attached bucket) around your head as fast as possible.',\n",
       " 'keep this up for about 45 minutes.',\n",
       " 'slow down gradually and very gently put the bucket on the floor.',\n",
       " 'the u-235 which is lighter will have risen to the top where it can be skimmed off like cream.',\n",
       " 'repeat this step until you have the required 10 pounds of uranium.',\n",
       " '\"(safety note: dont put all your enriched uranium hexafluoride in one bucket.\"',\n",
       " 'use at least two or three buckets and keep them in separate corners of the room.',\n",
       " 'this will prevent the premature build-up of a critical mass.)',\n",
       " '\"now its time to convert your enriched uranium back to metal form.\"',\n",
       " 'this is easily enough accomplished by spooning several ladlefuls of calcium (available in tablet form from your drugstore) into each bucket of uranium.',\n",
       " 'the calcium will react with the uranium hexafluoride to produce calcium fluoride a colorless salt which can be easily be separated from your pure enriched uranium metal.',\n",
       " '\"a few precautions: • while uranium is not dangerously radioactive in the amounts youll be handling if you plan to make more than one bomb it might be wise to wear gloves and a lead apron the kind you can buy in dental supply stores.\"',\n",
       " '• plutonium is one of the most toxic substances known.',\n",
       " 'if inhaled a thousandth of a gram can cause massive fibrosis of the lungs a painful way to go.',\n",
       " 'even a millionth of a gram in the lungs will cause cancer.',\n",
       " 'if eaten plutonium is metabolized like calcium.',\n",
       " 'it goes straight to the bones where it gives out alpha particles preventing bone marrow from manufacturing red blood cells.',\n",
       " 'the best way to avoid inhaling plutonium is to hold your breath while handling it.',\n",
       " 'if this is too difficult wear a mask.',\n",
       " 'to avoid ingesting plutonium orally follow this simple rule: never make an a-bomb on an empty stomach.',\n",
       " '\"• if you find yourself dozing off while youre working or if you begin to glow in the dark it might be wise to take a blood count.\"',\n",
       " 'prick your finger with a sterile pin place a drop of blood on a microscope slide cover it with a cover slip and examine under a microscope.',\n",
       " '(best results are obtained in the early morning.)',\n",
       " 'when you get leukemia immature cells are released into the bloodstream and usually the number of white cells increases (though this increase might take almost 2 weeks).',\n",
       " 'red blood cells look kind of like donuts (without the hole) and are slightly smaller than the white cells each of which has a nucleus.',\n",
       " 'immature red cells look similar to white cells (i.e.. slightly larger and have a nucleus).',\n",
       " 'if you have more than about 1 white cell (including immature ones) to 400 red cells then start to worry.',\n",
       " 'but depending upon your plans for the eventual use of the bomb a short life expectancy might not be a problem.',\n",
       " '\"step 2: assembling the a-bomb now that youve acquired the enriched uranium all thats left is to assemble your a-bomb.\"',\n",
       " 'go find a couple of stainless steel salad bowls.',\n",
       " 'you also want to separate your 10 pounds of u-235 into two hunks.',\n",
       " '(keep them apart) the idea is to push each half your uranium into the inside of a bowl.',\n",
       " 'take one hunk of your uranium and beat it into the inside of the first bowl.',\n",
       " 'uranium is malleable like gold so you should have no trouble hammering it into the bowl to get a good fit.',\n",
       " 'take another five-pound hunk of uranium and fit it into a second stainless steel bowl.',\n",
       " '\"these two bowls of u-235 are the subcritical masses which when brought together forcefully will provide the critical mass that makes your a-bomb go.\"',\n",
       " '\"keep them a respectful distance apart while working because you dont want them to go critical on you... at least not yet.\"',\n",
       " 'now hollow out the body of an old vacuum cleaner and place your two hemispherical bowls inside open ends facing each other no less than seven inches apart using masking tape to set them up in position.',\n",
       " '\"the reason for the steel bowls and the vacuum cleaner in case youre wondering is that these help reflect the neutrons back into the uranium for a more efficient explosion.\"',\n",
       " '\"a loose neutron is a useless neutron as the a-bomb pioneers used to say.\"',\n",
       " '\"as far as the a-bomb goes youre almost done.\"',\n",
       " 'the final problem is to figure out how to get the two u-235 hemispheres to smash into each other with sufficient force to set off a truly effective fission reaction.',\n",
       " 'almost any type of explosive can be used to drive them together.',\n",
       " 'gunpowder for example is easily made at home from potassium nitrate sulfur and carbon.',\n",
       " 'or you can get some blasting caps or tnt.',\n",
       " '(buy them or steal them from a construction site.)',\n",
       " 'best of all is c4 plastic explosive.',\n",
       " '\"you can mold it around your bowls and its fairly safe to work with.\"',\n",
       " '(but it might be wise to shape it around an extra salad bowl in another room and then fit it to your uranium-packed bowls.',\n",
       " 'this is particularly true in winter when a stray static electrical charge might induce ignition in the c4.',\n",
       " 'a responsible bomb maker considers it impolite to accidentally destroy more of the neighborhood than absolutely necessary.)',\n",
       " 'once the explosives are in place all you need to do is hook up a simple detonation device with a few batteries a switch and some wire.',\n",
       " 'remember though that it is essential that the two charges -- one on each side of the casing -- go off simultaneously.',\n",
       " '\"now put the whole thing in the casing of an old hoover vacuum cleaner and youre finished with this part of the process.\"',\n",
       " 'the rest is easy.',\n",
       " '\"step 3: make more a-bombs following the directions above a word to the wise about wastes after your a-bomb is completed youll have a pile of moderately fatal radioactive wastes like u-238.\"',\n",
       " 'these are not dangerous but you do have to get rid of them.',\n",
       " 'you can flush leftovers down the toilet.',\n",
       " '\"(dont worry about polluting the ocean there is already so much radioactive waste there a few more bucketfuls wont make any waves whatsoever.)\"',\n",
       " '\"if youre the fastidious type -- the kind who never leaves gum under their seat at the movies -- you can seal the nasty stuff in coffee cans and bury it in the backyard just like uncle sam does.\"',\n",
       " 'if the neighbor kids have a habit of trampling the lawn tell them to play over by the waste.',\n",
       " '\"youll soon find that theyre spending most of their time in bed.\"',\n",
       " '\"going first class if youre like us youre feeling the economic pinch and youll want to make your bomb as inexpensively as possible consonant of course with reasonable yield.\"',\n",
       " '\"the recipe weve given is for a budget-pleasing h-bomb no frills no flourishes; its just a simple 5-megaton bomb capable of wiping out the new york metropolitan area the san francisco bay area or boston.\"',\n",
       " '\"but dont forget your h-bomb will only be as good as the a-bombs in it.\"',\n",
       " 'if you want to spend a little more money you can punch-up your a-bomb considerably.',\n",
       " 'instead of centrifuging your uranium by hand you can buy a commercial centrifuge.',\n",
       " '(fisher scientific sells one for about $1000.)',\n",
       " 'you also might want to be fussier about your design.',\n",
       " '\"the hiroshima bomb a relatively crude one only fissioned 1 percent of its uranium and yielded only 13 kilotons.\"',\n",
       " '\"in order to fission more of the uranium the force of your explosive trigger needs to be evenly diffused around the sphere; the same pressure has to be exerted on every point of the sphere simultaneously.\"',\n",
       " '(it was a technique for producing this sort of simultaneous detonation by fashioning the explosives into lenses that the government accused julius and ethel rosenberg of trying to steal).',\n",
       " 'part 2: putting your h-bomb together the heart of the h-bomb is the fusion process.',\n",
       " 'several a-bombs are detonated in such a way as to create the extremely high temperature (100 million degrees c) necessary to fuse lithium deuteride (lid) into helium.',\n",
       " 'when the lithium nucleus slams into the deuterium nucleus two helium nuclei are created and if this happens to enough deuterium nuclei rapidly enough the result is an enormous amount of energy: the energy of the h-bomb.',\n",
       " '\"you dont have to worry about stealing lithium deuteride it can be purchased from any chemical-supply house.\"',\n",
       " 'it costs $1000 a pound.',\n",
       " '\"if your budget wont allow it you can substitute lithium hydride at $40 a pound.\"',\n",
       " 'you will need at least 100 pounds.',\n",
       " '\"its a corrosive and toxic powder so be careful.\"',\n",
       " 'place the lithium deuteride or hydride in glass jars and surround it with four a-bombs in their casings.',\n",
       " 'attach them to the same detonator so that they will go off simultaneously.',\n",
       " 'the container for the whole thing is no problem.',\n",
       " 'they can be placed anywhere: inside an old stereo console a discarded refrigerator etc... when the detonator sets off the four a-bombs all eight hemispheres of fissionable material will slam into each other at the same time creating four critical masses and four detonations.',\n",
       " 'this will raise the temperature of the lithium deuteride to 100 million degrees c fast enough (a few billionths of a second) so that the lithium will not be blown all over the neighborhood before the nuclei have time to fuse.',\n",
       " 'the result at least 1000 times the punch of the puny a-bomb that leveled hiroshima (20 million tons of tnt vs. 20 thousand tons.)',\n",
       " '\"part 3: what to do with your bomb now that you have a fully assembled h-bomb housed in an attractive console of your choice you may be wondering what should i do with it every family will have to answer this question according to its own tastes and preferences but you may want to explore some possibilities which have been successfully pioneered by the american government.\"',\n",
       " '1. sell your bomb and make a pile of money in these days of rising inflation increasing unemployment and an uncertain economic outlook few businesses make as much sense as weapons production.',\n",
       " 'if your career forecast is cloudy bomb sales may be the only sure way to avoid the humiliation of receiving welfare or unemployment.',\n",
       " 'regardless of your present income level a home h-bomb business can be an invaluable income supplement and certainly a profitable alternative to selling tupperware or pirated girl scout cookies.',\n",
       " 'unfortunately for the family bomb business big government has already cornered a large part of the world market.',\n",
       " 'but this does not mean that there is a shortage of potential customers.',\n",
       " 'the raid on entebee was the waterloo of hijacking and many nationalist groups are now on the alert for new means to get their message across.',\n",
       " '\"theyd jump at the chance to get hold of an h-bomb.\"',\n",
       " '\"emerging nations which cant ante up enough rice or sugar to buy themselves a reactor from g.e.\"',\n",
       " 'or westinghouse are also shopping around.',\n",
       " 'you may wonder about the ethics of selling to nations or groups whose goals you may disapprove of.',\n",
       " '\"but here again take a tip from our government: forget ideology -- its cash that counts.\"',\n",
       " 'and remember h-bomb sales have a way of escalating almost like a chain reaction.',\n",
       " 'suppose you make a sale to south yemen which you believe to be a soviet puppet.',\n",
       " 'well within a few days some discrete inquiries from north yemen and possibly the saudis the egyptians and the ethiopians as well can be expected.',\n",
       " 'similarly a sale to the ira will generate a sale to the ulster government; and a sale to the tanzanians will bring the ugandans running and so forth.',\n",
       " '\"it doesnt matter which side youre on only how many sides there are.\"',\n",
       " '\"dont forget about the possibility of repeat sales to the same customer.\"',\n",
       " 'as the experience of both the u.s. and the u.s.s.r. has shown each individual nation has a potentially infinite need for h-bombs.',\n",
       " 'no customer -- no matter how small -- can ever have too many.',\n",
       " '\"2. use your bomb at home many families are attracted to the h-bomb simply as a deterrent.\"',\n",
       " '\"a discrete sticker on the door or on the living room window saying this home protected by h-bomb will discourage irs investigators census takers and jehovahs witnesses.\"',\n",
       " '\"youll be surprised how fast the crime rate will go down and property values will go up.\"',\n",
       " '\"and once the news gets out that you are a home h-bomb owner youll find that you have unexpected leverage in neighborhood disputes over everything from parking places and stereo noise levels to school tax rates.\"',\n",
       " '\"so relax and enjoy the pride and excitement of home h-bomb ownership is it for you lets be honest.\"',\n",
       " '\"the h-bomb isnt for everyone.\"',\n",
       " '\"frankly there are people who cant handle it.\"',\n",
       " 'they break out in hives at the very mention of mega-death fallout or radiation sickness.',\n",
       " 'the following quiz will help you find out whether you have what it takes for home h-bomb ownership.',\n",
       " '\"if you can answer yes to six or more of these questions then youre emotionally eligible to join the nuclear club.\"',\n",
       " 'if not a more conventional weapon may be more your cup of tea try botulism-toxin laser rays or nerve gas.',\n",
       " '1. i ignore the demands of others.',\n",
       " '2. i subscribe to one or more of the following: soldier of fortune hustler popular mechanics self.',\n",
       " '3. though i have many interesting acquaintances i am my own best friend.',\n",
       " '\"4. i know what to say after you say hello but i am seldom interested in pursuing the conversation.\"',\n",
       " '\"5. i have seen the movie the deer hunter more than once.\"',\n",
       " '6. i know that everyone can be a winner if they want to and i resent whiners.',\n",
       " '7. i own one or more of the following: handgun video game trash compactor snowmobile.',\n",
       " '8. i am convinced that leukemia is psychosomatic.',\n",
       " '9. i am aware that most vegetarians are sexually impotent.',\n",
       " '10. i have read evidence that solar energy is a communist conspiracy.',\n",
       " 'myths about nuclear war ever since the first mushroom cloud over hiroshima ushered in the atomic age a small group of nay-sayers and doom-mongers has lobbied campaigned and demonstrated to convince americans that h-bomb ownership along with nuclear power is dangerous and unhealthy.',\n",
       " 'using their virtual stranglehold over the media these people have tried to discredit everything nuclear from energy to war.',\n",
       " 'they have vastly overrated the risks of nuclear bombs and left many americans feeling demoralized and indecisive; not sure where the truth lies.',\n",
       " 'well here are the myths and here are the facts.',\n",
       " 'myth: after a nuclear exchange the earth will no longer be suitable for human habitation.',\n",
       " 'fact: this is completely false.',\n",
       " '\"according to one scientist (quoted in john mcpees the curve of binding energy) the largest bomb that has ever been exploded anywhere was 60 megatons and that is one-thousandth the force of an earthquake one-thousandth the force of a hurricane.\"',\n",
       " '\"we have lived with earthquakes and hurricanes for a long time.\"',\n",
       " '\"another scientist adds it is often assumed that a full blown nuclear war would be the end of life on earth.\"',\n",
       " 'that is far from the truth.',\n",
       " '\"to end life on earth would take at least a thousand times the total yield of all the nuclear explosives existing in the world and probably a lot more.\"',\n",
       " 'even if humans succumbed many forms of life would survive a nuclear free-for-all cockroaches certain forms of bacteria and lichens for instance.',\n",
       " 'myth: radiation is bad for you.',\n",
       " 'fact: everything is bad for you if you have too much of it.',\n",
       " '\"if you eat too many bananas youll get a stomach-ache.\"',\n",
       " 'if you get too much sun you can get sunburned (or even skin cancer).',\n",
       " 'same thing with radiation.',\n",
       " 'too much may make you feel under the weather but nuclear industry officials insist that there is no evidence that low-level radiation has any really serious adverse effects.',\n",
       " 'and high-level radiation may bring unexpected benefits.',\n",
       " 'it speeds up evolution by weeding out unwanted genetic types and creating new ones.',\n",
       " '\"(remember the old saying two heads are better than one.)\"',\n",
       " '\"nearer to home its plain that radiation will get rid of pesky crab grass and weeds and teenagers will find that brief exposure to a nuclear burst vaporizes acne and other skin blemishes.\"',\n",
       " '\"(many survivors of the hiroshima bomb found that they were free from skin and its attendant problems forever.)\"',\n",
       " 'we hope this clears up any misconceptions you may have had.',\n",
       " 'enjoy your h-bomb']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pip.head(5)['sent_tokenize'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwording(liste):\n",
    "    return [word for word in liste if not word in stopwords.words()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_count_and_removal(data, col, language=\"english\"):\n",
    "\n",
    "    stoplist = stopwords.words(language)\n",
    "\n",
    "    number_of_stopwords = []\n",
    "    text_without_stopwords = []\n",
    "\n",
    "    col_name_number_of_stopwords = col + \"_num_of_stopwords\"\n",
    "    col_name_without_stopwords = col + \"_without_stopwords\"\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "        no_of_words = 0\n",
    "        without_stopwords = []\n",
    "        for word in data[col][i]:\n",
    "            if word in stoplist:\n",
    "                no_of_words += 1\n",
    "            else:\n",
    "                without_stopwords.append(word)\n",
    "        \n",
    "        text_without_stopwords.append(without_stopwords)\n",
    "        number_of_stopwords.append(no_of_words)\n",
    "\n",
    "    data[col_name_number_of_stopwords] = number_of_stopwords\n",
    "    data[col_name_without_stopwords] = text_without_stopwords\n",
    "\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 681284/681284 [04:24<00:00, 2572.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>word_tokenize</th>\n",
       "      <th>sent_tokenize</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>word_tokenize_num_of_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>info has been found (+/- 100 pages and 4.5 mb ...</td>\n",
       "      <td>[info, has, been, found, (, +/-, 100, pages, a...</td>\n",
       "      <td>[info has been found (+/- 100 pages and 4.5 mb...</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>[info, found, (, +/-, 100, pages, 4.5, mb, .pd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>these are the team members: drewes van der laa...</td>\n",
       "      <td>[these, are, the, team, members, :, drewes, va...</td>\n",
       "      <td>[these are the team members: drewes van der la...</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[team, members, :, drewes, van, der, laag, url...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>in het kader van kernfusie op aarde: maak je e...</td>\n",
       "      <td>[in, het, kader, van, kernfusie, op, aarde, :,...</td>\n",
       "      <td>[\"in het kader van kernfusie op aarde: maak je...</td>\n",
       "      <td>4783</td>\n",
       "      <td>235</td>\n",
       "      <td>1965</td>\n",
       "      <td>[het, kader, van, kernfusie, op, aarde, :, maa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing testing</td>\n",
       "      <td>[testing, testing]</td>\n",
       "      <td>[testing testing]</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[testing, testing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>thanks to yahoo's toolbar i can now 'capture' ...</td>\n",
       "      <td>[thanks, to, yahoo, \"s\", toolbar, i, can, now,...</td>\n",
       "      <td>[\"thanks to yahoos toolbar i can now capture t...</td>\n",
       "      <td>77</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>[thanks, yahoo, \"s\", toolbar, \"capture\", \"\", u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681279</th>\n",
       "      <td>681279</td>\n",
       "      <td>1713845</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>Student</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>01,July,2004</td>\n",
       "      <td>dear susan i could write some really bitter di...</td>\n",
       "      <td>[dear, susan, i, could, write, some, really, b...</td>\n",
       "      <td>[dear susan i could write some really bitter d...</td>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>[dear, susan, could, write, really, bitter, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681280</th>\n",
       "      <td>681280</td>\n",
       "      <td>1713845</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>Student</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>01,July,2004</td>\n",
       "      <td>dear susan 'i have the second yeast infection ...</td>\n",
       "      <td>[dear, susan, \"\", i, have, the, second, yeast,...</td>\n",
       "      <td>[\"dear susan i have the second yeast infection...</td>\n",
       "      <td>77</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>[dear, susan, \"\", second, yeast, infection, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681281</th>\n",
       "      <td>681281</td>\n",
       "      <td>1713845</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>Student</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>01,July,2004</td>\n",
       "      <td>dear susan your 'boyfriend' is fucking bald go...</td>\n",
       "      <td>[dear, susan, your, \"boyfriend\", \"\", is, fucki...</td>\n",
       "      <td>[\"dear susan your boyfriend is fucking bald go...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[dear, susan, \"boyfriend\", \"\", fucking, bald, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681282</th>\n",
       "      <td>681282</td>\n",
       "      <td>1713845</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>Student</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>01,July,2004</td>\n",
       "      <td>dear susan: just to clarify i am asking you to...</td>\n",
       "      <td>[dear, susan, :, just, to, clarify, i, am, ask...</td>\n",
       "      <td>[\"dear susan: just to clarify i am asking you ...</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>[dear, susan, :, clarify, asking, leave, house...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681283</th>\n",
       "      <td>681283</td>\n",
       "      <td>1713845</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "      <td>Student</td>\n",
       "      <td>Taurus</td>\n",
       "      <td>01,July,2004</td>\n",
       "      <td>hey everybody...and susan you might already kn...</td>\n",
       "      <td>[hey, everybody, ..., and, susan, you, might, ...</td>\n",
       "      <td>[hey everybody...and susan you might already k...</td>\n",
       "      <td>273</td>\n",
       "      <td>8</td>\n",
       "      <td>123</td>\n",
       "      <td>[hey, everybody, ..., susan, might, already, k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>681284 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0       id gender  age              topic      sign  \\\n",
       "0                0  2059027   male   15            Student       Leo   \n",
       "1                1  2059027   male   15            Student       Leo   \n",
       "2                2  2059027   male   15            Student       Leo   \n",
       "3                3  2059027   male   15            Student       Leo   \n",
       "4                4  3581210   male   33  InvestmentBanking  Aquarius   \n",
       "...            ...      ...    ...  ...                ...       ...   \n",
       "681279      681279  1713845   male   23            Student    Taurus   \n",
       "681280      681280  1713845   male   23            Student    Taurus   \n",
       "681281      681281  1713845   male   23            Student    Taurus   \n",
       "681282      681282  1713845   male   23            Student    Taurus   \n",
       "681283      681283  1713845   male   23            Student    Taurus   \n",
       "\n",
       "                date                                               text  \\\n",
       "0        14,May,2004  info has been found (+/- 100 pages and 4.5 mb ...   \n",
       "1        13,May,2004  these are the team members: drewes van der laa...   \n",
       "2        12,May,2004  in het kader van kernfusie op aarde: maak je e...   \n",
       "3        12,May,2004                                    testing testing   \n",
       "4       11,June,2004  thanks to yahoo's toolbar i can now 'capture' ...   \n",
       "...              ...                                                ...   \n",
       "681279  01,July,2004  dear susan i could write some really bitter di...   \n",
       "681280  01,July,2004  dear susan 'i have the second yeast infection ...   \n",
       "681281  01,July,2004  dear susan your 'boyfriend' is fucking bald go...   \n",
       "681282  01,July,2004  dear susan: just to clarify i am asking you to...   \n",
       "681283  01,July,2004  hey everybody...and susan you might already kn...   \n",
       "\n",
       "                                            word_tokenize  \\\n",
       "0       [info, has, been, found, (, +/-, 100, pages, a...   \n",
       "1       [these, are, the, team, members, :, drewes, va...   \n",
       "2       [in, het, kader, van, kernfusie, op, aarde, :,...   \n",
       "3                                      [testing, testing]   \n",
       "4       [thanks, to, yahoo, \"s\", toolbar, i, can, now,...   \n",
       "...                                                   ...   \n",
       "681279  [dear, susan, i, could, write, some, really, b...   \n",
       "681280  [dear, susan, \"\", i, have, the, second, yeast,...   \n",
       "681281  [dear, susan, your, \"boyfriend\", \"\", is, fucki...   \n",
       "681282  [dear, susan, :, just, to, clarify, i, am, ask...   \n",
       "681283  [hey, everybody, ..., and, susan, you, might, ...   \n",
       "\n",
       "                                            sent_tokenize  count_word  \\\n",
       "0       [info has been found (+/- 100 pages and 4.5 mb...          31   \n",
       "1       [these are the team members: drewes van der la...          23   \n",
       "2       [\"in het kader van kernfusie op aarde: maak je...        4783   \n",
       "3                                       [testing testing]           2   \n",
       "4       [\"thanks to yahoos toolbar i can now capture t...          77   \n",
       "...                                                   ...         ...   \n",
       "681279  [dear susan i could write some really bitter d...          45   \n",
       "681280  [\"dear susan i have the second yeast infection...          77   \n",
       "681281  [\"dear susan your boyfriend is fucking bald go...          13   \n",
       "681282  [\"dear susan: just to clarify i am asking you ...          68   \n",
       "681283  [hey everybody...and susan you might already k...         273   \n",
       "\n",
       "        count_sent  word_tokenize_num_of_stopwords  \\\n",
       "0                1                              12   \n",
       "1                1                               4   \n",
       "2              235                            1965   \n",
       "3                1                               0   \n",
       "4                3                              29   \n",
       "...            ...                             ...   \n",
       "681279           4                              17   \n",
       "681280           5                              26   \n",
       "681281           1                               4   \n",
       "681282           2                              35   \n",
       "681283           8                             123   \n",
       "\n",
       "                          word_tokenize_without_stopwords  \n",
       "0       [info, found, (, +/-, 100, pages, 4.5, mb, .pd...  \n",
       "1       [team, members, :, drewes, van, der, laag, url...  \n",
       "2       [het, kader, van, kernfusie, op, aarde, :, maa...  \n",
       "3                                      [testing, testing]  \n",
       "4       [thanks, yahoo, \"s\", toolbar, \"capture\", \"\", u...  \n",
       "...                                                   ...  \n",
       "681279  [dear, susan, could, write, really, bitter, di...  \n",
       "681280  [dear, susan, \"\", second, yeast, infection, pa...  \n",
       "681281  [dear, susan, \"boyfriend\", \"\", fucking, bald, ...  \n",
       "681282  [dear, susan, :, clarify, asking, leave, house...  \n",
       "681283  [hey, everybody, ..., susan, might, already, k...  \n",
       "\n",
       "[681284 rows x 14 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_count_and_removal(df_pip,'word_tokenize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_of_speech_tagging(data, column: str):\n",
    "    \n",
    "    number_column = column + \"_number_of_\"\n",
    "\n",
    "    num_nouns, num_verbs, num_adj, num_adv, num_int, num_sym, num_tos, num_prep, num_dig, num_con, num_other = [], [], [], [], [], [], [], [], [], [], []\n",
    "    \n",
    "    nouns, verbs, adjectives, adverbs, interjections, symbols, tos, prepositions, digits, conjunctions, others = [], [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "    \n",
    "    for i in tqdm.tqdm(range(len(data))):\n",
    "        tagged = nltk.pos_tag(data[column][i])\n",
    "        len_nouns, len_verbs, len_adj, len_adv, len_int, len_con, len_sym, len_tos, len_dig, len_prep, len_other = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "        \n",
    "        zw_nouns, zw_verbs, zw_adjectives, zw_adverbs, zw_interjections, zw_symbols, zw_tos, zw_prepositions, zw_digits, zw_conjunctions, zw_others = [], [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "        for c in range(len(tagged)):\n",
    "            if tagged[c][1] in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]:\n",
    "                zw_nouns.append(tagged[c][0])\n",
    "                len_nouns += 1\n",
    "            elif tagged[c][1] in [\"VB\", \"VBG\", \"VBD\", \"VBN\", \"VBP\", \"VBZ\", \"MD\"]:\n",
    "                zw_verbs.append(tagged[c][0])\n",
    "                len_verbs += 1\n",
    "            elif tagged[c][1] in [\"JJ\", \"JJR\", \"JJS\"]:\n",
    "                zw_adjectives.append(tagged[c][0])\n",
    "                len_adj += 1\n",
    "            elif tagged[c][1] in [\"RB\", \"RBR\", \"RBS\"]:\n",
    "                zw_adverbs.append(tagged[c][0])\n",
    "                len_adv += 1\n",
    "            elif tagged[c][1] in [\"UH\"]:\n",
    "                zw_interjections.append(tagged[c][0])\n",
    "                len_int += 1\n",
    "            elif tagged[c][1] in [\"SY\"]:\n",
    "                zw_symbols.append(tagged[c][0])\n",
    "                len_sym += 1\n",
    "            elif tagged[c][1] in [\"TO\"]:\n",
    "                zw_tos.append(tagged[c][0])\n",
    "                len_tos += 1\n",
    "            elif tagged[c][1] in [\"IN\"]:\n",
    "                zw_prepositions.append(tagged[c][0])\n",
    "                len_prep += 1\n",
    "            elif tagged[c][1] in [\"CD\"]:\n",
    "                zw_digits.append(tagged[c][0])\n",
    "                len_dig += 1\n",
    "            elif tagged[c][1] in [\"CC\"]:\n",
    "                zw_conjunctions.append(tagged[c][0])\n",
    "                len_con += 1\n",
    "            else:\n",
    "                zw_others.append(tagged[c][0])\n",
    "                len_other += 1\n",
    "                        \n",
    "        nouns.append(zw_nouns)\n",
    "        verbs.append(zw_verbs)\n",
    "        adjectives.append(zw_adjectives)\n",
    "        adverbs.append(zw_adverbs)\n",
    "        interjections.append(zw_interjections)\n",
    "        symbols.append(zw_symbols)\n",
    "        tos.append(zw_tos)\n",
    "        prepositions.append(zw_prepositions)\n",
    "        digits.append(zw_digits)\n",
    "        conjunctions.append(zw_conjunctions)\n",
    "        others.append(zw_others)\n",
    "\n",
    "        num_nouns.append(len_nouns)\n",
    "        num_verbs.append(len_verbs)\n",
    "        num_adj.append(len_adj)\n",
    "        num_adv.append(len_adv)\n",
    "        num_con.append(len_con)\n",
    "        num_prep.append(len_prep)\n",
    "        num_sym.append(len_sym)\n",
    "        num_int.append(len_int)\n",
    "        num_tos.append(len_tos)\n",
    "        num_dig.append(len_dig)\n",
    "        num_other.append(len_other)\n",
    "\n",
    "    data[number_column + \"nouns\"] = num_nouns\n",
    "    data[number_column + \"verbs\"] = num_verbs\n",
    "    data[number_column + \"adjectives\"] = num_adj\n",
    "    data[number_column + \"adverbs\"] = num_adv\n",
    "    data[number_column + \"interjections\"] = num_int\n",
    "    data[number_column + \"symbols\"] = num_sym\n",
    "    data[number_column + \"tos\"] = num_tos\n",
    "    data[number_column + \"prepositions\"] = num_prep\n",
    "    data[number_column + \"digits\"] = num_dig\n",
    "    data[number_column + \"conjunctions\"] = num_con\n",
    "    data[number_column + \"others\"] = num_other\n",
    "\n",
    "    data[column + \"_nouns\"] = nouns\n",
    "    data[column + \"_verbs\"] = verbs\n",
    "    data[column + \"_adjectives\"] = adjectives\n",
    "    data[column + \"_adverbs\"] = adverbs\n",
    "    data[column + \"_interjections\"] = interjections\n",
    "    data[column + \"_symbols\"] = symbols\n",
    "    data[column + \"_tos\"] = tos\n",
    "    data[column + \"_prepositions\"] = prepositions\n",
    "    data[column + \"_digits\"] = digits\n",
    "    data[column + \"_conjunctions\"] = conjunctions\n",
    "    data[column + \"_others\"] = others\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>word_tokenize</th>\n",
       "      <th>sent_tokenize</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>word_tokenize_num_of_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords_port</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>info has been found (+/- 100 pages and 4.5 mb ...</td>\n",
       "      <td>['info', 'has', 'been', 'found', '(', '+/-', '...</td>\n",
       "      <td>['info has been found (+/- 100 pages and 4.5 m...</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>['info', 'found', '(', '+/-', '100', 'pages', ...</td>\n",
       "      <td>['info', 'found', '(', '+/-', '100', 'page', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>these are the team members: drewes van der laa...</td>\n",
       "      <td>['these', 'are', 'the', 'team', 'members', ':'...</td>\n",
       "      <td>['these are the team members: drewes van der l...</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>['team', 'members', ':', 'drewes', 'van', 'der...</td>\n",
       "      <td>['team', 'member', ':', 'drew', 'van', 'der', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>in het kader van kernfusie op aarde: maak je e...</td>\n",
       "      <td>['in', 'het', 'kader', 'van', 'kernfusie', 'op...</td>\n",
       "      <td>['\"in het kader van kernfusie op aarde: maak j...</td>\n",
       "      <td>4783</td>\n",
       "      <td>235</td>\n",
       "      <td>1965</td>\n",
       "      <td>['het', 'kader', 'van', 'kernfusie', 'op', 'aa...</td>\n",
       "      <td>['het', 'kader', 'van', 'kernfusi', 'op', 'aar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id gender  age    topic sign         date  \\\n",
       "0  2059027   male   15  Student  Leo  14,May,2004   \n",
       "1  2059027   male   15  Student  Leo  13,May,2004   \n",
       "2  2059027   male   15  Student  Leo  12,May,2004   \n",
       "\n",
       "                                                text  \\\n",
       "0  info has been found (+/- 100 pages and 4.5 mb ...   \n",
       "1  these are the team members: drewes van der laa...   \n",
       "2  in het kader van kernfusie op aarde: maak je e...   \n",
       "\n",
       "                                       word_tokenize  \\\n",
       "0  ['info', 'has', 'been', 'found', '(', '+/-', '...   \n",
       "1  ['these', 'are', 'the', 'team', 'members', ':'...   \n",
       "2  ['in', 'het', 'kader', 'van', 'kernfusie', 'op...   \n",
       "\n",
       "                                       sent_tokenize  count_word  count_sent  \\\n",
       "0  ['info has been found (+/- 100 pages and 4.5 m...          31           1   \n",
       "1  ['these are the team members: drewes van der l...          23           1   \n",
       "2  ['\"in het kader van kernfusie op aarde: maak j...        4783         235   \n",
       "\n",
       "   word_tokenize_num_of_stopwords  \\\n",
       "0                              12   \n",
       "1                               4   \n",
       "2                            1965   \n",
       "\n",
       "                     word_tokenize_without_stopwords  \\\n",
       "0  ['info', 'found', '(', '+/-', '100', 'pages', ...   \n",
       "1  ['team', 'members', ':', 'drewes', 'van', 'der...   \n",
       "2  ['het', 'kader', 'van', 'kernfusie', 'op', 'aa...   \n",
       "\n",
       "                word_tokenize_without_stopwords_port  \n",
       "0  ['info', 'found', '(', '+/-', '100', 'page', '...  \n",
       "1  ['team', 'member', ':', 'drew', 'van', 'der', ...  \n",
       "2  ['het', 'kader', 'van', 'kernfusi', 'op', 'aar...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.drop([\"Unnamed: 0\", \"Unnamed: 0.1\"], inplace=True, axis=1)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 864/681284 [01:09<15:12:19, 12.43it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-dd02ce3ff94b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpart_of_speech_tagging\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"word_tokenize_without_stopwords\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-9dccbd8c1fc7>\u001b[0m in \u001b[0;36mpart_of_speech_tagging\u001b[1;34m(data, column)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mtagged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mlen_nouns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_verbs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_adj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_adv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_con\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_sym\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_tos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_dig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_prep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_other\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Win10\\anaconda3\\envs\\DataScience\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \"\"\"\n\u001b[0;32m    161\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Win10\\anaconda3\\envs\\DataScience\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[1;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[0;32m    117\u001b[0m         )\n\u001b[0;32m    118\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mtagged_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Maps to the specified tagset.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'eng'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Win10\\anaconda3\\envs\\DataScience\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                 \u001b[0mtag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_conf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_conf\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Win10\\anaconda3\\envs\\DataScience\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, features, return_conf)\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;31m# Do a secondary alphabetic sort, for stability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = part_of_speech_tagging(df, \"word_tokenize_without_stopwords\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/anaconda3/lib/python3.9/site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=28b396e3b297097d570aff54149a2f8c00117b37062cdeecf5990bfbeb7e635d\n",
      "  Stored in directory: /Users/niclascramer/Library/Caches/pip/wheels/d1/c1/d9/7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 681284/681284 [1:11:19<00:00, 159.20it/s]  \n"
     ]
    }
   ],
   "source": [
    "langs = []\n",
    "for entry in tqdm.tqdm(df[\"text\"]):\n",
    "    try:\n",
    "        langs.append(detect(entry))\n",
    "    except:\n",
    "        langs.append(\"Not Recognizable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

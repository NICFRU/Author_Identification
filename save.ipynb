{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as mltp\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "import sklearn as sk\n",
    "import seaborn as sea\n",
    "import re \n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "import pickle \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "1. Have a list of lemitised words of the text \n",
    "2. CSV of the Words with there respected wahrscheinlichkeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_overview(data_words,num_topics,df_topic,onlylist=1,name=''):\n",
    "    topic_of_text=[]\n",
    "    for text in tqdm(data_words):\n",
    "        liste_text_top=[]\n",
    "        for t in range(num_topics):\n",
    "            liste_top=[]\n",
    "            for element in df_topic[f'{t}_word'].tolist():\n",
    "                liste_top.append(len([i for i, e in enumerate(text) if e == element]))\n",
    "    \n",
    "            Result = []\n",
    "            for i1, i2 in zip(liste_top, df_topic[f'{t}_per'].tolist()):\n",
    "                Result.append(i1*i2)\n",
    "            liste_text_top.append(sum(Result))\n",
    "        \n",
    "        if sum(liste_text_top)!=0:\n",
    "            topic_of_text.append([text,max(range(len(liste_text_top)), key=liste_text_top.__getitem__),max(liste_text_top)/sum(liste_text_top),liste_text_top])\n",
    "        else:\n",
    "            topic_of_text.append([text,'None','None',liste_text_top])\n",
    "    if not onlylist:\n",
    "        df=pd.DataFrame(data=topic_of_text,columns=['text',f'topic_num_{name}',f'percent_{name}',f'perc_list_{name}'])\n",
    "        return df[[f'topic_num_{name}',f'percent_{name}',f'perc_list_{name}']]\n",
    "    else:    \n",
    "        return pd.DataFrame(data=topic_of_text,columns=['text','topic_num','percent','perc_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>language_2</th>\n",
       "      <th>word_tokenize</th>\n",
       "      <th>sent_tokenize</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>word_tokenize_num_of_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords_port</th>\n",
       "      <th>msg_lemmatized</th>\n",
       "      <th>cleanLinks</th>\n",
       "      <th>word_per_sent_mean</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>445164</td>\n",
       "      <td>2970791</td>\n",
       "      <td>25,May,2004</td>\n",
       "      <td>This blog is being posted due to the fact that...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>['This', 'blog', 'is', 'being', 'posted', 'due...</td>\n",
       "      <td>['This blog is being posted due to the fact th...</td>\n",
       "      <td>83</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>['This', 'blog', 'posted', 'due', 'fact', 'lit...</td>\n",
       "      <td>['thi', 'blog', 'post', 'due', 'fact', 'littl'...</td>\n",
       "      <td>['thi', 'blog', 'post', 'due', 'fact', 'littl'...</td>\n",
       "      <td>This blog is being posted due to the fact that...</td>\n",
       "      <td>20.75</td>\n",
       "      <td>male</td>\n",
       "      <td>14</td>\n",
       "      <td>Student</td>\n",
       "      <td>Sagittarius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>293091</td>\n",
       "      <td>3931851</td>\n",
       "      <td>02,agosto,2004</td>\n",
       "      <td>So I have a big fucking interview tomorrow for...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>['So', 'I', 'have', 'a', 'big', 'fucking', 'in...</td>\n",
       "      <td>['So I have a big fucking interview tomorrow f...</td>\n",
       "      <td>88</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>['So', 'I', 'big', 'fucking', 'interview', 'to...</td>\n",
       "      <td>['so', 'i', 'big', 'fuck', 'interview', 'tomor...</td>\n",
       "      <td>['so', 'i', 'big', 'fuck', 'interview', 'tomor...</td>\n",
       "      <td>So I have a big fucking interview tomorrow for...</td>\n",
       "      <td>17.60</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Pisces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>594697</td>\n",
       "      <td>3481650</td>\n",
       "      <td>07,July,2004</td>\n",
       "      <td>I was reminded just now of the time Ashley and...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>['I', 'was', 'reminded', 'just', 'now', 'of', ...</td>\n",
       "      <td>['I was reminded just now of the time Ashley a...</td>\n",
       "      <td>574</td>\n",
       "      <td>32</td>\n",
       "      <td>234</td>\n",
       "      <td>['I', 'reminded', 'time', 'Ashley', 'I', 'drov...</td>\n",
       "      <td>['i', 'remind', 'time', 'ashley', 'i', 'drove'...</td>\n",
       "      <td>['i', 'remind', 'time', 'ashley', 'i', 'drove'...</td>\n",
       "      <td>I was reminded just now of the time Ashley and...</td>\n",
       "      <td>17.94</td>\n",
       "      <td>female</td>\n",
       "      <td>17</td>\n",
       "      <td>Student</td>\n",
       "      <td>Gemini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3764</td>\n",
       "      <td>4187211</td>\n",
       "      <td>20,August,2004</td>\n",
       "      <td>I was checking up on my cousin Dylan and Fanni...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>['I', 'was', 'checking', 'up', 'on', 'my', 'co...</td>\n",
       "      <td>['\"I was checking up on my cousin Dylan and Fa...</td>\n",
       "      <td>84</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>['I', 'checking', 'cousin', 'Dylan', 'Fannie',...</td>\n",
       "      <td>['i', 'check', 'cousin', 'dylan', 'fanni', '\"s...</td>\n",
       "      <td>['i', 'check', 'cousin', 'dylan', 'fanni', '\"s...</td>\n",
       "      <td>I was checking up on my cousin Dylan and Fanni...</td>\n",
       "      <td>28.00</td>\n",
       "      <td>female</td>\n",
       "      <td>23</td>\n",
       "      <td>Student</td>\n",
       "      <td>Taurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>162953</td>\n",
       "      <td>3686696</td>\n",
       "      <td>24,June,2004</td>\n",
       "      <td>for the NME interview click urlLink part 1 and...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>['for', 'the', 'NME', 'interview', 'click', 'u...</td>\n",
       "      <td>['for the NME interview click urlLink part 1 a...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>['NME', 'interview', 'click', 'urlLink', 'part...</td>\n",
       "      <td>['nme', 'interview', 'click', 'urllink', 'part...</td>\n",
       "      <td>['nme', 'interview', 'click', 'urllink', 'part...</td>\n",
       "      <td>for the NME interview click urlLink part 1 and...</td>\n",
       "      <td>12.00</td>\n",
       "      <td>female</td>\n",
       "      <td>23</td>\n",
       "      <td>Student</td>\n",
       "      <td>Aquarius</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id            date  \\\n",
       "0      445164  2970791     25,May,2004   \n",
       "1      293091  3931851  02,agosto,2004   \n",
       "2      594697  3481650    07,July,2004   \n",
       "3        3764  4187211  20,August,2004   \n",
       "4      162953  3686696    24,June,2004   \n",
       "\n",
       "                                                text lang language_2  \\\n",
       "0  This blog is being posted due to the fact that...   en         en   \n",
       "1  So I have a big fucking interview tomorrow for...   en         en   \n",
       "2  I was reminded just now of the time Ashley and...   en         en   \n",
       "3  I was checking up on my cousin Dylan and Fanni...   en         en   \n",
       "4  for the NME interview click urlLink part 1 and...   en         en   \n",
       "\n",
       "                                       word_tokenize  \\\n",
       "0  ['This', 'blog', 'is', 'being', 'posted', 'due...   \n",
       "1  ['So', 'I', 'have', 'a', 'big', 'fucking', 'in...   \n",
       "2  ['I', 'was', 'reminded', 'just', 'now', 'of', ...   \n",
       "3  ['I', 'was', 'checking', 'up', 'on', 'my', 'co...   \n",
       "4  ['for', 'the', 'NME', 'interview', 'click', 'u...   \n",
       "\n",
       "                                       sent_tokenize  count_word  count_sent  \\\n",
       "0  ['This blog is being posted due to the fact th...          83           4   \n",
       "1  ['So I have a big fucking interview tomorrow f...          88           5   \n",
       "2  ['I was reminded just now of the time Ashley a...         574          32   \n",
       "3  ['\"I was checking up on my cousin Dylan and Fa...          84           3   \n",
       "4  ['for the NME interview click urlLink part 1 a...          12           1   \n",
       "\n",
       "   word_tokenize_num_of_stopwords  \\\n",
       "0                              34   \n",
       "1                              31   \n",
       "2                             234   \n",
       "3                              32   \n",
       "4                               3   \n",
       "\n",
       "                     word_tokenize_without_stopwords  \\\n",
       "0  ['This', 'blog', 'posted', 'due', 'fact', 'lit...   \n",
       "1  ['So', 'I', 'big', 'fucking', 'interview', 'to...   \n",
       "2  ['I', 'reminded', 'time', 'Ashley', 'I', 'drov...   \n",
       "3  ['I', 'checking', 'cousin', 'Dylan', 'Fannie',...   \n",
       "4  ['NME', 'interview', 'click', 'urlLink', 'part...   \n",
       "\n",
       "                word_tokenize_without_stopwords_port  \\\n",
       "0  ['thi', 'blog', 'post', 'due', 'fact', 'littl'...   \n",
       "1  ['so', 'i', 'big', 'fuck', 'interview', 'tomor...   \n",
       "2  ['i', 'remind', 'time', 'ashley', 'i', 'drove'...   \n",
       "3  ['i', 'check', 'cousin', 'dylan', 'fanni', '\"s...   \n",
       "4  ['nme', 'interview', 'click', 'urllink', 'part...   \n",
       "\n",
       "                                      msg_lemmatized  \\\n",
       "0  ['thi', 'blog', 'post', 'due', 'fact', 'littl'...   \n",
       "1  ['so', 'i', 'big', 'fuck', 'interview', 'tomor...   \n",
       "2  ['i', 'remind', 'time', 'ashley', 'i', 'drove'...   \n",
       "3  ['i', 'check', 'cousin', 'dylan', 'fanni', '\"s...   \n",
       "4  ['nme', 'interview', 'click', 'urllink', 'part...   \n",
       "\n",
       "                                          cleanLinks  word_per_sent_mean  \\\n",
       "0  This blog is being posted due to the fact that...               20.75   \n",
       "1  So I have a big fucking interview tomorrow for...               17.60   \n",
       "2  I was reminded just now of the time Ashley and...               17.94   \n",
       "3  I was checking up on my cousin Dylan and Fanni...               28.00   \n",
       "4  for the NME interview click urlLink part 1 and...               12.00   \n",
       "\n",
       "   gender  age    topic         sign  \n",
       "0    male   14  Student  Sagittarius  \n",
       "1    male   15  Student       Pisces  \n",
       "2  female   17  Student       Gemini  \n",
       "3  female   23  Student       Taurus  \n",
       "4  female   23  Student     Aquarius  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_spacy=pd.read_csv('data/train_500.csv')\n",
    "df_text_spacy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_words=\n",
    "df_topic=pd.read_csv('LDA/lda_gesamt.csv')\n",
    "df_topic=df_topic.loc[:, ~df_topic.columns.str.contains('^Unnamed')]\n",
    "num_topics=int(len(df_topic.columns)/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_word</th>\n",
       "      <th>0_per</th>\n",
       "      <th>1_word</th>\n",
       "      <th>1_per</th>\n",
       "      <th>2_word</th>\n",
       "      <th>2_per</th>\n",
       "      <th>3_word</th>\n",
       "      <th>3_per</th>\n",
       "      <th>4_word</th>\n",
       "      <th>4_per</th>\n",
       "      <th>...</th>\n",
       "      <th>33_word</th>\n",
       "      <th>33_per</th>\n",
       "      <th>34_word</th>\n",
       "      <th>34_per</th>\n",
       "      <th>35_word</th>\n",
       "      <th>35_per</th>\n",
       "      <th>36_word</th>\n",
       "      <th>36_per</th>\n",
       "      <th>37_word</th>\n",
       "      <th>37_per</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go</td>\n",
       "      <td>0.176</td>\n",
       "      <td>world</td>\n",
       "      <td>0.123</td>\n",
       "      <td>time</td>\n",
       "      <td>0.023</td>\n",
       "      <td>weird</td>\n",
       "      <td>0.194</td>\n",
       "      <td>free</td>\n",
       "      <td>0.104</td>\n",
       "      <td>...</td>\n",
       "      <td>learn</td>\n",
       "      <td>0.099</td>\n",
       "      <td>nbsp</td>\n",
       "      <td>0.494</td>\n",
       "      <td>note</td>\n",
       "      <td>0.122</td>\n",
       "      <td>add</td>\n",
       "      <td>0.224</td>\n",
       "      <td>link</td>\n",
       "      <td>0.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>then</td>\n",
       "      <td>0.069</td>\n",
       "      <td>believe</td>\n",
       "      <td>0.103</td>\n",
       "      <td>good</td>\n",
       "      <td>0.021</td>\n",
       "      <td>brother</td>\n",
       "      <td>0.137</td>\n",
       "      <td>study</td>\n",
       "      <td>0.059</td>\n",
       "      <td>...</td>\n",
       "      <td>however</td>\n",
       "      <td>0.096</td>\n",
       "      <td>comment</td>\n",
       "      <td>0.073</td>\n",
       "      <td>view</td>\n",
       "      <td>0.112</td>\n",
       "      <td>create</td>\n",
       "      <td>0.110</td>\n",
       "      <td>hair</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there</td>\n",
       "      <td>0.040</td>\n",
       "      <td>student</td>\n",
       "      <td>0.050</td>\n",
       "      <td>day</td>\n",
       "      <td>0.021</td>\n",
       "      <td>st</td>\n",
       "      <td>0.080</td>\n",
       "      <td>sign</td>\n",
       "      <td>0.058</td>\n",
       "      <td>...</td>\n",
       "      <td>question</td>\n",
       "      <td>0.087</td>\n",
       "      <td>box</td>\n",
       "      <td>0.042</td>\n",
       "      <td>step</td>\n",
       "      <td>0.092</td>\n",
       "      <td>character</td>\n",
       "      <td>0.097</td>\n",
       "      <td>church</td>\n",
       "      <td>0.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>guy</td>\n",
       "      <td>0.035</td>\n",
       "      <td>lie</td>\n",
       "      <td>0.044</td>\n",
       "      <td>now</td>\n",
       "      <td>0.020</td>\n",
       "      <td>stare</td>\n",
       "      <td>0.072</td>\n",
       "      <td>level</td>\n",
       "      <td>0.047</td>\n",
       "      <td>...</td>\n",
       "      <td>apparently</td>\n",
       "      <td>0.056</td>\n",
       "      <td>smart</td>\n",
       "      <td>0.032</td>\n",
       "      <td>build</td>\n",
       "      <td>0.065</td>\n",
       "      <td>window</td>\n",
       "      <td>0.064</td>\n",
       "      <td>ill</td>\n",
       "      <td>0.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>home</td>\n",
       "      <td>0.034</td>\n",
       "      <td>chance</td>\n",
       "      <td>0.043</td>\n",
       "      <td>really</td>\n",
       "      <td>0.019</td>\n",
       "      <td>universe</td>\n",
       "      <td>0.050</td>\n",
       "      <td>floor</td>\n",
       "      <td>0.042</td>\n",
       "      <td>...</td>\n",
       "      <td>result</td>\n",
       "      <td>0.040</td>\n",
       "      <td>accept</td>\n",
       "      <td>0.030</td>\n",
       "      <td>interested</td>\n",
       "      <td>0.060</td>\n",
       "      <td>difference</td>\n",
       "      <td>0.054</td>\n",
       "      <td>click</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  0_word  0_per   1_word  1_per  2_word  2_per    3_word  3_per 4_word  4_per  \\\n",
       "0     go  0.176    world  0.123    time  0.023     weird  0.194   free  0.104   \n",
       "1   then  0.069  believe  0.103    good  0.021   brother  0.137  study  0.059   \n",
       "2  there  0.040  student  0.050     day  0.021        st  0.080   sign  0.058   \n",
       "3    guy  0.035      lie  0.044     now  0.020     stare  0.072  level  0.047   \n",
       "4   home  0.034   chance  0.043  really  0.019  universe  0.050  floor  0.042   \n",
       "\n",
       "   ...     33_word  33_per  34_word  34_per     35_word  35_per     36_word  \\\n",
       "0  ...       learn   0.099     nbsp   0.494        note   0.122         add   \n",
       "1  ...     however   0.096  comment   0.073        view   0.112      create   \n",
       "2  ...    question   0.087      box   0.042        step   0.092   character   \n",
       "3  ...  apparently   0.056    smart   0.032       build   0.065      window   \n",
       "4  ...      result   0.040   accept   0.030  interested   0.060  difference   \n",
       "\n",
       "   36_per 37_word  37_per  \n",
       "0   0.224    link   0.120  \n",
       "1   0.110    hair   0.077  \n",
       "2   0.097  church   0.068  \n",
       "3   0.064     ill   0.063  \n",
       "4   0.054   click   0.040  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_liste=df_text_spacy.text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization_test(texts,  allowed_posttags=['NOUN','ADJ','VERB','ADV']):\n",
    "    nlp=spacy.load('en_core_web_md',disable=['parser','ner'])\n",
    "    texts_out=[]\n",
    "    for text in tqdm(texts):\n",
    "        doc= nlp(text)\n",
    "        new_text=[]\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_posttags:\n",
    "                new_text.append(token.lemma_)\n",
    "        final=' '.join(new_text)\n",
    "        texts_out.append(final)\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a3652a1173428badbe8a2bb507f401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lemma_text=lemmatization_test(text_liste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_words(texts):\n",
    "    final=[]\n",
    "    for text in tqdm(texts):\n",
    "        new= gensim.utils.simple_preprocess(text,deacc=True)\n",
    "        final.append(new)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f122fe1a53d44eb9cc9edfa50595fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_words=gen_words(lemma_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f2fbb5ae45485888c4a8e5b2133b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m create_df_overview(data_words,num_topics,df_topic)\n",
      "Cell \u001b[0;32mIn[28], line 11\u001b[0m, in \u001b[0;36mcreate_df_overview\u001b[0;34m(data_words, num_topics, df_topic)\u001b[0m\n\u001b[1;32m      8\u001b[0m     liste_top\u001b[39m.\u001b[39mappend(\u001b[39mlen\u001b[39m([i \u001b[39mfor\u001b[39;00m i, e \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(text) \u001b[39mif\u001b[39;00m e \u001b[39m==\u001b[39m element]))\n\u001b[1;32m     10\u001b[0m Result \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfor\u001b[39;00m i1, i2 \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(liste_top, df_topic[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m}\u001b[39;00m\u001b[39m_per\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()):\n\u001b[1;32m     12\u001b[0m     Result\u001b[39m.\u001b[39mappend(i1\u001b[39m*\u001b[39mi2)\n\u001b[1;32m     13\u001b[0m liste_text_top\u001b[39m.\u001b[39mappend(\u001b[39msum\u001b[39m(Result))    \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "create_df_overview(data_words,num_topics,df_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath='LDA'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lda_nv_24'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a='lda_nv_24.csv'\n",
    "a[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a004f55470b4326a4bce5196694cedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3136429b7189448685deaadfd96e5ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdee396f2260400791cb1d29b3503ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3603eb1c0a8d4a319e50dd25bb9c4a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f233a18d194da7a1a143359e425d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489887a241534c8488d80780fc817ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72952e61c4084b2a9527b4ed613481f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502043b8dd87480d9275ef74ff18694c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=0\n",
    "for lda_file in onlyfiles:\n",
    "    df_topic=pd.read_csv(f'{mypath}/{lda_file}')\n",
    "    df_topic=df_topic.loc[:, ~df_topic.columns.str.contains('^Unnamed')]\n",
    "    num_topics=int(len(df_topic.columns)/2)\n",
    "    df=create_df_overview(data_words,num_topics,df_topic,0,lda_file[:-4])\n",
    "    if not x:\n",
    "        df_1=df.copy()\n",
    "    else:\n",
    "        df_1=pd.merge(df_1, df, left_index=True, right_index=True)\n",
    "    x+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.to_csv('save.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_num_lda_nv_24</th>\n",
       "      <th>percent_lda_nv_24</th>\n",
       "      <th>perc_list_lda_nv_24</th>\n",
       "      <th>topic_num_lda_500</th>\n",
       "      <th>percent_lda_500</th>\n",
       "      <th>perc_list_lda_500</th>\n",
       "      <th>topic_num_lda_gesamt_76</th>\n",
       "      <th>percent_lda_gesamt_76</th>\n",
       "      <th>perc_list_lda_gesamt_76</th>\n",
       "      <th>topic_num_lda_gesamt</th>\n",
       "      <th>...</th>\n",
       "      <th>perc_list_lda_nv_76</th>\n",
       "      <th>topic_num_lda_500_76</th>\n",
       "      <th>percent_lda_500_76</th>\n",
       "      <th>perc_list_lda_500_76</th>\n",
       "      <th>topic_num_lda_gesamt_24</th>\n",
       "      <th>percent_lda_gesamt_24</th>\n",
       "      <th>perc_list_lda_gesamt_24</th>\n",
       "      <th>topic_num_lda_nv</th>\n",
       "      <th>percent_lda_nv</th>\n",
       "      <th>perc_list_lda_nv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>0.522646</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>31</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.258929</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0,...</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.472, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>21</td>\n",
       "      <td>0.286765</td>\n",
       "      <td>[0.0, 0.007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.442623</td>\n",
       "      <td>[0.0, 0.0, 0.13500000000000004, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>18</td>\n",
       "      <td>0.485856</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.055999999999999994, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>0.400248</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.055999999999999994, 0.1...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0...</td>\n",
       "      <td>37</td>\n",
       "      <td>0.261361</td>\n",
       "      <td>[0.063, 0.0, 0.432, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.542, 0.108, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>27</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.0, 0.0...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.46924</td>\n",
       "      <td>[0.0, 0.0, 0.09200000000000003, 0.0, 0.389, 0....</td>\n",
       "      <td>1</td>\n",
       "      <td>0.599576</td>\n",
       "      <td>[0.0, 0.849, 0.0, 0.0, 0.023, 0.0, 0.153000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0.318367</td>\n",
       "      <td>[0.084, 0.396, 0.0, 0.0, 0.14900000000000002, ...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.10101</td>\n",
       "      <td>[0.004, 0.0, 0.0, 0.018000000000000002, 0.0, 0...</td>\n",
       "      <td>27</td>\n",
       "      <td>0.187367</td>\n",
       "      <td>[1.119, 0.0, 0.0, 0.064, 0.719, 0.0, 0.0, 0.17...</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.586, 0.0, 0.0, 2.196, 1.338, 0.0, 0.004, 0....</td>\n",
       "      <td>73</td>\n",
       "      <td>0.261307</td>\n",
       "      <td>[0.0, 0.022, 0.0, 0.006, 0.0, 0.0, 0.015, 0.0,...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.477825</td>\n",
       "      <td>[0.28600000000000003, 0.04300000000000001, 0.8...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.49402</td>\n",
       "      <td>[0.0, 5.121999999999999, 0.0, 0.0, 0.417000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.316395</td>\n",
       "      <td>[0.0, 0.0, 0.025, 0.0, 0.018, 0.071, 0.015, 0....</td>\n",
       "      <td>31</td>\n",
       "      <td>0.275</td>\n",
       "      <td>[0.0, 0.0, 0.003, 0.006, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>48</td>\n",
       "      <td>0.196437</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.538, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>40</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>22</td>\n",
       "      <td>0.577972</td>\n",
       "      <td>[0.0, 0.0, 0.11700000000000002, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>18</td>\n",
       "      <td>0.342181</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.069, 0.0, 0.042, 0.0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>0.93568</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.06, 0.026, 0.0, 0.0, 0....</td>\n",
       "      <td>13</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0...</td>\n",
       "      <td>52</td>\n",
       "      <td>0.940789</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>27</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.832765</td>\n",
       "      <td>[0.0, 0.238, 0.0, 0.0, 0.0, 0.0, 0.0, 1.464, 0...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.863244</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014, 0.0, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  topic_num_lda_nv_24 percent_lda_nv_24  \\\n",
       "0                  18          0.522646   \n",
       "1                   8          0.400248   \n",
       "2                   8          0.318367   \n",
       "3                  18          0.316395   \n",
       "4                  22           0.93568   \n",
       "\n",
       "                                 perc_list_lda_nv_24 topic_num_lda_500  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0...                31   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.055999999999999994, 0.1...                13   \n",
       "2  [0.084, 0.396, 0.0, 0.0, 0.14900000000000002, ...                26   \n",
       "3  [0.0, 0.0, 0.025, 0.0, 0.018, 0.071, 0.015, 0....                31   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.06, 0.026, 0.0, 0.0, 0....                13   \n",
       "\n",
       "  percent_lda_500                                  perc_list_lda_500  \\\n",
       "0        0.272727  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0...   \n",
       "1        0.358974  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0...   \n",
       "2         0.10101  [0.004, 0.0, 0.0, 0.018000000000000002, 0.0, 0...   \n",
       "3           0.275  [0.0, 0.0, 0.003, 0.006, 0.0, 0.0, 0.0, 0.0, 0...   \n",
       "4        0.777778  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0...   \n",
       "\n",
       "  topic_num_lda_gesamt_76 percent_lda_gesamt_76  \\\n",
       "0                       9              0.258929   \n",
       "1                      37              0.261361   \n",
       "2                      27              0.187367   \n",
       "3                      48              0.196437   \n",
       "4                      52              0.940789   \n",
       "\n",
       "                             perc_list_lda_gesamt_76 topic_num_lda_gesamt  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0,...                    7   \n",
       "1  [0.063, 0.0, 0.432, 0.0, 0.0, 0.0, 0.0, 0.0, 0...                   20   \n",
       "2  [1.119, 0.0, 0.0, 0.064, 0.719, 0.0, 0.0, 0.17...                   27   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                   20   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                   31   \n",
       "\n",
       "   ...                                perc_list_lda_nv_76  \\\n",
       "0  ...  [0.0, 0.0, 0.0, 0.0, 0.472, 0.0, 0.0, 0.0, 0.0...   \n",
       "1  ...  [0.0, 0.0, 0.0, 0.542, 0.108, 0.0, 0.0, 0.0, 0...   \n",
       "2  ...  [0.586, 0.0, 0.0, 2.196, 1.338, 0.0, 0.004, 0....   \n",
       "3  ...  [0.0, 0.0, 0.0, 0.0, 0.538, 0.0, 0.0, 0.0, 0.0...   \n",
       "4  ...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "  topic_num_lda_500_76 percent_lda_500_76  \\\n",
       "0                   21           0.286765   \n",
       "1                   27           0.276596   \n",
       "2                   73           0.261307   \n",
       "3                   40           0.376471   \n",
       "4                   27           0.342105   \n",
       "\n",
       "                                perc_list_lda_500_76 topic_num_lda_gesamt_24  \\\n",
       "0  [0.0, 0.007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...                       2   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.0, 0.0...                       4   \n",
       "2  [0.0, 0.022, 0.0, 0.006, 0.0, 0.0, 0.015, 0.0,...                       4   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                      22   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                       7   \n",
       "\n",
       "  percent_lda_gesamt_24                            perc_list_lda_gesamt_24  \\\n",
       "0              0.442623  [0.0, 0.0, 0.13500000000000004, 0.0, 0.0, 0.0,...   \n",
       "1               0.46924  [0.0, 0.0, 0.09200000000000003, 0.0, 0.389, 0....   \n",
       "2              0.477825  [0.28600000000000003, 0.04300000000000001, 0.8...   \n",
       "3              0.577972  [0.0, 0.0, 0.11700000000000002, 0.0, 0.0, 0.0,...   \n",
       "4              0.832765  [0.0, 0.238, 0.0, 0.0, 0.0, 0.0, 0.0, 1.464, 0...   \n",
       "\n",
       "  topic_num_lda_nv percent_lda_nv  \\\n",
       "0               18       0.485856   \n",
       "1                1       0.599576   \n",
       "2                1        0.49402   \n",
       "3               18       0.342181   \n",
       "4               13       0.863244   \n",
       "\n",
       "                                    perc_list_lda_nv  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.055999999999999994, 0.0...  \n",
       "1  [0.0, 0.849, 0.0, 0.0, 0.023, 0.0, 0.153000000...  \n",
       "2  [0.0, 5.121999999999999, 0.0, 0.0, 0.417000000...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.069, 0.0, 0.042, 0.0, 0...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014, 0.0, 0.0...  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54c35701683d52397dec8c1b1dabb7d7599bf83c9aec0584240eb3e578830d84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as mltp\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "import sklearn as sk\n",
    "import seaborn as sea\n",
    "import re \n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niclascramer/opt/miniconda3/envs/NLP/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "import pickle \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "1. Have a list of lemitised words of the text \n",
    "2. CSV of the Words with there respected wahrscheinlichkeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_overview(data_words,num_topics,df_topic):\n",
    "    topic_of_text=[]\n",
    "    for text in tqdm(data_words):\n",
    "        liste_text_top=[]\n",
    "        for t in range(num_topics):\n",
    "            liste_top=[]\n",
    "            for element in df_topic[f'{t}_word'].tolist():\n",
    "                liste_top.append(len([i for i, e in enumerate(text) if e == element]))\n",
    "    \n",
    "            Result = []\n",
    "            for i1, i2 in zip(liste_top, df_topic[f'{t}_per'].tolist()):\n",
    "                Result.append(i1*i2)\n",
    "            liste_text_top.append(sum(Result))    \n",
    "        if sum(liste_text_top)!=0:\n",
    "            topic_of_text.append([text,max(range(len(liste_text_top)), key=liste_text_top.__getitem__),max(liste_text_top)/sum(liste_text_top),liste_text_top])\n",
    "        else:\n",
    "            topic_of_text.append([text,'None','None',liste_text_top])\n",
    "    return pd.DataFrame(data=topic_of_text,columns=['text','topic_num','percent','perc_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>language_2</th>\n",
       "      <th>word_tokenize</th>\n",
       "      <th>sent_tokenize</th>\n",
       "      <th>count_word</th>\n",
       "      <th>count_sent</th>\n",
       "      <th>word_tokenize_num_of_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords</th>\n",
       "      <th>word_tokenize_without_stopwords_port</th>\n",
       "      <th>msg_lemmatized</th>\n",
       "      <th>cleanLinks</th>\n",
       "      <th>word_per_sent_mean</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>445164</td>\n",
       "      <td>2970791</td>\n",
       "      <td>25,May,2004</td>\n",
       "      <td>This blog is being posted due to the fact that...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>['This', 'blog', 'is', 'being', 'posted', 'due...</td>\n",
       "      <td>['This blog is being posted due to the fact th...</td>\n",
       "      <td>83</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>['This', 'blog', 'posted', 'due', 'fact', 'lit...</td>\n",
       "      <td>['thi', 'blog', 'post', 'due', 'fact', 'littl'...</td>\n",
       "      <td>['thi', 'blog', 'post', 'due', 'fact', 'littl'...</td>\n",
       "      <td>This blog is being posted due to the fact that...</td>\n",
       "      <td>20.75</td>\n",
       "      <td>male</td>\n",
       "      <td>14</td>\n",
       "      <td>Student</td>\n",
       "      <td>Sagittarius</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>293091</td>\n",
       "      <td>3931851</td>\n",
       "      <td>02,agosto,2004</td>\n",
       "      <td>So I have a big fucking interview tomorrow for...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>['So', 'I', 'have', 'a', 'big', 'fucking', 'in...</td>\n",
       "      <td>['So I have a big fucking interview tomorrow f...</td>\n",
       "      <td>88</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>['So', 'I', 'big', 'fucking', 'interview', 'to...</td>\n",
       "      <td>['so', 'i', 'big', 'fuck', 'interview', 'tomor...</td>\n",
       "      <td>['so', 'i', 'big', 'fuck', 'interview', 'tomor...</td>\n",
       "      <td>So I have a big fucking interview tomorrow for...</td>\n",
       "      <td>17.60</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Pisces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>594697</td>\n",
       "      <td>3481650</td>\n",
       "      <td>07,July,2004</td>\n",
       "      <td>I was reminded just now of the time Ashley and...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>['I', 'was', 'reminded', 'just', 'now', 'of', ...</td>\n",
       "      <td>['I was reminded just now of the time Ashley a...</td>\n",
       "      <td>574</td>\n",
       "      <td>32</td>\n",
       "      <td>234</td>\n",
       "      <td>['I', 'reminded', 'time', 'Ashley', 'I', 'drov...</td>\n",
       "      <td>['i', 'remind', 'time', 'ashley', 'i', 'drove'...</td>\n",
       "      <td>['i', 'remind', 'time', 'ashley', 'i', 'drove'...</td>\n",
       "      <td>I was reminded just now of the time Ashley and...</td>\n",
       "      <td>17.94</td>\n",
       "      <td>female</td>\n",
       "      <td>17</td>\n",
       "      <td>Student</td>\n",
       "      <td>Gemini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3764</td>\n",
       "      <td>4187211</td>\n",
       "      <td>20,August,2004</td>\n",
       "      <td>I was checking up on my cousin Dylan and Fanni...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>['I', 'was', 'checking', 'up', 'on', 'my', 'co...</td>\n",
       "      <td>['\"I was checking up on my cousin Dylan and Fa...</td>\n",
       "      <td>84</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>['I', 'checking', 'cousin', 'Dylan', 'Fannie',...</td>\n",
       "      <td>['i', 'check', 'cousin', 'dylan', 'fanni', '\"s...</td>\n",
       "      <td>['i', 'check', 'cousin', 'dylan', 'fanni', '\"s...</td>\n",
       "      <td>I was checking up on my cousin Dylan and Fanni...</td>\n",
       "      <td>28.00</td>\n",
       "      <td>female</td>\n",
       "      <td>23</td>\n",
       "      <td>Student</td>\n",
       "      <td>Taurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>162953</td>\n",
       "      <td>3686696</td>\n",
       "      <td>24,June,2004</td>\n",
       "      <td>for the NME interview click urlLink part 1 and...</td>\n",
       "      <td>en</td>\n",
       "      <td>en</td>\n",
       "      <td>['for', 'the', 'NME', 'interview', 'click', 'u...</td>\n",
       "      <td>['for the NME interview click urlLink part 1 a...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>['NME', 'interview', 'click', 'urlLink', 'part...</td>\n",
       "      <td>['nme', 'interview', 'click', 'urllink', 'part...</td>\n",
       "      <td>['nme', 'interview', 'click', 'urllink', 'part...</td>\n",
       "      <td>for the NME interview click urlLink part 1 and...</td>\n",
       "      <td>12.00</td>\n",
       "      <td>female</td>\n",
       "      <td>23</td>\n",
       "      <td>Student</td>\n",
       "      <td>Aquarius</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id            date  \\\n",
       "0      445164  2970791     25,May,2004   \n",
       "1      293091  3931851  02,agosto,2004   \n",
       "2      594697  3481650    07,July,2004   \n",
       "3        3764  4187211  20,August,2004   \n",
       "4      162953  3686696    24,June,2004   \n",
       "\n",
       "                                                text lang language_2  \\\n",
       "0  This blog is being posted due to the fact that...   en         en   \n",
       "1  So I have a big fucking interview tomorrow for...   en         en   \n",
       "2  I was reminded just now of the time Ashley and...   en         en   \n",
       "3  I was checking up on my cousin Dylan and Fanni...   en         en   \n",
       "4  for the NME interview click urlLink part 1 and...   en         en   \n",
       "\n",
       "                                       word_tokenize  \\\n",
       "0  ['This', 'blog', 'is', 'being', 'posted', 'due...   \n",
       "1  ['So', 'I', 'have', 'a', 'big', 'fucking', 'in...   \n",
       "2  ['I', 'was', 'reminded', 'just', 'now', 'of', ...   \n",
       "3  ['I', 'was', 'checking', 'up', 'on', 'my', 'co...   \n",
       "4  ['for', 'the', 'NME', 'interview', 'click', 'u...   \n",
       "\n",
       "                                       sent_tokenize  count_word  count_sent  \\\n",
       "0  ['This blog is being posted due to the fact th...          83           4   \n",
       "1  ['So I have a big fucking interview tomorrow f...          88           5   \n",
       "2  ['I was reminded just now of the time Ashley a...         574          32   \n",
       "3  ['\"I was checking up on my cousin Dylan and Fa...          84           3   \n",
       "4  ['for the NME interview click urlLink part 1 a...          12           1   \n",
       "\n",
       "   word_tokenize_num_of_stopwords  \\\n",
       "0                              34   \n",
       "1                              31   \n",
       "2                             234   \n",
       "3                              32   \n",
       "4                               3   \n",
       "\n",
       "                     word_tokenize_without_stopwords  \\\n",
       "0  ['This', 'blog', 'posted', 'due', 'fact', 'lit...   \n",
       "1  ['So', 'I', 'big', 'fucking', 'interview', 'to...   \n",
       "2  ['I', 'reminded', 'time', 'Ashley', 'I', 'drov...   \n",
       "3  ['I', 'checking', 'cousin', 'Dylan', 'Fannie',...   \n",
       "4  ['NME', 'interview', 'click', 'urlLink', 'part...   \n",
       "\n",
       "                word_tokenize_without_stopwords_port  \\\n",
       "0  ['thi', 'blog', 'post', 'due', 'fact', 'littl'...   \n",
       "1  ['so', 'i', 'big', 'fuck', 'interview', 'tomor...   \n",
       "2  ['i', 'remind', 'time', 'ashley', 'i', 'drove'...   \n",
       "3  ['i', 'check', 'cousin', 'dylan', 'fanni', '\"s...   \n",
       "4  ['nme', 'interview', 'click', 'urllink', 'part...   \n",
       "\n",
       "                                      msg_lemmatized  \\\n",
       "0  ['thi', 'blog', 'post', 'due', 'fact', 'littl'...   \n",
       "1  ['so', 'i', 'big', 'fuck', 'interview', 'tomor...   \n",
       "2  ['i', 'remind', 'time', 'ashley', 'i', 'drove'...   \n",
       "3  ['i', 'check', 'cousin', 'dylan', 'fanni', '\"s...   \n",
       "4  ['nme', 'interview', 'click', 'urllink', 'part...   \n",
       "\n",
       "                                          cleanLinks  word_per_sent_mean  \\\n",
       "0  This blog is being posted due to the fact that...               20.75   \n",
       "1  So I have a big fucking interview tomorrow for...               17.60   \n",
       "2  I was reminded just now of the time Ashley and...               17.94   \n",
       "3  I was checking up on my cousin Dylan and Fanni...               28.00   \n",
       "4  for the NME interview click urlLink part 1 and...               12.00   \n",
       "\n",
       "   gender  age    topic         sign  \n",
       "0    male   14  Student  Sagittarius  \n",
       "1    male   15  Student       Pisces  \n",
       "2  female   17  Student       Gemini  \n",
       "3  female   23  Student       Taurus  \n",
       "4  female   23  Student     Aquarius  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_spacy=pd.read_csv('data/train_500.csv')\n",
    "df_text_spacy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_words=\n",
    "df_topic=pd.read_csv('LDA/lda_gesamt.csv')\n",
    "df_topic=df_topic.loc[:, ~df_topic.columns.str.contains('^Unnamed')]\n",
    "num_topics=int(len(df_topic.columns)/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_word</th>\n",
       "      <th>0_per</th>\n",
       "      <th>1_word</th>\n",
       "      <th>1_per</th>\n",
       "      <th>2_word</th>\n",
       "      <th>2_per</th>\n",
       "      <th>3_word</th>\n",
       "      <th>3_per</th>\n",
       "      <th>4_word</th>\n",
       "      <th>4_per</th>\n",
       "      <th>...</th>\n",
       "      <th>33_word</th>\n",
       "      <th>33_per</th>\n",
       "      <th>34_word</th>\n",
       "      <th>34_per</th>\n",
       "      <th>35_word</th>\n",
       "      <th>35_per</th>\n",
       "      <th>36_word</th>\n",
       "      <th>36_per</th>\n",
       "      <th>37_word</th>\n",
       "      <th>37_per</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go</td>\n",
       "      <td>0.176</td>\n",
       "      <td>world</td>\n",
       "      <td>0.123</td>\n",
       "      <td>time</td>\n",
       "      <td>0.023</td>\n",
       "      <td>weird</td>\n",
       "      <td>0.194</td>\n",
       "      <td>free</td>\n",
       "      <td>0.104</td>\n",
       "      <td>...</td>\n",
       "      <td>learn</td>\n",
       "      <td>0.099</td>\n",
       "      <td>nbsp</td>\n",
       "      <td>0.494</td>\n",
       "      <td>note</td>\n",
       "      <td>0.122</td>\n",
       "      <td>add</td>\n",
       "      <td>0.224</td>\n",
       "      <td>link</td>\n",
       "      <td>0.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>then</td>\n",
       "      <td>0.069</td>\n",
       "      <td>believe</td>\n",
       "      <td>0.103</td>\n",
       "      <td>good</td>\n",
       "      <td>0.021</td>\n",
       "      <td>brother</td>\n",
       "      <td>0.137</td>\n",
       "      <td>study</td>\n",
       "      <td>0.059</td>\n",
       "      <td>...</td>\n",
       "      <td>however</td>\n",
       "      <td>0.096</td>\n",
       "      <td>comment</td>\n",
       "      <td>0.073</td>\n",
       "      <td>view</td>\n",
       "      <td>0.112</td>\n",
       "      <td>create</td>\n",
       "      <td>0.110</td>\n",
       "      <td>hair</td>\n",
       "      <td>0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there</td>\n",
       "      <td>0.040</td>\n",
       "      <td>student</td>\n",
       "      <td>0.050</td>\n",
       "      <td>day</td>\n",
       "      <td>0.021</td>\n",
       "      <td>st</td>\n",
       "      <td>0.080</td>\n",
       "      <td>sign</td>\n",
       "      <td>0.058</td>\n",
       "      <td>...</td>\n",
       "      <td>question</td>\n",
       "      <td>0.087</td>\n",
       "      <td>box</td>\n",
       "      <td>0.042</td>\n",
       "      <td>step</td>\n",
       "      <td>0.092</td>\n",
       "      <td>character</td>\n",
       "      <td>0.097</td>\n",
       "      <td>church</td>\n",
       "      <td>0.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>guy</td>\n",
       "      <td>0.035</td>\n",
       "      <td>lie</td>\n",
       "      <td>0.044</td>\n",
       "      <td>now</td>\n",
       "      <td>0.020</td>\n",
       "      <td>stare</td>\n",
       "      <td>0.072</td>\n",
       "      <td>level</td>\n",
       "      <td>0.047</td>\n",
       "      <td>...</td>\n",
       "      <td>apparently</td>\n",
       "      <td>0.056</td>\n",
       "      <td>smart</td>\n",
       "      <td>0.032</td>\n",
       "      <td>build</td>\n",
       "      <td>0.065</td>\n",
       "      <td>window</td>\n",
       "      <td>0.064</td>\n",
       "      <td>ill</td>\n",
       "      <td>0.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>home</td>\n",
       "      <td>0.034</td>\n",
       "      <td>chance</td>\n",
       "      <td>0.043</td>\n",
       "      <td>really</td>\n",
       "      <td>0.019</td>\n",
       "      <td>universe</td>\n",
       "      <td>0.050</td>\n",
       "      <td>floor</td>\n",
       "      <td>0.042</td>\n",
       "      <td>...</td>\n",
       "      <td>result</td>\n",
       "      <td>0.040</td>\n",
       "      <td>accept</td>\n",
       "      <td>0.030</td>\n",
       "      <td>interested</td>\n",
       "      <td>0.060</td>\n",
       "      <td>difference</td>\n",
       "      <td>0.054</td>\n",
       "      <td>click</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  0_word  0_per   1_word  1_per  2_word  2_per    3_word  3_per 4_word  4_per  \\\n",
       "0     go  0.176    world  0.123    time  0.023     weird  0.194   free  0.104   \n",
       "1   then  0.069  believe  0.103    good  0.021   brother  0.137  study  0.059   \n",
       "2  there  0.040  student  0.050     day  0.021        st  0.080   sign  0.058   \n",
       "3    guy  0.035      lie  0.044     now  0.020     stare  0.072  level  0.047   \n",
       "4   home  0.034   chance  0.043  really  0.019  universe  0.050  floor  0.042   \n",
       "\n",
       "   ...     33_word  33_per  34_word  34_per     35_word  35_per     36_word  \\\n",
       "0  ...       learn   0.099     nbsp   0.494        note   0.122         add   \n",
       "1  ...     however   0.096  comment   0.073        view   0.112      create   \n",
       "2  ...    question   0.087      box   0.042        step   0.092   character   \n",
       "3  ...  apparently   0.056    smart   0.032       build   0.065      window   \n",
       "4  ...      result   0.040   accept   0.030  interested   0.060  difference   \n",
       "\n",
       "   36_per 37_word  37_per  \n",
       "0   0.224    link   0.120  \n",
       "1   0.110    hair   0.077  \n",
       "2   0.097  church   0.068  \n",
       "3   0.064     ill   0.063  \n",
       "4   0.054   click   0.040  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stopwords \u001b[39m=\u001b[39m stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_liste=df_text_spacy.text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization_test(texts,  allowed_posttags=['NOUN','ADJ','VERB','ADV']):\n",
    "    nlp=spacy.load('en_core_web_md',disable=['parser','ner'])\n",
    "    texts_out=[]\n",
    "    for text in tqdm(texts):\n",
    "        doc= nlp(text)\n",
    "        new_text=[]\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_posttags:\n",
    "                new_text.append(token.lemma_)\n",
    "        final=' '.join(new_text)\n",
    "        texts_out.append(final)\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61511cb25e5841beb6bda9b97234aba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lemma_text=lemmatization_test(text_liste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_words(texts):\n",
    "    final=[]\n",
    "    for text in tqdm(texts):\n",
    "        new= gensim.utils.simple_preprocess(text,deacc=True)\n",
    "        final.append(new)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6651ae3848c4f8da8629a796e8b2681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_words=gen_words(lemma_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4c67c707f54dfcb0885aa9aed83ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic_num</th>\n",
       "      <th>percent</th>\n",
       "      <th>perc_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[blog, post, fact, little, development, happen...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.441402</td>\n",
       "      <td>[0.0, 0.0, 0.16700000000000004, 0.0, 0.021, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[so, big, fucking, interview, tomorrow, new, s...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.312445</td>\n",
       "      <td>[0.176, 0.0, 0.083, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[remind, just, now, time, drive, window, down,...</td>\n",
       "      <td>27</td>\n",
       "      <td>0.203974</td>\n",
       "      <td>[1.863999999999999, 0.05, 0.5950000000000003, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[check, cousin, wedding, site, just, get, marr...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.278658</td>\n",
       "      <td>[0.0, 0.0, 0.08300000000000002, 0.08, 0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[interview, click, urllink, part, urllink, part]</td>\n",
       "      <td>31</td>\n",
       "      <td>0.946866</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19495</th>\n",
       "      <td>[urllink, talk, evolutionary, flop, breakfast,...</td>\n",
       "      <td>31</td>\n",
       "      <td>0.240484</td>\n",
       "      <td>[0.142, 0.24799999999999997, 0.302000000000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19496</th>\n",
       "      <td>[government, work, pretty, good, website, thin...</td>\n",
       "      <td>31</td>\n",
       "      <td>0.688119</td>\n",
       "      <td>[0.015, 0.05, 0.045, 0.0, 0.059, 0.0, 0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19497</th>\n",
       "      <td>[totally, pump, read, book, call, excited, boo...</td>\n",
       "      <td>31</td>\n",
       "      <td>0.436421</td>\n",
       "      <td>[0.020999999999999998, 0.0, 0.1009999999999999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19498</th>\n",
       "      <td>[married]</td>\n",
       "      <td>36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19499</th>\n",
       "      <td>[get, hair, cut, again, today, rather, cute, s...</td>\n",
       "      <td>27</td>\n",
       "      <td>0.323714</td>\n",
       "      <td>[0.222, 0.0, 0.158, 0.0, 0.0, 0.0, 0.034, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text topic_num   percent  \\\n",
       "0      [blog, post, fact, little, development, happen...         7  0.441402   \n",
       "1      [so, big, fucking, interview, tomorrow, new, s...        20  0.312445   \n",
       "2      [remind, just, now, time, drive, window, down,...        27  0.203974   \n",
       "3      [check, cousin, wedding, site, just, get, marr...        20  0.278658   \n",
       "4       [interview, click, urllink, part, urllink, part]        31  0.946866   \n",
       "...                                                  ...       ...       ...   \n",
       "19495  [urllink, talk, evolutionary, flop, breakfast,...        31  0.240484   \n",
       "19496  [government, work, pretty, good, website, thin...        31  0.688119   \n",
       "19497  [totally, pump, read, book, call, excited, boo...        31  0.436421   \n",
       "19498                                          [married]        36       1.0   \n",
       "19499  [get, hair, cut, again, today, rather, cute, s...        27  0.323714   \n",
       "\n",
       "                                               perc_list  \n",
       "0      [0.0, 0.0, 0.16700000000000004, 0.0, 0.021, 0....  \n",
       "1      [0.176, 0.0, 0.083, 0.0, 0.0, 0.0, 0.0, 0.0, 0...  \n",
       "2      [1.863999999999999, 0.05, 0.5950000000000003, ...  \n",
       "3      [0.0, 0.0, 0.08300000000000002, 0.08, 0.0, 0.0...  \n",
       "4      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "...                                                  ...  \n",
       "19495  [0.142, 0.24799999999999997, 0.302000000000000...  \n",
       "19496  [0.015, 0.05, 0.045, 0.0, 0.059, 0.0, 0.0, 0.0...  \n",
       "19497  [0.020999999999999998, 0.0, 0.1009999999999999...  \n",
       "19498  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "19499  [0.222, 0.0, 0.158, 0.0, 0.0, 0.0, 0.034, 0.0,...  \n",
       "\n",
       "[19500 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_df_overview(data_words,num_topics,df_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54c35701683d52397dec8c1b1dabb7d7599bf83c9aec0584240eb3e578830d84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
